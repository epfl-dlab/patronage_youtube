{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterizing Patronage on YouTube\n",
    "\n",
    "## 0. Files and brief explanation of those\n",
    "\n",
    "All data is located in `/dlabdata1/youtube_large/`\n",
    "\n",
    "**YouNiverse dataset:**\n",
    "\n",
    "- `df_channels_en.tsv.gz`: channel metadata.\n",
    "- `df_timeseries_en.tsv.gz`: channel-level time-series.\n",
    "- `yt_metadata_en.jsonl.gz`: raw video metadata.\n",
    "- `youtube_comments.tsv.gz`: user-comment matrices.\n",
    "- `youtube_comments.ndjson.zst`: raw comments â€” this is a HUGE file.\n",
    "\n",
    "**Graphteon dataset:**\n",
    "- `creators.csv` list with all creator names.\n",
    "- `final_processed_file.jsonl.gz` all graphteon time-series.\n",
    "- `pages.zip` raw html of the pages in graphteon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import tqdm\n",
    "import zstandard\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/dlabdata1/youtube_large/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list all files in DATA_FOLDER\n",
    "# !ls -lh /dlabdata1/youtube_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. YouNiverse dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Channel metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/df_channels_en.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel metadata\n",
    "df_yt_channels = pd.read_csv(DATA_FOLDER+'df_channels_en.tsv.gz', sep=\"\\t\", compression='gzip')\n",
    "df_yt_channels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Youtube channel-level time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/df_timeseries_en.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel-level time-series.\n",
    "df_yt_timeseries = pd.read_csv(DATA_FOLDER+'df_timeseries_en.tsv.gz', sep=\"\\t\", compression='gzip', nrows=100)\n",
    "df_yt_timeseries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Raw video metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dlabdata1/youtube_large/yt_metadata_en.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! zcat /dlabdata1/youtube_large/yt_metadata_en.jsonl.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata = pd.read_json(DATA_FOLDER+'yt_metadata_en.jsonl.gz', compression='gzip', lines=True, nrows=100)\n",
    "df_yt_metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 user-comment matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/youtube_comments.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-comment matrices\n",
    "df_yt_comments = pd.read_csv(DATA_FOLDER+'youtube_comments.tsv.gz', sep=\"\\t\", compression='gzip', nrows=100)\n",
    "df_yt_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5 raw comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/youtube_comments.ndjson.zst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_jsonify(line): \n",
    "    \"\"\"\n",
    "\n",
    "    :param line: string to parse and jsonify\n",
    "    :return: \n",
    "    \"\"\"    \n",
    "    \n",
    "    # add square brackets around line\n",
    "    line = \"[\" + line + \"]\"\n",
    "\n",
    "    # remove quotes before and after square brackets   \n",
    "    line = line.replace(\"\\\"[{\", \"[{\")\n",
    "    line = line.replace(\"}]\\\"\", \"}]\")    \n",
    "    \n",
    "    # replace double double-quotes with single double-quotes\n",
    "    line = line.replace(\"{\\\"\\\"\", \"{\\\"\")\n",
    "    line = line.replace(\"\\\"\\\"}\", \"\\\"}\")\n",
    "    line = line.replace(\"\\\"\\\":\\\"\\\"\", \"\\\":\\\"\")\n",
    "    line = line.replace(\":\\\"\\\"\", \":\\\"\")\n",
    "    line = line.replace(\"\\\"\\\":\", \"\\\":\")\n",
    "    \n",
    "    # line = line.replace(\"\\\"\\\":\", \"\\\":\")\n",
    "    line = line.replace(\"\\\"\\\",\\\"\\\"\", \"\\\",\\\"\")\n",
    "    line = line.replace(\"\\\"\\\",\\\"\\\"\", \"\\\",\\\"\")\n",
    "    line = line.replace(\"\\\\\\\"\\\"\", \"\\\\\\\"\")\n",
    "    line = line.replace(\"\\\\\\\",[\", \"\\\\\\\\ \\\",[\")\n",
    "    \n",
    "    line = re.sub(r',\\\"\\\"(?!\\,)', ',\\\"', line)\n",
    "\n",
    "    line = line.replace(\"true,\\\"\\\"\", \"true,\\\"\")\n",
    "    line = line.replace(\"false,\\\"\\\"\", \"false,\\\"\")\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        '''Init method'''\n",
    "        import codecs\n",
    "        self.fh = open(file,'rb')\n",
    "        print(f\"reading {file} in chunks ...\")\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstandard.ZstdDecompressor(max_window_size=2147483648)\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def readlines(self):\n",
    "        '''Generator method that creates an iterator for each line of JSON'''\n",
    "        nb_chunk = 0\n",
    "        while True:\n",
    "            nb_chunk = nb_chunk + 1\n",
    "            if nb_chunk % 5000 == 0:\n",
    "                print(\"number of chunks read: \", nb_chunk)\n",
    "                \n",
    "            chunk = self.reader.read(self.chunk_size).decode(\"utf-8\", \"replace\")\n",
    "\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            # print(\"lines per chunk: \", len(lines))\n",
    "            # print(lines)\n",
    "            \n",
    "            for line in lines[:-1]:\n",
    "                # print(line)\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_OF_LINES = 350000\n",
    "lines_json = []\n",
    "inp_file = DATA_FOLDER+\"youtube_comments.ndjson.zst\"\n",
    "reader = Zreader(inp_file, chunk_size=4092)\n",
    "\n",
    "for i, line in enumerate(reader.readlines()):\n",
    "    if i > NB_OF_LINES:\n",
    "        # print(line)\n",
    "        break\n",
    "    line_json = json.loads(line_jsonify(line))\n",
    "    lines_json.append(line_json)\n",
    "\n",
    "print(\"==> number of lines read:\", len(lines_json))\n",
    "\n",
    "df_yt_comments_raw = pd.DataFrame(data=lines_json[1:], columns=lines_json[0])\n",
    "df_yt_comments_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Graphtreon dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 List with all creator names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/creators.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with all creator names.\n",
    "df_gt_creators = pd.read_csv(DATA_FOLDER+'creators.csv')\n",
    "df_gt_creators.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 All graphtreon time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/final_processed_file.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_processed_file.jsonl.gz all graphteon time-series.\n",
    "df_gt_timeseries = pd.read_json(DATA_FOLDER+'final_processed_file.jsonl.gz', compression='gzip', lines=True, nrows=100)\n",
    "df_gt_timeseries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Raw html of the pages in graphteon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dlabdata1/youtube_large/pages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages.zip raw html of the pages in graphteon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Merge channels data with YouTube timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries_merged = df_yt_timeseries.merge(df_yt_channels)\n",
    "df_yt_timeseries_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Merge YouTube timeseries and Graphtreon timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
