{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Characterizing Patronage on YouTube\n",
    "\n",
    "## 0. Files and brief explanation of those\n",
    "\n",
    "All data is located in `/dlabdata1/youtube_large/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/dlabdata1/youtube_large/\"\n",
    "LOCAL_DATA_FOLDER = \"local_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YouNiverse dataset:**\n",
    "\n",
    "- `df_channels_en.tsv.gz`: channel metadata.\n",
    "- `df_timeseries_en.tsv.gz`: channel-level time-series.\n",
    "- `yt_metadata_en.jsonl.gz`: raw video metadata.\n",
    "- `youtube_comments.tsv.gz`: user-comment matrices.\n",
    "- `youtube_comments.ndjson.zst`: raw comments — this is a HUGE file.\n",
    "\n",
    "**Graphteon dataset:**\n",
    "- `creators.csv` list with all creator names.\n",
    "- `final_processed_file.jsonl.gz` all graphteon time-series.\n",
    "- `pages.zip` raw html of the pages in graphteon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libaries imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import zstandard\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "import ast\n",
    "import math\n",
    "import ruptures as rpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in current directory\n",
    "# !ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list all files in DATA_FOLDER\n",
    "!ls -lh {DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. YouNiverse dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Channel metadata\n",
    "Metadata associated with the 136,470 channels: **channel ID**, **join date**, **country**, **number of subscribers**, **most frequent category**, and the **channel’s position** in socialblade.com’s subscriber ranking. \\\n",
    "The number of subscribers is provided both as obtained from channelcrawler.com (between 2019-09-12 and 2019-09-17) and as crawled from socialblade.com (2019-09-27). Additionally, we also provide a set of **weights** (derived from socialblade.com’s subscriber rankings) that can be used to partially correct sample biases in our dataset.\n",
    "\n",
    "- `category_cc`: category of the channel (majority based)\n",
    "- `join_date`: join date of the channel\n",
    "- `channel`: channel id\n",
    "- `name_cc`: name of the channel.\n",
    "- `subscribers_cc`: number of subscribers\n",
    "- `videos_cc`: number of videos\n",
    "- `subscriber_rank_sb`: rank in terms of number of subscribers (channel’s position in socialblade.com’s subscriber ranking)\n",
    "- `weights`: weights cal (Set of weights derived from socialblade.com’s subscriber rankings. Can be used to partially correct sample biases in our dataset. -> correction for representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {DATA_FOLDER}df_channels_en.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel metadata\n",
    "# df_yt_channels = pd.read_csv(DATA_FOLDER+'df_channels_en.tsv.gz', sep=\"\\t\", compression='gzip', nrows=10)\n",
    "df_yt_channels = pd.read_csv(DATA_FOLDER+'df_channels_en.tsv.gz', sep=\"\\t\", compression='gzip')\n",
    "df_yt_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facts about this data (taken from [YouNiverse github page](https://github.com/epfl-dlab/YouNiverse)) \n",
    "\n",
    "- This dataframe has 136,470 rows, where each one corresponds to a different channel.\n",
    "- We obtained all channels with >10k subscribers and >10 videos from channelcrawler.com in the 27 October 2019.\n",
    "- Additionally we filtered all channels that were not in english given their video metadata (see `Raw Channels')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of unique categories:         {:,}'.format(df_yt_channels['category_cc'].nunique()))\n",
    "print('Number of unique channels:      {:,}'.format(df_yt_channels['channel'].nunique()))\n",
    "print('Number of unique channel names: {:,}'.format(df_yt_channels['name_cc'].nunique()))\n",
    "\n",
    "print('\\nNote: there are more unique channels than unique names, so some channels might have the same name!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of videos and subscribers per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['videos_cc', 'subscribers_cc']\n",
    "\n",
    "# plot with linear scale for x axis and log scale for y axis\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.histplot(data=df_yt_channels[col], ax=ax, bins=50, kde=False, color=f'C{i}')\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(\"Count - number of channels (log scale)\")\n",
    "    ax.set(yscale=\"log\")\n",
    "    # ax.set(xscale=\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot with log scale for x axis \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "xlabels = [r'$\\log_{10}(videos)$', r'$\\log_{10}(subscribers)$']\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.histplot(data=np.log10(df_yt_channels[col]), ax=ax, bins=50, kde=False, cumulative=False, color=f'C{i}')\n",
    "\n",
    "    ax.set(title=f'Distribution of {col} (log-log scale)')\n",
    "    ax.set_xlabel(xlabels[i])\n",
    "    ax.set_ylabel(\"Count - number of channels\")\n",
    "\n",
    "    # ax.set(yscale=\"log\")\n",
    "    # ax.set(xscale=\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot with linear scale for both axes \n",
    "# fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "# for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "#     sns.histplot(data=df_yt_channels[col], ax=ax, bins=50, kde=False, color=f'C{i}')\n",
    "#     ax.set(title=f'Distribution of {col}')\n",
    "#     ax.set_ylabel(\"Count - number of channels\")\n",
    "#     # ax.set(yscale=\"log\")\n",
    "#     ax.set(xscale=\"log\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # plot with log scale for x axis (distplot)\n",
    "# fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "# for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "#     sns.distplot(np.log10(df_yt_channels[col]), hist_kws=kwargs, kde=False, kde_kws=kwargs, ax=ax, norm_hist=True)\n",
    "\n",
    "#     ax.set(title=f'Distribution of {col} (log-log scale)')\n",
    "#     ax.set_ylabel(\"Count - number of channels\")\n",
    "#     # ax.set(yscale=\"log\")\n",
    "#     # ax.set(xscale=\"log\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# descriptive statistics table\n",
    "df_yt_channels[selected_cols].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "From the above graphs and table, we can see that _videos_ and _subscribers_ distributions among YouTube channels follow a **power law**, meaning that most channels have a only a few videos and a few subscribers, but a few of them have a lot of videos and a lot of subscribers.\n",
    "\n",
    "More specifically:\n",
    "- 50% of the YouTube channels have less than 175 videos\n",
    "- 50% of the YouTube channels have less than 42,400 subscribers\n",
    "\n",
    "_Note: only channels with at least 10 videos and 10,000 subscribers were considered for this study._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_cat_chan = df_yt_channels.groupby(['category_cc', 'channel'])[['videos_cc', 'subscribers_cc']].agg(['max'])\n",
    "\n",
    "# set the columns to the top level of the multi-index\n",
    "data_per_cat_chan.columns = data_per_cat_chan.columns.get_level_values(0)\n",
    "data_per_cat_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_cat_chan.reset_index(inplace=True)\n",
    "data_per_cat_chan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of channels per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_per_cat = data_per_cat_chan.groupby('category_cc')[['channel']].count().sort_values('channel', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_per_cat.plot(kind='bar')\n",
    "plt.title(\"Number of channels per category\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Number of channels\")\n",
    "plt.show()\n",
    "chan_per_cat['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_per_cat = data_per_cat_chan.groupby('category')['videos_cc','subscribers_cc'].agg(['min', 'max', 'count', 'sum'])\n",
    "data_per_cat = data_per_cat_chan.groupby('category_cc')[['videos_cc','subscribers_cc']].agg(['sum'])\n",
    "data_per_cat.columns = data_per_cat.columns.get_level_values(0)\n",
    "data_per_cat = data_per_cat.add_suffix('_sum')\n",
    "data_per_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of videos per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_cat['videos_cc_sum'].sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title(\"Number of videos per category\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Number of videos\")\n",
    "plt.show()\n",
    "\n",
    "data_per_cat['videos_cc_sum'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of subscribers per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_cat['subscribers_cc_sum'].sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title(\"Number of subscribers per category\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Number of videos\")\n",
    "plt.show()\n",
    "\n",
    "data_per_cat['subscribers_cc_sum'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1.2 YouTube Channels time-series data\n",
    "Weekly number of viewers and subscribers. We have a data point for each channel and each week.\n",
    "\n",
    "Time series of channel activity at **weekly granularity**. The span of time series varies by channel depending on when socialblade.com started tracking the channel. On average, it contains **2.8 years of data per channel** for **133k channels** (notice that this means there are roughly 4k channels for which there is no time-series data). \\\n",
    "Each data point includes the **number of views** (`views`) and **subscribers** (`subs`) obtained in the given week, as well as the **number of videos** (`videos`) posted by the **channel** (`channel`). The number of videos is calculated using the video upload dates in our video metadata, such that videos that were unavailable at crawl time are not accounted for. \n",
    "\n",
    "---\n",
    "\n",
    "Time series related to each channel.\\\n",
    "These come from a mix of YouTube data and time series crawled from [socialblade.com](https://socialblade.com/):\n",
    "- From the former (YouTube data): derived weekly time series indicating **how many videos each channel had posted per week**. \n",
    "- From the latter (socialblade.com): crawled weekly statistics on the **number of viewers** `views` and **subscribers** `subs` per channel `channel`. This data was available for around 153k channels.\n",
    "\n",
    "    - `channel`: unique channel ID, which is the numbers and letters at the end of the URL.\n",
    "    - `category`: category of the channel as assigned by [socialblade.com](https://socialblade.com/) according to the last 10 videos at time of crawl (categories organize channels and videos on YouTube and help creators, advertisers, and channel managers identify with content and audiences they wish to associate with).\n",
    "    - `datetime`: First day of the week related to the data point\n",
    "    - `views`: Total number of views the channel had this week.\n",
    "    - `delta_views`: Delta views obtained this week (difference of nb of views between current and former week). (Interpolation)\n",
    "    - `subs`: Total number of subscribers the channel had this week.\n",
    "    - `delta_subs`: Delta subscribers obtained this week (difference of nb of subscribers between current and former week)\n",
    "    - `videos`: Number of videos posted by the channel up to date\n",
    "    - `delta_videos`:  Delta videos obtained this week (difference of number of videos posted by the channel between current and former week).\n",
    "    - `activity`: Number of videos published in the last 15 days.\n",
    "    \n",
    "    \n",
    "Note: Can view the channel by appending the channel id to the url, e.g.  https://www.youtube.com/channel/UCBJuEqXfXTdcPSbGO9qqn1g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dlabdata1/youtube_large/df_timeseries_en.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load channel-level time-series. (takes btw 50 secs and 2 mins)\n",
    "df_yt_timeseries = pd.read_csv(DATA_FOLDER+'df_timeseries_en.tsv.gz', sep=\"\\t\", compression='gzip', parse_dates=['datetime'])\n",
    "df_yt_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_yt_timeseries.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_ts_uniq_chan_cnt = df_yt_timeseries['channel'].nunique()\n",
    "\n",
    "print('Timeseries data was gathered between {} and {}'.format(df_yt_timeseries['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                         df_yt_timeseries['datetime'].max().strftime('%B %d, %Y')))\n",
    "print('Total number of datapoints accross all channels: {:>12,}'.format(len(df_yt_timeseries)))\n",
    "data_points_dist = df_yt_timeseries['channel'].value_counts()\n",
    "print('Average number of datapoints per channel:       {:>12.0f} weeks (≈{:,.1f} years)'.format(data_points_dist.mean(), data_points_dist.mean()/52))\n",
    "print('Number of unique categories:                     {:>12,}'.format(df_yt_timeseries['category'].nunique()))\n",
    "print('Number of unique channels:                       {:>12,}'.format(yt_ts_uniq_chan_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datetime points per channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all channels timeseries start and end at the same time, therefore we have a different amount of datapoints for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_data = df_yt_timeseries.groupby('channel')['datetime'].agg(['min', 'max'])\n",
    "datetime_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datetime points per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_ts_year_cnt = df_yt_timeseries.groupby(df_yt_timeseries.datetime.dt.year).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Timeseries data was gathered between {} and {}'.format(df_yt_timeseries['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                         df_yt_timeseries['datetime'].max().strftime('%B %d, %Y')))\n",
    "yt_ts_year_cnt.plot(kind='bar')\n",
    "plt.title(\"Nb of datapoints per year accross all channels\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count (datapoints)\")\n",
    "plt.show()\n",
    "\n",
    "yt_ts_year_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datetime points per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_ts_month_cnt = df_yt_timeseries.groupby([df_yt_timeseries.datetime.dt.year, df_yt_timeseries.datetime.dt.month]).size()\n",
    "yt_ts_month_cnt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas.Grouper\n",
    "# yt_ts_month_cnt_grouper = df_yt_timeseries.groupby(pd.Grouper(key='datetime', freq='M')).count().channel\n",
    "# yt_ts_month_cnt_grouper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of datapoints per month\n",
    "plt.figure(figsize=(15,2))\n",
    "yt_ts_month_cnt.plot(kind='bar')\n",
    "plt.title(\"Number of datapoints per month accross all channels (using regular group by method)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count (datapoints)\")\n",
    "plt.show()\n",
    "\n",
    "# plot number of datapoints per month using grouper\n",
    "# plt.figure(figsize=(15,2))\n",
    "# yt_ts_month_cnt_grouper.plot(kind='bar')\n",
    "# plt.title(\"Number of datapoints per month accross all channels (using grouper)\")\n",
    "# plt.xlabel(\"Month\")\n",
    "# plt.ylabel(\"Count (datapoints)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider unique values per channel\n",
    "yt_ts_month_unique_cnt = df_yt_timeseries.groupby(pd.Grouper(key='datetime', freq='M')).agg({\"channel\": pd.Series.nunique})\n",
    "yt_ts_month_unique_cnt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_yt_timeseries.groupby(['datetime', 'channel']).count() > 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels with timeseries (only consider unique values per channel) --> see https://stackoverflow.com/questions/38309729/count-unique-values-per-groups-with-pandas\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(7,3), sharey=True, sharex=True,\n",
    "                       gridspec_kw={\"wspace\": 0.05})\n",
    "\n",
    "ax.plot(yt_ts_month_unique_cnt)\n",
    "\n",
    "ax.set(title='Number of channels with timeseries')\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"# channels\")\n",
    "ax.xaxis.set_major_locator(years)\n",
    "ax.xaxis.set_minor_locator(months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datetime points accross channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of datapoints accross channels\n",
    "\n",
    "print('Total number of datapoints accross all channels: {:>12,}'.format(len(df_yt_timeseries)))\n",
    "data_points_dist = df_yt_timeseries['channel'].value_counts()\n",
    "print('Average number of datapoints per channel:        {:>12,.0f} weeks (≈{:,.1f} years)'.format(data_points_dist.mean(), data_points_dist.mean()/52))\n",
    "\n",
    "ax = sns.histplot(data=data_points_dist, bins=50, kde=False, color=f'C{1}')\n",
    "\n",
    "ax.set(title=f'Distribution of datapoints (weeks) accross channels')\n",
    "ax.set_xlabel('number of data points (weeks)')\n",
    "ax.set_ylabel('number of channels')\n",
    "\n",
    "# ax.set(yscale=\"log\")\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregates per channel\n",
    "sel_cols = ['datetime', 'views', 'delta_views', 'subs', 'delta_subs', 'videos', 'delta_videos', 'activity']\n",
    "data_per_channel = df_yt_timeseries.groupby('channel')[sel_cols].agg(['min', 'max', 'count', 'mean'])\n",
    "data_per_channel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Views per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_channel['views'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of total views per channel\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "\n",
    "sns.histplot(data=data_per_channel['views']['max'], ax=axs[0], bins=20, kde=False, color=f'C{1}')\n",
    "axs[0].set(title=f'Distribution of total views per channel')\n",
    "axs[0].set_xlabel('number of views (in billions)')\n",
    "axs[0].set_ylabel('number of channels')\n",
    "axs[0].set(yscale=\"log\")\n",
    "xlabels1 = ['{:,.0f}'.format(x) + 'bn' for x in axs[0].get_xticks()/1_000_000_000]\n",
    "axs[0].set_xticklabels(xlabels1)\n",
    "\n",
    "# Distribution of total views per channel (log scale)\n",
    "sns.histplot(data=data_per_channel['views']['max'], ax=axs[1], bins=1000, kde=False, color=f'C{1}')\n",
    "axs[1].set(title=f'Distribution of total views per channel (log-log scale)')\n",
    "axs[1].set_xlabel('number of views (in millions)')\n",
    "axs[1].set_ylabel('number of channels')\n",
    "axs[1].set(yscale=\"log\")\n",
    "axs[1].set(xscale=\"log\")\n",
    "xlabels2 = ['{:,.0f}'.format(x) + 'M' for x in axs[1].get_xticks()/1_000_000]\n",
    "axs[1].set_xticklabels(xlabels2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "data_per_channel['views'][['max']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 channels with the most total views (in billions):\")\n",
    "\n",
    "for index, value in data_per_channel['views']['max'].sort_values(ascending=False)[:10].items():\n",
    "    print('https://www.youtube.com/channel/{} : {:,.1f} bn views'.format(index, value/1_000_000_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Videos per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_channel['videos'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of total videos per channel\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "sns.histplot(data=data_per_channel['videos']['max'], ax=axs[0], bins=20, kde=False, color=f'C{1}')\n",
    "\n",
    "axs[0].set(title=f'Distribution of total videos per channel')\n",
    "axs[0].set_xlabel('number of videos')\n",
    "axs[0].set_ylabel('number of channels')\n",
    "axs[0].set(yscale=\"log\")\n",
    "\n",
    "# # Distribution of total views per channel (log scale)\n",
    "sns.histplot(data=data_per_channel['videos']['max'], ax=axs[1], bins=100, kde=False, color=f'C{1}')\n",
    "\n",
    "axs[1].set(title=f'Distribution of total videos per channel (log-log scale)')\n",
    "axs[1].set_xlabel('number of videos')\n",
    "axs[1].set_ylabel('number of channels')\n",
    "axs[1].set(yscale=\"log\")\n",
    "axs[1].set(xscale=\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "data_per_channel['videos'][['max']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 channels with the most total videos:\")\n",
    "\n",
    "for index, value in data_per_channel['videos']['max'].sort_values(ascending=False)[:10].items():\n",
    "    print('https://www.youtube.com/channel/{} : {:,.0f} videos'.format(index, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscribers per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_channel['subs'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of total subscribers per channel\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "sns.histplot(data=data_per_channel['subs']['max'], ax=axs[0], bins=20, kde=False, color=f'C{1}')\n",
    "\n",
    "axs[0].set(title=f'Distribution of total subscribers per channel')\n",
    "axs[0].set_xlabel('number of subscribers (in millions)')\n",
    "axs[0].set_ylabel('number of channels')\n",
    "axs[0].set(yscale=\"log\")\n",
    "xlabels0 = ['{:,.0f}'.format(x) + 'M' for x in axs[0].get_xticks()/1_000_000]\n",
    "axs[0].set_xticklabels(xlabels0)\n",
    "\n",
    "# # Distribution of total views per channel (log scale)\n",
    "sns.histplot(data=data_per_channel['subs']['max'], ax=axs[1], bins=500, kde=False, color=f'C{1}')\n",
    "\n",
    "axs[1].set(title=f'Distribution of total subscribers per channel (log-log scale)')\n",
    "axs[1].set_xlabel('number of subscribers')\n",
    "axs[1].set_ylabel('number of channels')\n",
    "axs[1].set(yscale=\"log\")\n",
    "axs[1].set(xscale=\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "data_per_channel['videos'][['max']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_per_channel['subs']['max'].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 channels with the most total subscribers:\")\n",
    "\n",
    "for index, value in data_per_channel['subs']['max'].sort_values(ascending=False)[:10].items():\n",
    "    print('https://www.youtube.com/channel/{} : {:,.1f}M subscribers'.format(index, value/1_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the columns to the top level of the multi-index\n",
    "# data_per_channel.columns = data_per_channel.columns.get_level_values(0)\n",
    "# data_per_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Raw video metadata\n",
    "The file `df_videos_raw.jsonl.gz` contains metadata data related to ~73M videos from ~137k channels. Below we show the data recorded for each of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {DATA_FOLDER}yt_metadata_en.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! zcat {DATA_FOLDER}yt_metadata_en.jsonl.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata = pd.read_json(DATA_FOLDER+'yt_metadata_en.jsonl.gz', compression='gzip', lines=True, nrows=2000)\n",
    "df_yt_metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 user-comment matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh {DATA_FOLDER}youtube_comments.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-comment matrices\n",
    "df_yt_comments = pd.read_csv(DATA_FOLDER+'youtube_comments.tsv.gz', sep=\"\\t\", compression='gzip', nrows=100)\n",
    "df_yt_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5 raw comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh {DATA_FOLDER}youtube_comments.ndjson.zst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_jsonify(line): \n",
    "    \"\"\"\n",
    "\n",
    "    :param line: string to parse and jsonify\n",
    "    :return: \n",
    "    \"\"\"    \n",
    "    \n",
    "    # add square brackets around line\n",
    "    line = \"[\" + line + \"]\"\n",
    "\n",
    "    # remove quotes before and after square brackets   \n",
    "    line = line.replace(\"\\\"[{\", \"[{\")\n",
    "    line = line.replace(\"}]\\\"\", \"}]\")    \n",
    "    \n",
    "    # replace double double-quotes with single double-quotes\n",
    "    line = line.replace(\"{\\\"\\\"\", \"{\\\"\")\n",
    "    line = line.replace(\"\\\"\\\"}\", \"\\\"}\")\n",
    "    line = line.replace(\"\\\"\\\":\\\"\\\"\", \"\\\":\\\"\")\n",
    "    line = line.replace(\":\\\"\\\"\", \":\\\"\")\n",
    "    line = line.replace(\"\\\"\\\":\", \"\\\":\")\n",
    "    \n",
    "    # line = line.replace(\"\\\"\\\":\", \"\\\":\")\n",
    "    line = line.replace(\"\\\"\\\",\\\"\\\"\", \"\\\",\\\"\")\n",
    "    line = line.replace(\"\\\"\\\",\\\"\\\"\", \"\\\",\\\"\")\n",
    "    line = line.replace(\"\\\\\\\"\\\"\", \"\\\\\\\"\")\n",
    "    line = line.replace(\"\\\\\\\",[\", \"\\\\\\\\ \\\",[\")\n",
    "    \n",
    "    line = re.sub(r',\\\"\\\"(?!\\,)', ',\\\"', line)\n",
    "\n",
    "    line = line.replace(\"true,\\\"\\\"\", \"true,\\\"\")\n",
    "    line = line.replace(\"false,\\\"\\\"\", \"false,\\\"\")\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        '''Init method'''\n",
    "        import codecs\n",
    "        self.fh = open(file,'rb')\n",
    "        print(f\"reading {file} in chunks ...\")\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstandard.ZstdDecompressor(max_window_size=2147483648)\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def readlines(self):\n",
    "        '''Generator method that creates an iterator for each line of JSON'''\n",
    "        nb_chunk = 0\n",
    "        while True:\n",
    "            nb_chunk = nb_chunk + 1\n",
    "            if nb_chunk % 5000 == 0:\n",
    "                print(\"number of chunks read: \", nb_chunk)\n",
    "                \n",
    "            chunk = self.reader.read(self.chunk_size).decode(\"utf-8\", \"replace\")\n",
    "\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            # print(\"lines per chunk: \", len(lines))\n",
    "            # print(lines)\n",
    "            \n",
    "            for line in lines[:-1]:\n",
    "                # print(line)\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_OF_LINES = 350000\n",
    "lines_json = []\n",
    "inp_file = DATA_FOLDER+\"youtube_comments.ndjson.zst\"\n",
    "reader = Zreader(inp_file, chunk_size=4092)\n",
    "\n",
    "for i, line in enumerate(reader.readlines()):\n",
    "    if i > NB_OF_LINES:\n",
    "        # print(line)\n",
    "        break\n",
    "    line_json = json.loads(line_jsonify(line))\n",
    "    lines_json.append(line_json)\n",
    "\n",
    "print(\"==> number of lines read:\", len(lines_json))\n",
    "\n",
    "df_yt_comments_raw = pd.DataFrame(data=lines_json[1:], columns=lines_json[0])\n",
    "df_yt_comments_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Graphtreon dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 List with all creator names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh {DATA_FOLDER}creators.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with all creator names.\n",
    "df_gt_creators = pd.read_csv(DATA_FOLDER+'creators.csv')\n",
    "df_gt_creators.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 All graphtreon time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {DATA_FOLDER}final_processed_file.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_processed_file.jsonl.gz all graphteon time-series.\n",
    "df_gt_timeseries = pd.read_json(DATA_FOLDER+'final_processed_file.jsonl.gz', compression='gzip', lines=True, nrows=10)\n",
    "df_gt_timeseries.head()\n",
    "\n",
    "# df_gt_timeseries = pd.read_json(DATA_FOLDER+'final_processed_file.jsonl.gz', compression='gzip', lines=True)\n",
    "\n",
    "# get only id and match them first\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries['startDate'] = pd.to_datetime(df_gt_timeseries['startDate'])\n",
    "df_gt_timeseries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of unique creators:         {:,}'.format(df_gt_timeseries['creatorName'].nunique()))\n",
    "print('Number of unique patreon ids:         {:,}'.format(df_gt_timeseries['patreon'].nunique()))\n",
    "\n",
    "print('Timeseries data was gathered between {} and {}'.format(df_gt_timeseries['startDate'].min().strftime('%B %d, %Y'),\n",
    "                                                         df_gt_timeseries['startDate'].max().strftime('%B %d, %Y')))\n",
    "print('Total number of datapoints accross all channels: {:>12,}'.format(len(df_gt_timeseries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Raw html of the pages in graphteon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {DATA_FOLDER}pages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages.zip raw html of the pages in graphteon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Match data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_FOLDER = \"/dlabdata1/youtube_large/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files used in this section\n",
    "\n",
    "**YouNiverse dataset:**\n",
    "\n",
    "- (`df_channels_en.tsv.gz`: channel metadata.)\n",
    "- `df_timeseries_en.tsv.gz`: channel-level time-series.\n",
    "- `yt_metadata_en.jsonl.gz`: raw video metadata.\n",
    "\n",
    "**Graphteon dataset:**\n",
    "- `final_processed_file.jsonl.gz` all graphteon time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libaries imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os \n",
    "# import io\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# import re\n",
    "# import zstandard\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.dates as mdates\n",
    "# from matplotlib.ticker import FuncFormatter\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import gzip\n",
    "# from tqdm import tqdm\n",
    "# import timeit\n",
    "# import ast\n",
    "# import math\n",
    "# import datetime\n",
    "# import ruptures as rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1. Filter YouTube metadata containing patreon id\n",
    "_Extract Patreon urls from YouTube metadata description (if they exist) and keep only those rows_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YT_metadata_filter_results_040422.jpg _(filter script in script/scripts.ipynb)_\n",
    "<div>\n",
    "    <img src=\"img/YT_metadata_filter_results_040422.jpg\" alt=\"YT_metadata_filter_results_040422.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original YT dataset\n",
    "DF_YT_METADATA_ROWS = 72_924_794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YT metadata containing patreon ids in description\n",
    "!ls -lh {LOCAL_DATA_FOLDER}yt_metadata_en_pt_040422.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filtered youtube metadata file (takes about 2 mins)\n",
    "df_yt_metadata_pt = pd.read_csv(LOCAL_DATA_FOLDER+\"yt_metadata_en_pt_040422.tsv.gz\", sep=\"\\t\", lineterminator='\\n', compression='gzip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where patreon_ids = patreon.com/posts or patreon.com/user (in the future fix in regex)\n",
    "df_yt_metadata_pt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'] != 'patreon.com/posts']\n",
    "df_yt_metadata_pt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'] != 'patreon.com/user']\n",
    "\n",
    "# lowercase all patreon ids to avoid duplicates\n",
    "df_yt_metadata_pt['patreon_id'] = df_yt_metadata_pt['patreon_id'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats \n",
    "print(\"[YouTube metadata] Total number of videos:                                                {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "print(\"[Filtered YouTube metadata] number of videos that contain a patreon link in description:  {:>10,} ({:.1%} of total dataset)\".format(len(df_yt_metadata_pt), len(df_yt_metadata_pt)/DF_YT_METADATA_ROWS))\n",
    "\n",
    "# get list of all unique patreon ids in df_yt_metadata_pt\n",
    "yt_patreon_list = df_yt_metadata_pt['patreon_id'].unique()\n",
    "yt_pt_channel_list = df_yt_metadata_pt['channel_id'].unique()\n",
    "\n",
    "print(\"[Filtered YouTube metadata] total number of unique patreon ids:                           {:>9,}\".format(len(yt_patreon_list)))\n",
    "print(\"[Filtered YouTube metadata] number of unique channels that contain a patreon account:     {:>9,}\".format(len(yt_pt_channel_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** \\\n",
    "We can see that we have _**more patreon ids than channels**_ . Let's investigate further:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restrict to 1 patreon id per youtube channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by channel_id AND patreon_id and count the number of unique videos (display_ids)\n",
    "df_yt_metadata_pt_grp_chan = df_yt_metadata_pt.groupby(['channel_id','patreon_id']).agg(display_id_cnt=(\"display_id\", pd.Series.nunique))\n",
    "df_yt_metadata_pt_grp_chan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_yt_metadata_pt_grp_chan = df_yt_metadata_pt_grp_chan.reset_index()\n",
    "# df_yt_metadata_pt_grp_chan.head(4)\n",
    "\n",
    "# count the number of patreon_ids per channel\n",
    "pt_id_cnt_pr_chan = df_yt_metadata_pt_grp_chan.groupby('channel_id').count()['patreon_id'].sort_values(ascending=False)\n",
    "pt_id_cnt_pr_chan = pt_id_cnt_pr_chan.to_frame(name='patreon_id_cnt')\n",
    "pt_id_cnt_pr_chan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Distribution of patreon ids per channel\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(6,4))\n",
    "\n",
    "# plot with log scale for x axis and log scale for y axis\n",
    "sns.histplot(data=pt_id_cnt_pr_chan, ax=axs, bins=50, kde=False, legend=False, color=f'C{0}')\n",
    "axs.set(title=f'Distribution of patreon ids per channel (log scale)')\n",
    "axs.set_xlabel(\"Number of patreon ids\")\n",
    "axs.set_ylabel(\"Count of channels (log scale)\")\n",
    "axs.set(yscale=\"log\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# descriptive statistics table\n",
    "pt_id_cnt_pr_chan.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "As we observed earlier, some channels use more than 1 patreon id, and use different patreon urls for different videos. For example:\n",
    "- [Patreon_Gaming](https://www.youtube.com/channel/UCAsLyFlWkbdhvri02tO6veA) uses 73 different patreon ids.\n",
    "- [Artistic Maniacs](https://www.youtube.com/channel/UC3pcSD6_RRisNLaHGznemJA) uses 69 different patreon ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for Artistic Maniacs\n",
    "df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan['channel_id'] == 'UC3pcSD6_RRisNLaHGznemJA'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Optional: Keep only most used patreon_id per channel (patreon_id with most videos for each channel)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort metadata df by diplay_id_cnt within each channel_id group\n",
    "df_yt_metadata_pt_grp_chan = df_yt_metadata_pt_grp_chan.sort_values(['channel_id','display_id_cnt'], ascending=[True, False])\n",
    "df_yt_metadata_pt_grp_chan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of duplicate of rows with same channel id but different patreon ids\n",
    "dup_chan_id = df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan.duplicated(subset=['channel_id'], keep='first')]\n",
    "print(\"Number of duplicate rows (same channel id with multiple patreon_ids): {:,}\".format(len(dup_chan_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows, keep the patreon ids with the most videos\n",
    "df_yt_metadata_unique_pt = df_yt_metadata_pt_grp_chan.drop_duplicates(subset='channel_id', keep='first')\n",
    "print('Removed {:,} rows'.format(len(df_yt_metadata_pt_grp_chan) - len(df_yt_metadata_unique_pt)))\n",
    "df_yt_metadata_unique_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Match\" dataframe (channel/patreon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider them linked only if \n",
    "- [TODO] there is a Patreon link >10% of their videos and if the second most common Patreon link occurs less than 2-3 videos.\n",
    "- [TODO] Remove channels whose patreon ids are not unique\n",
    "- Match YouTube channel to Patreon id which appears in most of its videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store into new \"matched\" dataframe\n",
    "df_matched_channel_patreon = df_yt_metadata_unique_pt[['channel_id', 'patreon_id']]\n",
    "df_matched_channel_patreon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"matched\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"df_matched_channel_patreon.tsv.gz\"\n",
    "# df_matched_channel_patreon.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [_Ignore for now_] Further Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Observation:** \\\n",
    "When grouping YouTube metadata by `channel_id` and `patreon_id`, we also notice that we have more rows than the total number of unique patreon ids. \\\n",
    "This is because some `patreon_id` are used on multiple channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total rows:                        {:,}\".format(len(df_yt_metadata_pt_grp_chan)))\n",
    "print(\"total number of unique patreon ids {:,}\".format(df_yt_metadata_pt.patreon_id.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show patreon_id that are used on multiple channels.\n",
    "df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan.duplicated(subset=['patreon_id'], keep=False)].sort_values(by='patreon_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Filtered YouTube metadata] number of channels per patreon id:\")\n",
    "\n",
    "chan_cnt_per_patreon_id = df_yt_metadata_pt.groupby('patreon_id')\\\n",
    "                                            .agg(channel_id_count=('channel_id', 'count'))\\\n",
    "                                            .sort_values(by=['channel_id_count'], ascending=False)\n",
    "chan_cnt_per_patreon_id\n",
    "# chan_cnt_per_patreon_id.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [_Ignore for now_] Number of videos per patreon id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by patreon_id and count the number of unique display_ids\n",
    "vids_cnt_per_patreon_id = df_yt_metadata_pt.groupby('patreon_id').agg({\"display_id\": pd.Series.nunique}).sort_values(by='display_id', ascending=False)\n",
    "vids_cnt_per_patreon_id.rename(columns={'display_id':'display_id_cnt'}, inplace=True)\n",
    "\n",
    "print(\"[Filtered YouTube metadata] number of videos per patreon id:\")\n",
    "vids_cnt_per_patreon_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with linear scale for both axes\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(6,4))\n",
    "\n",
    "\n",
    "# plot with log scale for x axis and log scale for y axis\n",
    "sns.histplot(data=vids_cnt_per_patreon_id, ax=axs, bins=50, kde=False, color=f'C{0}')\n",
    "axs.set(title=f'Distribution of videos per patreon id (log scale)')\n",
    "axs.set_xlabel(\"Number of videos\")\n",
    "axs.set_ylabel(\"# patreon ids (log scale)\")\n",
    "axs.set(yscale=\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# descriptive statistics table\n",
    "vids_cnt_per_patreon_id.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "From the above graph and table, we can see that the _videos_ distributions among patreon ids follows a **power law**, meaning that most patreon accounts have a only a few videos, but a few of them have a lot of videos.\n",
    "\n",
    "More specifically:\n",
    "- 25% of the Patreon accounts have 1 video\n",
    "- 50% of the Patreon accounts have less than 4 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filter YouTube timeseries - Restrict YouTube channels (4 filters)\n",
    "Restrict YouTube channels according to the following criteria (filters are applied sequentially):\n",
    "- Filter 1: Keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account \n",
    "- Filter 2: At least 2 year between first and last video\n",
    "- Filter 3: At least 20 videos with patreon ids\n",
    "- Filter 4: At least 250k subscribers at data crawling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {DATA_FOLDER}df_timeseries_en.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load channel-level time-series. (takes about 50 secs)\n",
    "df_yt_timeseries = pd.read_csv(DATA_FOLDER+'df_timeseries_en.tsv.gz', sep=\"\\t\", compression='gzip', parse_dates=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global values for filters\n",
    "MIN_DAYS_DELTA = \"730 day\"    # filter 2\n",
    "NB_PATREON_VIDS = 20          # filter 3\n",
    "NB_SUBS = 250_000             # filter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of channels of original YT timeseries dataset (need to first load df_yt_timeseries in 1.1.2)\n",
    "yt_ts_uniq_chan_cnt = df_yt_timeseries['channel'].nunique()\n",
    "print(\"[YouTube Timeseries] Nb of rows of original dataset:                  {:>10,}\".format(len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of channels of original dataset:              {:>10,}\".format(yt_ts_uniq_chan_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **• Filter 1:** Keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter 1: retain only the YT channels that exist in the filtered YT metadata dataset (need to first load df_yt_metadata_pt and yt_pt_channel_list in 2.2.1)\n",
    "df_yt_timeseries_filt1 = df_yt_timeseries[df_yt_timeseries['channel'].isin(yt_pt_channel_list)]\n",
    "chan_list_filt1 = df_yt_timeseries_filt1['channel'].unique()\n",
    "chan_list_filt1_cnt = len(chan_list_filt1)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1:           {:>10,} ({:5.1%} of original dataset)\".format(len(df_yt_timeseries_filt1), len(df_yt_timeseries_filt1)/len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1:          {:>10,} ({:5.1%} of original dataset)\".format(chan_list_filt1_cnt, chan_list_filt1_cnt/yt_ts_uniq_chan_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **• Filter 2:** At least 2 year between first and last video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# among filter1 channels, calculate time difference between the first and the last video for each channel\n",
    "datetime_data = df_yt_timeseries_filt1.groupby('channel').agg(datetime_min=('datetime', 'min'),\n",
    "                                                              datetime_max=('datetime', 'max'))\n",
    "datetime_data['delta_datetime'] = datetime_data['datetime_max'] - datetime_data['datetime_min']\n",
    "\n",
    "# filter channels that we have data for at least MIN_TIME_DELTA days\n",
    "datetime_data_filt2 = datetime_data[datetime_data['delta_datetime'] > pd.Timedelta(MIN_DAYS_DELTA)]\n",
    "\n",
    "# Apply filter on YT Timeseries dataset: retain only those channels that have data for at least MIN_TIME_DELTA days\n",
    "df_yt_timeseries_filt2 = df_yt_timeseries_filt1[df_yt_timeseries_filt1['channel'].isin(datetime_data_filt2.index)]\n",
    "\n",
    "chan_list_filt2 = df_yt_timeseries_filt2['channel'].unique()\n",
    "chan_list_filt2_cnt = len(chan_list_filt2)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2:         {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 dataset)\".format(len(df_yt_timeseries_filt2), len(df_yt_timeseries_filt2)/len(df_yt_timeseries), len(df_yt_timeseries_filt2)/len(df_yt_timeseries_filt1)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2:        {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 channels)\".format(chan_list_filt2_cnt, chan_list_filt2_cnt/yt_ts_uniq_chan_cnt, chan_list_filt2_cnt/chan_list_filt1_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **• Filter 3:** At least 20 videos with patreon ids per channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by channel_id AND patreon_id and count the number of unique videos (=display_ids). (need to load df_yt_metadata_pt_grp_chan from point 2.2.1)\n",
    "# Then filter rows that have at least 20 videos (display_ids) \n",
    "df_yt_metadata_pt_grp_chan_filt3 = df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan['display_id_cnt'] > NB_PATREON_VIDS]\n",
    "df_yt_metadata_pt_grp_chan_filt3\n",
    "\n",
    "# get list of unique channels satisfying filter 3\n",
    "chan_list_filt_3 = df_yt_metadata_pt_grp_chan_filt3['channel_id'].unique()\n",
    "\n",
    "# Apply filter on YT Timeseries dataset: retain only those channels from filt 2 that are in the chan_list_filt_3\n",
    "df_yt_timeseries_filt3 = df_yt_timeseries_filt2[df_yt_timeseries_filt2['channel'].isin(chan_list_filt_3)]\n",
    "\n",
    "chan_list_filt3 = df_yt_timeseries_filt3['channel'].unique()\n",
    "chan_list_filt3_cnt = len(chan_list_filt3)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3:       {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 dataset)\".format(len(df_yt_timeseries_filt3), len(df_yt_timeseries_filt3)/len(df_yt_timeseries), len(df_yt_timeseries_filt3)/len(df_yt_timeseries_filt2)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3:      {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 channels)\".format(chan_list_filt3_cnt, chan_list_filt3_cnt/yt_ts_uniq_chan_cnt, chan_list_filt3_cnt/chan_list_filt2_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **• Filter 4:** At least 250k subscribers at data crawling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregates per channel\n",
    "subs_aggr_per_channel = df_yt_timeseries_filt3.groupby('channel')\\\n",
    "                                               .agg(min_subs=('subs', 'min'),\n",
    "                                                    max_subs=('subs', 'max'))\\\n",
    "                                                .sort_values(by=['max_subs'], ascending=False)\\\n",
    "                                                .reset_index()\n",
    "# subs_aggr_per_channel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to first load data_per_channel (aggregates per channel in 1.1.2 'Datetime points accross channels' section)\n",
    "subs_per_channel_filt4 = subs_aggr_per_channel[subs_aggr_per_channel['max_subs'] > NB_SUBS]\n",
    "\n",
    "# get list of unique channels satisfying filter 4\n",
    "chan_list_filt_4 = subs_per_channel_filt4['channel'].unique()\n",
    "\n",
    "# # Apply filter on YT Timeseries dataset: retain only those channels from filt_3 that are in the chan_list_filt_4\n",
    "df_yt_timeseries_filt4 = df_yt_timeseries_filt3[df_yt_timeseries_filt3['channel'].isin(chan_list_filt_4)]\n",
    "\n",
    "chan_list_filt4 = df_yt_timeseries_filt4['channel'].unique()\n",
    "chan_list_filt4_cnt = len(chan_list_filt4)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3+4:     {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 dataset)\".format(len(df_yt_timeseries_filt4), len(df_yt_timeseries_filt4)/len(df_yt_timeseries), len(df_yt_timeseries_filt4)/len(df_yt_timeseries_filt3)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3+4:    {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 channels)\".format(chan_list_filt4_cnt, chan_list_filt4_cnt/yt_ts_uniq_chan_cnt, chan_list_filt4_cnt/chan_list_filt3_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **• Filter 4b**: At least 50k subscribers in the first 6 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "**• Filters summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[YouTube Timeseries] Stats before and after filters:\")\n",
    "print()\n",
    "\n",
    "print(\"Filter 1 = \\\"keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account\\\"\")\n",
    "print(\"Filter 2 = \\\"at least {:.1f} years ({} days) between first and last video\\\"\".format(pd.Timedelta(MIN_DAYS_DELTA).days/365, pd.Timedelta(MIN_DAYS_DELTA).days))\n",
    "print(\"Filter 3 = \\\"at least {:,} videos with patreon ids per channel\\\"\".format(NB_PATREON_VIDS))\n",
    "print(\"Filter 4 = \\\"at least {:,} subscribers at data crawling time\\\"\".format(NB_SUBS))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of original dataset:                  {:>10,}\".format(len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1:           {:>10,} ({:5.1%} of original dataset)\".format(len(df_yt_timeseries_filt1), len(df_yt_timeseries_filt1)/len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2:         {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 dataset)\".format(len(df_yt_timeseries_filt2), len(df_yt_timeseries_filt2)/len(df_yt_timeseries), len(df_yt_timeseries_filt2)/len(df_yt_timeseries_filt1)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3:       {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 dataset)\".format(len(df_yt_timeseries_filt3), len(df_yt_timeseries_filt3)/len(df_yt_timeseries), len(df_yt_timeseries_filt3)/len(df_yt_timeseries_filt2)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3+4:     {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 dataset)\".format(len(df_yt_timeseries_filt4), len(df_yt_timeseries_filt4)/len(df_yt_timeseries), len(df_yt_timeseries_filt4)/len(df_yt_timeseries_filt3)))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of channels of original dataset:              {:>10,}\".format(yt_ts_uniq_chan_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1:          {:>10,} ({:5.1%} of original dataset)\".format(chan_list_filt1_cnt, chan_list_filt1_cnt/yt_ts_uniq_chan_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2:        {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 channels)\".format(chan_list_filt2_cnt, chan_list_filt2_cnt/yt_ts_uniq_chan_cnt, chan_list_filt2_cnt/chan_list_filt1_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3:      {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 channels)\".format(chan_list_filt3_cnt, chan_list_filt3_cnt/yt_ts_uniq_chan_cnt, chan_list_filt3_cnt/chan_list_filt2_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3+4:    {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 channels)\".format(chan_list_filt4_cnt, chan_list_filt4_cnt/yt_ts_uniq_chan_cnt, chan_list_filt4_cnt/chan_list_filt3_cnt))\n",
    "print()\n",
    "\n",
    "\n",
    "print('[YouTube Timeseries] Time range of original dataset                   {} and {}'.format(df_yt_timeseries['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                              df_yt_timeseries['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "print('[YouTube Timeseries] Time range after applying filter 1+2+3+4        {} and {}'.format(df_yt_timeseries_filt4['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                              df_yt_timeseries_filt4['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "display(df_yt_timeseries_filt4.head())\n",
    "\n",
    "print(\"Restricted list of channels after 4 filters (count = {:,}):\".format(chan_list_filt4_cnt))\n",
    "print(chan_list_filt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[ignore] Match patreon_ids and channel_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter YT metadata dataset by list of filtered channels from YT timeseries above\n",
    "# df_yt_metadata_pt_restr = df_yt_metadata_pt[df_yt_metadata_pt['channel_id'].isin(chan_list_filt4)]\n",
    "\n",
    "# # get unique channels for youtube metadata (original and restricted)\n",
    "# yt_metadata_uniq_chan = df_yt_metadata_pt['channel_id'].unique()\n",
    "# yt_metadata_uniq_chan_restr = df_yt_metadata_pt_restr['channel_id'].unique()\n",
    "\n",
    "# # get unique patreon ids for youtube metadata (original and restricted)\n",
    "# yt_metadata_uniq_pat = df_yt_metadata_pt['patreon_id'].unique()\n",
    "# yt_metadata_uniq_pat_restr = df_yt_metadata_pt_restr['patreon_id'].unique()\n",
    "\n",
    "# print(\"[YouTube Metadata]:\")\n",
    "# print()\n",
    "# print(\"Restriction = \\\"keep only YouTube channels that are in YouTube Timeseries filtered (filters 1-4) dataset\\\"\")\n",
    "# print()\n",
    "# # print(\"[YouTube Metadata] Nb of videos in original dataset:                                   {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "# # print(\"[YouTube Metadata] Nb of videos in pre-filtered (containing patreon id) dataset:       {:>10,}\".format(len(df_yt_metadata_pt)))\n",
    "# # print(\"[YouTube Metadata] Nb of videos after filtering by restricted channels:                {:>10,} ({:5.1%} of pre-filtered dataset dataset)\".format(len(df_yt_metadata_pt_restr), len(df_yt_metadata_pt_restr)/len(df_yt_metadata_pt)))\n",
    "# # print()\n",
    "# print(\"[YouTube Metadata] Nb of channels in pre-filtered (containing patreon id) dataset:     {:>10,}\".format(len(yt_metadata_uniq_chan)))\n",
    "# print(\"[YouTube Metadata] Nb of channels after filtering by restricted channels:              {:>10,} ({:5.1%} of pre-filtered dataset dataset)\".format(len(yt_metadata_uniq_chan_restr), len(yt_metadata_uniq_chan_restr)/len(yt_metadata_uniq_chan)))\n",
    "# print()\n",
    "# print(\"[YouTube Metadata] Nb of patreon ids in pre-filtered (containing patreon id) dataset:  {:>10,}\".format(len(yt_metadata_uniq_pat)))\n",
    "# print(\"[YouTube Metadata] Nb of patreon ids after filtering by restricted channels:           {:>10,} ({:5.1%} of pre-filtered dataset dataset)\".format(len(yt_metadata_uniq_pat_restr), len(yt_metadata_uniq_pat_restr)/len(yt_metadata_uniq_pat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Filter Graphtreon to keep only the ones matching patreon id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GT_timeseries_filter_results_032622.jpg _(filter script in scripts/scripts.ipynb)_\n",
    "<div>\n",
    "    <img src=\"img/GT_timeseries_filter_results_032622.jpg\" alt=\"GT_timeseries_filter_results_032622.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original GT dataset\n",
    "GT_final_processed_file_ROWS = 232_269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_gt_timeseries_filtered.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_filtered = pd.read_csv(LOCAL_DATA_FOLDER+\"df_gt_timeseries_filtered.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_gt_timeseries_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistics of loaded pre-filtered Graphtreon Timeseries file:\")\n",
    "print(\"[Graphtreon Timeseries] Total number of patreon ids:                                                   {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids that exist in both GT Timeseries and YT metadata:            {:>9,} ({:.1%} of GT timeseries dataset)\".format(len(df_gt_timeseries_filtered), len(df_gt_timeseries_filtered)/GT_final_processed_file_ROWS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.3.1. Join GT timeseries with matched channel_id\n",
    "match the channels in the restricted list of channels of the matched dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join GT timeseries and matched channels\n",
    "df_gt_timeseries_merged = df_gt_timeseries_filtered.merge(df_matched_channel_patreon, left_on='patreon', right_on='patreon_id')\n",
    "df_gt_timeseries_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.3.2. Filter/Restrict GT timeseries further\n",
    "We now want to reduce the Graphtreon dataset by keeping only rows in filtered list of channels (chan_list_filt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter Graphtreon dataset by keeping only rows in filtered list of channels (chan_list_filt4)\n",
    "df_gt_timeseries_restricted = df_gt_timeseries_merged[df_gt_timeseries_merged['channel_id'].isin(chan_list_filt4)]\n",
    "\n",
    "print(\"[Graphtreon Timeseries] Total number of patreon ids:                                                   {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids that exist in both GT Timeseries and YT metadata:            {:>9,} ({:.1%} of GT timeseries dataset)\".format(len(df_gt_timeseries_filtered), len(df_gt_timeseries_filtered)/GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids that exist in both GT Timeseries and YT metadata restricted  {:>9,} ({:.1%} of GT timeseries dataset)\".format(len(df_gt_timeseries_restricted), len(df_gt_timeseries_restricted)/GT_final_processed_file_ROWS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Extract the date and daily earnings per patreon account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique patreon ids in df_gt_timeseries_restricted\n",
    "yt_gt_patreon_list_restricted = df_gt_timeseries_restricted.patreon.unique()\n",
    "print(\"list of restricted patreon ids\", yt_gt_patreon_list_restricted)\n",
    "print(\"number of restricted patreon ids\", len(yt_gt_patreon_list_restricted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_restricted.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of NaN value\n",
    "# df_gt_timeseries_sample[df_gt_timeseries_sample['creatorName'] == 'Comedy Trap House']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From the Graphtreon dataset, for each channel, extract the date and earnings from “dailyGraph_earningsSeriesData” (takes about 3 mins)\n",
    "# input_file_path = DATA_FOLDER+\"/final_processed_file.jsonl.gz\"\n",
    "\n",
    "# MAX_ITER = 100\n",
    "\n",
    "# nb_rows_read = 0\n",
    "# valid_predicate_count = 0\n",
    "# JSONDecodeErrors_cnt = 0 \n",
    "# dailyEarningsError_cnt = 0 \n",
    "# lines_json = []    \n",
    "\n",
    "# compressed_file_size = os.stat(input_file_path).st_size\n",
    "# print(\"Compressed file size is :                 {:>8,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "# uncompressed_file_size = 13_310_000_000\n",
    "# print(\"Estimated Uncompressed file size is :     {:>8,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "# start = timeit.default_timer()\n",
    "\n",
    "# # Load tqdm with size counter instead of file counter\n",
    "# with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "#     with gzip.open(input_file_path, \"r\") as f:\n",
    "#         for i, line in enumerate(f): \n",
    "\n",
    "#             read_bytes = len(line)\n",
    "#             if read_bytes:\n",
    "#                 pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "#                 pbar.update(read_bytes)\n",
    "\n",
    "#             nb_rows_read += 1\n",
    "            \n",
    "#             # set a maximum iteration for tests\n",
    "#             if nb_rows_read >= MAX_ITER:\n",
    "#                 break\n",
    "    \n",
    "#             try:\n",
    "#                 line_json = json.loads(line)\n",
    "#             except Exception as e:\n",
    "#                 JSONDecodeErrors_cnt += 1\n",
    "#                 continue\n",
    "                \n",
    "#             # add line if patreon id is exists in df_yt_metadata_pt\n",
    "#             if line_json['patreon'] in yt_gt_patreon_list_restricted:\n",
    "#                 valid_predicate_count += 1\n",
    "                \n",
    "#                 # Use ast.literal_eval to convert string of lists, to list of list\n",
    "#                 dailyGraph_earningsSeriesData = line_json.get('dailyGraph_earningsSeriesData')\n",
    "                \n",
    "#                 if dailyGraph_earningsSeriesData:\n",
    "#                     daily_earnings = ast.literal_eval(dailyGraph_earningsSeriesData)\n",
    "#                 else:\n",
    "#                     daily_earnings = [[np.nan, np.nan]]\n",
    "                                            \n",
    "#                 for daily_earning in daily_earnings:\n",
    "#                     # case where there are multiple tuples per row\n",
    "#                     if isinstance(daily_earning, list):\n",
    "#                         date = daily_earning[0]\n",
    "#                         earning = daily_earning[1]\n",
    "#                         lines_json.append({\n",
    "#                             'creatorName':   line_json.get('creatorName'), \n",
    "#                             'creatorRange':  line_json.get('creatorRange'), \n",
    "#                             'startDate':     line_json.get('startDate'),\n",
    "#                             'categoryTitle': line_json.get('categoryTitle'),\n",
    "#                             'patreon':       line_json.get('patreon'),\n",
    "#                             'date':          date,\n",
    "#                             'earning':       earning\n",
    "#                         })\n",
    "#                     else:\n",
    "#                         dailyEarningsError_cnt += 1\n",
    "#                         print(\">>>> dailyEarningsError - skipped line value: \")\n",
    "#                         print(line_json.get('creatorName'), line_json.get('creatorRange'), line_json.get('startDate'), line_json.get('categoryTitle'), line_json.get('patreon'), daily_earnings)\n",
    "\n",
    "# stop = timeit.default_timer()\n",
    "# time_diff = stop - start\n",
    "\n",
    "# print()\n",
    "# print(\"==> total time to read and filter graphtreon time series:                      {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "# print(\"==> number of rows read:                                                       {:>10,}\".format(nb_rows_read))\n",
    "# print(\"==> number of patreon ids that exist in both GTts and restricted YT metadata:  {:>10,} ({:.2%})\".format(valid_predicate_count, valid_predicate_count/nb_rows_read ))\n",
    "# print(\"==> number of skipped rows (JSONDecodeErrors):                                 {:>10,}\".format(JSONDecodeErrors_cnt))\n",
    "# print(\"==> number of skipped rows (dailyEarningsError):                               {:>10,}\".format(dailyEarningsError_cnt))\n",
    "\n",
    "# # create new dataframe with the filtered lines\n",
    "# df_dailyGraph_earningsSeries = pd.DataFrame(data=lines_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GT_timeseries_date_earnings_extract_040422.jpg _(filter script above)_\n",
    "<div>\n",
    "    <img src=\"img/GT_timeseries_date_earnings_extract_040422.jpg\" alt=\"GT_timeseries_date_earnings_extract_040422.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values\n",
    "# df_dailyGraph_earningsSeries[df_dailyGraph_earningsSeries.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv (5.3Mb)\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"dailyGraph_earningsSeries.tsv.gz\"\n",
    "# df_dailyGraph_earningsSeries.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Extract the date and daily patrons per patreon account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From the Graphtreon dataset, for each channel, extract the date and patrons from “dailyGraph_patronSeriesData” (takes about 3 mins)\n",
    "# input_file_path = DATA_FOLDER+\"/final_processed_file.jsonl.gz\"\n",
    "\n",
    "# # MAX_ITER = 1000\n",
    "\n",
    "# nb_rows_read = 0\n",
    "# valid_predicate_count = 0\n",
    "# JSONDecodeErrors_cnt = 0 \n",
    "# dailyPatronsError_cnt = 0 \n",
    "# lines_json = []    \n",
    "\n",
    "# compressed_file_size = os.stat(input_file_path).st_size\n",
    "# print(\"Compressed file size is :                 {:>8,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "# uncompressed_file_size = 13_310_000_000\n",
    "# print(\"Estimated Uncompressed file size is :     {:>8,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "# start = timeit.default_timer()\n",
    "\n",
    "# # Load tqdm with size counter instead of file counter\n",
    "# with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "#     with gzip.open(input_file_path, \"r\") as f:\n",
    "#         for i, line in enumerate(f): \n",
    "\n",
    "#             read_bytes = len(line)\n",
    "#             if read_bytes:\n",
    "#                 pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "#                 pbar.update(read_bytes)\n",
    "\n",
    "#             nb_rows_read += 1\n",
    "            \n",
    "#             # set a maximum iteration for tests\n",
    "#             # if nb_rows_read >= MAX_ITER:\n",
    "#             #     break\n",
    "    \n",
    "#             try:\n",
    "#                 line_json = json.loads(line)\n",
    "#             except Exception as e:\n",
    "#                 JSONDecodeErrors_cnt += 1\n",
    "#                 continue\n",
    "                \n",
    "#             # add line if patreon id is exists in df_yt_metadata_pt\n",
    "#             if line_json['patreon'] in yt_gt_patreon_list_restricted:\n",
    "#                 valid_predicate_count += 1\n",
    "                \n",
    "#                 # Use ast.literal_eval to convert string of lists, to list of list\n",
    "#                 dailyGraph_patronSeriesData = line_json.get('dailyGraph_patronSeriesData')\n",
    "                \n",
    "#                 if dailyGraph_patronSeriesData:\n",
    "#                     daily_patrons = ast.literal_eval(dailyGraph_patronSeriesData)\n",
    "#                 else:\n",
    "#                     daily_patrons = [[np.nan, np.nan]]\n",
    "                                            \n",
    "#                 for daily_patron in daily_patrons:\n",
    "#                     # case where there are multiple tuples per row\n",
    "#                     if isinstance(daily_patron, list):\n",
    "#                         date = daily_patron[0]\n",
    "#                         patrons = daily_patron[1]\n",
    "#                         lines_json.append({\n",
    "#                             'creatorName':   line_json.get('creatorName'), \n",
    "#                             'creatorRange':  line_json.get('creatorRange'), \n",
    "#                             'startDate':     line_json.get('startDate'),\n",
    "#                             'categoryTitle': line_json.get('categoryTitle'),\n",
    "#                             'patreon':       line_json.get('patreon'),\n",
    "#                             'date':          date,\n",
    "#                             'patrons':       patrons\n",
    "#                         })\n",
    "#                     else:\n",
    "#                         dailyPatronsError_cnt += 1\n",
    "#                         print(\">>>> dailyPatronsError - skipped line value: \")\n",
    "#                         print(line_json.get('creatorName'), line_json.get('creatorRange'), line_json.get('startDate'), line_json.get('categoryTitle'), line_json.get('patreon'), daily_patrons)\n",
    "\n",
    "# stop = timeit.default_timer()\n",
    "# time_diff = stop - start\n",
    "\n",
    "# print()\n",
    "# print(\"==> total time to read and filter graphtreon time series:                      {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "# print(\"==> number of rows read:                                                       {:>10,}\".format(nb_rows_read))\n",
    "# print(\"==> number of patreon ids that exist in both GTts and restricted YT metadata:  {:>10,} ({:.2%})\".format(valid_predicate_count, valid_predicate_count/nb_rows_read ))\n",
    "# print(\"==> number of skipped rows (JSONDecodeErrors):                                 {:>10,}\".format(JSONDecodeErrors_cnt))\n",
    "# print(\"==> number of skipped rows (dailyPatronsError):                               {:>10,}\".format(dailyPatronsError_cnt))\n",
    "\n",
    "# # create new dataframe with the filtered lines\n",
    "# df_dailyGraph_patronsSeries = pd.DataFrame(data=lines_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GT_timeseries_date_patrons_extract_042922.jpg _(filter script above)_\n",
    "<div>\n",
    "    <img src=\"img/GT_timeseries_date_patrons_extract_042922.jpg\" alt=\"GT_timeseries_date_patrons_extract_042922.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values\n",
    "# df_dailyGraph_patronsSeries[df_dailyGraph_patronsSeries.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv (7.1Mb)\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"dailyGraph_patronsSeries.tsv.gz\"\n",
    "# df_dailyGraph_patronsSeries.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Merge extracted times series of daily earnings and daily patrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dailyGraph_earningsSeries.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dailyGraph_earningsSeries file from disk and convert dates\n",
    "df_dailyGraph_earningsSeries = pd.read_csv(LOCAL_DATA_FOLDER+\"dailyGraph_earningsSeries.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "# df_dailyGraph_earningsSeries.date = pd.to_datetime(df_dailyGraph_earningsSeries.date, unit='ms')\n",
    "df_dailyGraph_earningsSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dailyGraph_patronsSeries.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dailyGraph_patronsSeries from disk and convert dates\n",
    "df_dailyGraph_patronsSeries = pd.read_csv(LOCAL_DATA_FOLDER+\"dailyGraph_patronsSeries.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "# df_dailyGraph_patronsSeries.date = pd.to_datetime(df_dailyGraph_patronsSeries.date, unit='ms')\n",
    "df_dailyGraph_patronsSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dailyGraph_earningsSeries with df_dailyGraph_patronsSeries\n",
    "df_dailyGraph_patrons_and_earnings_Series = df_dailyGraph_earningsSeries.merge(df_dailyGraph_patronsSeries, how='outer')\n",
    "\n",
    "# convert patrons column to Int64 so it can hold NaN values after outer join\n",
    "df_dailyGraph_patrons_and_earnings_Series['patrons'] = df_dailyGraph_patrons_and_earnings_Series['patrons'].astype('Int64')\n",
    "df_dailyGraph_patrons_and_earnings_Series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv (6.2Mb)\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"dailyGraph_patrons_and_earnings_Series.tsv.gz\"\n",
    "# df_dailyGraph_patrons_and_earnings_Series.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dailyGraph_patrons_and_earnings_Series.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read merged dailyGraph_patrons_and_earnings_Series from disk\n",
    "df_dailyGraph_patrons_and_earnings_Series = pd.read_csv(LOCAL_DATA_FOLDER+\"dailyGraph_patrons_and_earnings_Series.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_dailyGraph_patrons_and_earnings_Series['date'] = pd.to_datetime(df_dailyGraph_patrons_and_earnings_Series['date'], unit='ms')\n",
    "df_dailyGraph_patrons_and_earnings_Series['patrons'] = df_dailyGraph_patrons_and_earnings_Series['patrons'].astype('Int64')\n",
    "\n",
    "print(df_dailyGraph_patrons_and_earnings_Series.dtypes)\n",
    "df_dailyGraph_patrons_and_earnings_Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.4.1 Plot Patreon Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = mdates.YearLocator()\n",
    "months = mdates.MonthLocator()\n",
    "years_fmt = mdates.DateFormatter('%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-declare global variable for size of original GT dataset\n",
    "GT_final_processed_file_ROWS = 232_269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restrict to top 10 patrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_CNT = 20\n",
    "# group by patreon account\n",
    "dailyGraph_grp_patreon = df_dailyGraph_patrons_and_earnings_Series.groupby('patreon')\\\n",
    "                                                     .agg(date_cnt=('date', 'count'),\n",
    "                                                          earliest_date=('date', 'min'),\n",
    "                                                          lastest_date=('date', 'max'),\n",
    "                                                          daily_earning_mean=('earning', 'mean'),\n",
    "                                                          daily_earning_max=('earning', 'max'))\\\n",
    "                                                     .sort_values(by=['daily_earning_max'], ascending=False)\\\n",
    "                                                     .reset_index()\\\n",
    "                                                     .round(2)\n",
    "\n",
    "# remove hours from dates\n",
    "dailyGraph_grp_patreon.earliest_date = dailyGraph_grp_patreon.earliest_date.dt.date\n",
    "dailyGraph_grp_patreon.lastest_date = dailyGraph_grp_patreon.lastest_date.dt.date\n",
    "\n",
    "dailyGraph_grp_patreon\n",
    "\n",
    "# extract the top 10 most profitable patreon accounts\n",
    "top_patreons = dailyGraph_grp_patreon[:TOP_CNT]['patreon']\n",
    "\n",
    "print(\"[Graphtreon Timeseries] Total number of patreon ids (original file):                      {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids in dailyGraph patreon + earnings time series:   {:>9,} ({:.1%} of original dataset)\".format(len(dailyGraph_grp_patreon), len(dailyGraph_grp_patreon)/GT_final_processed_file_ROWS))\n",
    "\n",
    "print()\n",
    "\n",
    "dailyGraph_grp_patreon[:TOP_CNT].style.set_caption(f\"Top {TOP_CNT} highest-earning Patreon accounts (sorted by max daily earnings)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_pt_daily_earnings = df_dailyGraph_patrons_and_earnings_Series[df_dailyGraph_patrons_and_earnings_Series['patreon'].isin(top_patreons)]\n",
    "df_top_pt_daily_earnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "\n",
    "# plot Patreon daily earningsSeriesData for top patreon accounts\n",
    "fig, axs = plt.subplots(int(TOP_CNT/2), 2, figsize=(12, TOP_CNT*1.2), sharey=False, sharex=False)\n",
    "for idx, patreon in tqdm(enumerate(top_patreons)):\n",
    "    row = math.floor(idx/2)\n",
    "    col = idx % 2\n",
    "    ax1 = axs[row, col]\n",
    "    \n",
    "    # ax1.scatter(x[:4], y[:4], s=10, c='b', marker=\"s\", label='first')\n",
    "    # ax1.scatter(x[40:],y[40:], s=10, c='r', marker=\"o\", label='second')\n",
    "\n",
    "    tmp_df = df_top_pt_daily_earnings[df_top_pt_daily_earnings['patreon'] == patreon]\n",
    "\n",
    "    # sbplt = axs[idx, 0]\n",
    "    \n",
    "\n",
    "    color = 'tab:blue'\n",
    "    patrons, = ax1.plot(tmp_df['date'], tmp_df['patrons'], color=color, label='patrons')\n",
    "    ax1.set_ylabel('# Patrons', color=color) \n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set(title=patreon)\n",
    "    \n",
    "    \n",
    "    color = 'tab:orange'\n",
    "    ax2 = ax1.twinx()  # Create a twin Axes sharing the xaxis.\n",
    "    earnings, = ax2.plot(tmp_df['date'], tmp_df['earning'], color=color, label='earnings')\n",
    "    ax2.set_ylabel(\"Earnings per month\", color=color) \n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax1.xaxis.set_major_locator(years)\n",
    "    ax1.xaxis.set_major_formatter(years_fmt)\n",
    "    ax1.xaxis.set_minor_locator(months)\n",
    "    # ax1.legend(handles=[earnings, patrons], loc='upper left');\n",
    "    \n",
    "fig.suptitle(f'Timeseries of the top {TOP_CNT} highest-earning Patreon accounts \\n (earnings per month in dollars)', fontweight=\"bold\")\n",
    "fig.text(0.5,0, 'Month')\n",
    "# fig.text(0,0.5, 'Earnings per month ($)', rotation = 90)\n",
    "fig.tight_layout(pad=3, w_pad=5, h_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "We can see a drop of income at the beginning of each month. \n",
    "--> due to people unsubscribing\n",
    "--> could do some averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse 1 account in detail\n",
    "patreon_account = 'patreon.com/tinymeatgang'\n",
    "\n",
    "with pd.option_context('display.max_rows', 90, 'display.min_rows', 90):\n",
    "    display(df_top_pt_daily_earnings[(df_top_pt_daily_earnings['patreon'] == patreon_account) \n",
    "                                     # & (df_top_pt_daily_earnings['date'] > pd.Timestamp('2021-01-01'))\n",
    "                                    ].head(20))\n",
    "        \n",
    "df_top_pt_daily_earnings.dtypes\n",
    "\n",
    "# check for NaN values\n",
    "# df_top_pt_daily_earnings[df_top_pt_daily_earnings.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect breaks / shocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_breaks = 3\n",
    "# model = rpt.Dynp(model=\"l1\")\n",
    "\n",
    "# # plot Patreon daily earningsSeriesData for top patreon accounts\n",
    "# fig, axs = plt.subplots(int(TOP_CNT/2), 2, figsize=(12, TOP_CNT*1.2), sharey=False, sharex=False)\n",
    "# for idx, patreon in enumerate(top_patreons):\n",
    "#     print(patreon)\n",
    "#     row = math.floor(idx/2)\n",
    "#     col = idx % 2\n",
    "#     sbplt = axs[row, col]\n",
    "\n",
    "#     tmp_df = df_top_pt_daily_earnings[df_top_pt_daily_earnings['patreon'] == patreon]\n",
    "\n",
    "#     # convert the dataframe into a time series.\n",
    "#     ts_df = tmp_df.set_index(tmp_df['date'])\n",
    "#     ts = ts_df['earning']\n",
    "\n",
    "#     y = np.array(ts.tolist())\n",
    "\n",
    "#     model.fit(y)\n",
    "#     breaks = model.predict(n_bkps=n_breaks-1)\n",
    "    \n",
    "#     breaks_rpt = []\n",
    "#     for i in breaks:\n",
    "#         breaks_rpt.append(ts.index[i-1])\n",
    "#     breaks_rpt = pd.to_datetime(breaks_rpt)\n",
    "    \n",
    "#     sbplt.plot(ts, label='data')\n",
    "#     sbplt.set(title=patreon)\n",
    "#     print_legend = True\n",
    "#     for i in breaks_rpt:\n",
    "#         if print_legend:\n",
    "#             sbplt.axvline(i, color='red',linestyle='dashed', label='breaks')\n",
    "#             print_legend = False\n",
    "#         else:\n",
    "#             sbplt.axvline(i, color='red',linestyle='dashed')\n",
    "\n",
    "#     sbplt.xaxis.set_major_locator(years)\n",
    "#     sbplt.xaxis.set_major_formatter(years_fmt)\n",
    "#     sbplt.xaxis.set_minor_locator(months)\n",
    "#     sbplt.xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "#     sbplt.yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "    \n",
    "    \n",
    "# fig.suptitle(f'Timeseries of the top {TOP_CNT} highest-earning Patreon accounts \\n (earnings per month in dollars)', fontweight=\"bold\")\n",
    "# fig.text(0.5,0, 'Month')\n",
    "# fig.text(0,0.5, 'Earnings per month ($)', rotation = 90)\n",
    "# fig.tight_layout(pad=3, w_pad=5, h_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Plot YouTube timeseries for channels matching top Patreon accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matching dataframe\n",
    "df_matched_channel_patreon = pd.read_csv(LOCAL_DATA_FOLDER+\"df_matched_channel_patreon.tsv.gz\", sep=\"\\t\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add patreon_id column to YT timeseries\n",
    "df_yt_timeseries_filt4_merged = df_yt_timeseries_filt4.merge(df_matched_channel_patreon, left_on='channel', right_on='channel_id')\n",
    "df_yt_timeseries_filt4_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter channels matching top patreon accounts\n",
    "df_yt_timeseries_top_pt = df_yt_timeseries_filt4_merged[df_yt_timeseries_filt4_merged['patreon_id'].isin(top_patreons)]\n",
    "\n",
    "\n",
    "print('[YouTube Timeseries] Time range after applying filter 1+2+3+4              {} and {}'.format(df_yt_timeseries_filt4['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                                                                    df_yt_timeseries_filt4['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "print('[YouTube Timeseries] Time range after matching top patreon accounts        {} and {}'.format(df_yt_timeseries_top_pt['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                                                                    df_yt_timeseries_top_pt['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "top_yt_patreons = df_yt_timeseries_top_pt.patreon_id.unique()\n",
    "top_yt_patreons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries_top_pt.groupby(['patreon_id', 'channel_id'])\\\n",
    "                                                     .agg(datetime_cnt=('datetime', 'count'),\n",
    "                                                          date_min=('datetime', 'min'),\n",
    "                                                          date_max=('datetime', 'max'),\n",
    "                                                          views_max=('views', 'max'),\n",
    "                                                          subs_date=('subs', 'max'),\n",
    "                                                          videos_max=('videos', 'mean'))\\\n",
    "                                                     .sort_values(by=['videos_max'], ascending=False)\n",
    "                                                     #      \\\n",
    "                                                     # .reset_index()\\\n",
    "                                                     # .round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries_top_pt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot YT cumulative views timeseries for top patreon accounts\n",
    "# fig, axs = plt.subplots(int(math.ceil(len(top_yt_patreons)/2)), 2, figsize=(12, len(top_yt_patreons)*1.2), sharey=False, sharex=False)\n",
    "# for idx, patreon in enumerate(top_yt_patreons):\n",
    "#     row = math.floor(idx/2)\n",
    "#     col = idx % 2\n",
    "#     sbplt = axs[row, col]\n",
    "\n",
    "#     tmp_df = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon]\n",
    "\n",
    "#     sbplt.plot(tmp_df['datetime'], tmp_df['views'])\n",
    "#     sbplt.set(title=patreon+\"\\n\"+tmp_df['channel'].iloc[0])\n",
    "#     sbplt.xaxis.set_major_locator(years)\n",
    "#     sbplt.xaxis.set_major_formatter(years_fmt)\n",
    "#     sbplt.xaxis.set_minor_locator(months)\n",
    "    \n",
    "    \n",
    "# fig.suptitle(f'YouTube timeseries of the channels corresponging to the top {TOP_CNT} highest-earning Patreon accounts \\n (YT views per week)', fontweight=\"bold\")\n",
    "# fig.text(0.5,0, 'Week')\n",
    "# fig.text(0,0.5, 'Views', rotation = 90)\n",
    "# fig.tight_layout(pad=3, w_pad=5, h_pad=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot YT views timeseries for top patreon accounts\n",
    "# print(f'YouTube views per week timeseries per channel (for the top {TOP_CNT} highest-earning Patreon accounts)')\n",
    "\n",
    "# for idx, patreon in enumerate(top_yt_patreons):\n",
    "#     fig, axs = plt.subplots(1, 2, figsize=(12, 3), sharey=False, sharex=True)\n",
    "#     row = idx    \n",
    "\n",
    "#     tmp_df = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon]\n",
    "\n",
    "#     # delta views per week\n",
    "#     sbplt = axs[0]\n",
    "#     sbplt.plot(tmp_df['datetime'], tmp_df['delta_views'])\n",
    "#     sbplt.set(title=\"YouTube delta views per week\")\n",
    "#     sbplt.set_xlabel('Week')\n",
    "#     sbplt.set_ylabel('Delta Views')\n",
    "#     sbplt.xaxis.set_major_locator(years)\n",
    "#     sbplt.xaxis.set_major_formatter(years_fmt)\n",
    "#     sbplt.xaxis.set_minor_locator(months)\n",
    "    \n",
    "#     # cumulative views per week\n",
    "#     sbplt = axs[1]\n",
    "#     sbplt.plot(tmp_df['datetime'], tmp_df['views'])\n",
    "#     sbplt.set(title=\"YouTube cumulative views per week\")\n",
    "#     sbplt.set_xlabel('Week')\n",
    "#     sbplt.set_ylabel('Views')\n",
    "#     sbplt.xaxis.set_major_locator(years)\n",
    "#     sbplt.xaxis.set_major_formatter(years_fmt)\n",
    "#     sbplt.xaxis.set_minor_locator(months)\n",
    "    \n",
    "    \n",
    "#     print(f\"{idx+1}: https://youtube.com/channel/{tmp_df['channel'].iloc[0]} \\t\\t {patreon}\")\n",
    "#     fig.suptitle(f\"{patreon}\\n{tmp_df['channel'].iloc[0]}\", fontweight=\"bold\") \n",
    "#     fig.tight_layout(w_pad=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore the descending drop in cumulative views per week for sonicether channel \n",
    "# with pd.option_context('display.max_rows', 90, 'display.min_rows', 90):\n",
    "#     display(df_yt_timeseries_top_pt[\n",
    "#         (df_yt_timeseries_top_pt['channel_id'] == 'UCAbpj6UljjAz7JvJt-yJIjg') \n",
    "#      & (df_yt_timeseries_top_pt['datetime'] > pd.Timestamp('2018-04-08'))\n",
    "#      & (df_yt_timeseries_top_pt['datetime'] < pd.Timestamp('2018-06-01'))\n",
    "#     ].head(90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot YT subsciptions timeseries for top patreon accounts\n",
    "# fig, axs = plt.subplots(int(math.ceil(len(top_yt_patreons)/2)), 2, figsize=(12, len(top_yt_patreons)*1.2), sharey=False, sharex=False)\n",
    "# for idx, patreon in enumerate(top_yt_patreons):\n",
    "#     row = math.floor(idx/2)\n",
    "#     col = idx % 2\n",
    "#     sbplt = axs[row, col]\n",
    "\n",
    "#     tmp_df = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon]\n",
    "\n",
    "#     sbplt.plot(tmp_df['datetime'], tmp_df['subs'])\n",
    "#     sbplt.set(title=patreon+\"\\n\"+tmp_df['channel'].iloc[0])\n",
    "#     sbplt.xaxis.set_major_locator(years)\n",
    "#     sbplt.xaxis.set_major_formatter(years_fmt)\n",
    "#     sbplt.xaxis.set_minor_locator(months)\n",
    "    \n",
    "    \n",
    "# fig.suptitle(f'YouTube timeseries of the channels corresponging to the top {TOP_CNT} highest-earning Patreon accounts \\n (YT subscriptions per week)', fontweight=\"bold\")\n",
    "# fig.text(0.5,0, 'Week')\n",
    "# fig.text(0,0.5, 'Views', rotation = 90)\n",
    "# fig.tight_layout(pad=3, w_pad=5, h_pad=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot YT # videos timeseries for top patreon accounts\n",
    "\n",
    "# fig, axs = plt.subplots(int(math.ceil(len(top_yt_patreons)/2)), 2, figsize=(12, len(top_yt_patreons)*1.2), sharey=False, sharex=False)\n",
    "# for idx, patreon in enumerate(top_yt_patreons):\n",
    "#     row = math.floor(idx/2)\n",
    "#     col = idx % 2\n",
    "#     sbplt = axs[row, col]\n",
    "\n",
    "#     tmp_df = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon]\n",
    "\n",
    "#     sbplt.plot(tmp_df['datetime'], tmp_df['videos'])\n",
    "#     sbplt.set(title=patreon+\"\\n\"+tmp_df['channel'].iloc[0])\n",
    "#     sbplt.xaxis.set_major_locator(years)\n",
    "#     sbplt.xaxis.set_major_formatter(years_fmt)\n",
    "#     sbplt.xaxis.set_minor_locator(months)\n",
    "    \n",
    "# fig.suptitle(f'YouTube timeseries of the channels corresponging to the top {TOP_CNT} highest-earning Patreon accounts \\n (YT videos per week)', fontweight=\"bold\")\n",
    "# fig.text(0.5,0, 'Week')\n",
    "# fig.text(0,0.5, 'Views', rotation = 90)\n",
    "# fig.tight_layout(pad=3, w_pad=5, h_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Compare YouTube and top Patreon timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove patreon accounts that have more than 1 youtube channel\n",
    "df_yt_timeseries_top_pt_chan_id_cnt = df_yt_timeseries_top_pt.groupby(['patreon_id','channel_id']).agg(channel_id_cnt=(\"channel_id\", pd.Series.nunique))\n",
    "df_yt_timeseries_top_pt_chan_id_cnt = df_yt_timeseries_top_pt_chan_id_cnt.groupby('patreon_id').count()\n",
    "df_yt_timeseries_top_pt_unique_chan = df_yt_timeseries_top_pt_chan_id_cnt[df_yt_timeseries_top_pt_chan_id_cnt['channel_id_cnt']==1]\n",
    "\n",
    "top_patreons_unique_chan = df_yt_timeseries_top_pt_unique_chan.index\n",
    "top_patreons_unique_chan.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KM(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    if x > 999_999:\n",
    "        return '%2.1fM' % (x * 1e-6)\n",
    "    elif x > 999:\n",
    "        return '%2.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%3.0f ' % (x)\n",
    "KM_formatter = FuncFormatter(KM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare YouTube and Patreon timeseries for top patreon accounts\n",
    "n_breaks = 1\n",
    "model = rpt.Dynp(model=\"l1\")\n",
    "date_offset = pd.DateOffset(months=1)\n",
    "week_offset = pd.DateOffset(weeks=1)\n",
    "\n",
    "for idx, patreon in enumerate(top_patreons_unique_chan):\n",
    "    fig, axs = plt.subplots(5, 4, figsize=(26, 10), sharey=False, sharex=False)\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = df_top_pt_daily_earnings[df_top_pt_daily_earnings['patreon'] == patreon]\n",
    "    # tmp_df_pt = df_top_pt_daily_earnings[(df_top_pt_daily_earnings['patreon'] == patreon) & (df_top_pt_daily_earnings['earning'].notna())]\n",
    "    \n",
    "    # youtube videos\n",
    "    tmp_df_yt = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon]\n",
    "\n",
    "    # set min and max dates for plots   \n",
    "    date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "    date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "\n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "    tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    \n",
    "    \n",
    "    # patreon earnings with breaks/shocks\n",
    "    # code inspired by https://towardsdatascience.com/getting-started-with-breakpoints-analysis-in-python-124471708d38\n",
    "\n",
    "    # convert the dataframe into a time series.\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date'])\n",
    "    ts_pt = ts_pt_df['patrons']\n",
    "    y = np.array(ts_pt.tolist())\n",
    "    \n",
    "    # train the model\n",
    "    model.fit(y)\n",
    "    \n",
    "    # get the breakpoints\n",
    "    breaks = model.predict(n_breaks)\n",
    "    breaks_rpt = []\n",
    "    for i in breaks[:-1]:\n",
    "        breaks_rpt.append(ts_pt.index[i-1])\n",
    "\n",
    "        \n",
    "    # plot number of patrons (twice the same plot: 1 for each column)\n",
    "    axs[0,0].plot(ts_pt)\n",
    "    axs[0,0].set(title=\"Number of patrons\")\n",
    "    axs[0,0].set_ylabel(\"# Patrons\")    \n",
    "    \n",
    "    axs[0,1].plot(ts_pt)\n",
    "    axs[0,1].set(title=\"Number of patrons\")\n",
    "    axs[0,1].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "\n",
    "    # plot vertical lines for each breakpoint\n",
    "    print_legend = True\n",
    "    for breakpoint in breaks_rpt:\n",
    "        for i in range(axs.shape[0]):\n",
    "            for j in range(axs.shape[1]):\n",
    "                if print_legend:\n",
    "                    axs[i,j].axvline(breakpoint, color='red', linestyle='--', label='breaks', linewidth=2.5)\n",
    "                    axs[i,j].axvline(breakpoint - date_offset*2, color='pink', linestyle=':', label='- ' + str(2*date_offset.months)+' months', linewidth=2)\n",
    "                    axs[i,j].axvline(breakpoint - date_offset, color='green', linestyle=':', label='- ' + str(date_offset.months)+' months', linewidth=2)\n",
    "                    axs[i,j].axvline(breakpoint + date_offset, color='orange', linestyle=':', label='+' + str(date_offset.months)+' months', linewidth=2)          \n",
    "                    print_legend = False\n",
    "                else:\n",
    "                    axs[i,j].axvline(breakpoint, color='red', linestyle='--', linewidth=2.5)\n",
    "                    axs[i,j].axvline(breakpoint - date_offset*2, color='pink', linestyle=':', linewidth=2)\n",
    "                    axs[i,j].axvline(breakpoint - date_offset, color='green', linestyle=':', linewidth=2)\n",
    "                    axs[i,j].axvline(breakpoint + date_offset, color='orange', linestyle=':', linewidth=2)\n",
    "\n",
    "    axs[0,0].legend()\n",
    "    # axs[0,1].legend()\n",
    "\n",
    "    # model_2 = \"l2\"\n",
    "    # signal = ts_pt.values\n",
    "    # algo = rpt.n(model=model_2).fit(signal)\n",
    "    # my_bkps = algo.predict(n_bkps=1)\n",
    "    # rpt.show.display(signal, my_bkps, figsize=(6, 2))\n",
    "\n",
    "    # plot patreon earnings (twice the same plot: 1 for each column)\n",
    "    axs[1,0].plot(ts_pt_df['date'], ts_pt_df['earning'])\n",
    "    axs[1,0].set(title=\"Patreon earnings per month\")\n",
    "    axs[1,0].set_ylabel(\"Earnings\")    \n",
    "    \n",
    "    axs[1,1].plot(ts_pt_df['date'], ts_pt_df['earning'])\n",
    "    axs[1,1].set(title=\"Patreon earnings per month\")\n",
    "    axs[1,1].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    \n",
    "    # youtube videos (delta)\n",
    "    # axs[2,0].plot(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], 'r')\n",
    "    # axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=3, marker='o')\n",
    "    axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,0].set(title=\"YouTube delta videos per week\")\n",
    "    axs[2,0].set_ylabel(\"Δ Videos\")\n",
    "\n",
    "    # youtube videos (cumulative)\n",
    "    axs[2,1].plot(tmp_df_yt['datetime'], tmp_df_yt['videos'], 'r')\n",
    "    axs[2,1].set(title=\"YouTube cumulative videos\")\n",
    "    axs[2,1].set_ylabel(\"# Videos\")\n",
    "\n",
    "    \n",
    "    # youtube views (delta)\n",
    "    # axs[3,0].plot(tmp_df_yt['datetime'], tmp_df_yt['delta_views'], 'g')\n",
    "    axs[3,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,0].set(title=\"YouTube delta views per week\")\n",
    "    axs[3,0].set_ylabel(\"Δ Views\")\n",
    "\n",
    "    # youtube views (cumulative)\n",
    "    axs[3,1].plot(tmp_df_yt['datetime'], tmp_df_yt['views'], 'g')\n",
    "    axs[3,1].set(title=\"YouTube cumulative views\")\n",
    "    axs[3,1].set_ylabel(\"# Views\")\n",
    "\n",
    "    \n",
    "    # youtube subs (delta)\n",
    "    # axs[4,0].plot(tmp_df_yt['datetime'], tmp_df_yt['delta_subs'], 'm')\n",
    "    axs[4,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_subs'], c='m', s=30, marker='+')\n",
    "\n",
    "    axs[4,0].set(title=\"YouTube delta subscriptions per week\")\n",
    "    axs[4,0].set_ylabel(\"Δ Subscriptions\")\n",
    "\n",
    "    # youtube subs (cumulative)\n",
    "    axs[4,1].plot(tmp_df_yt['datetime'], tmp_df_yt['subs'], 'm')\n",
    "    axs[4,1].set(title=\"YouTube cumulative subscriptions\")\n",
    "    axs[4,1].set_ylabel(\"# Subscriptions\")\n",
    "\n",
    "    \n",
    "        \n",
    "    ################################### ZOOM IN THE BREAK PERIOD ###################################\n",
    "\n",
    "    date_min_zoom = breaks_rpt[0] - date_offset*2 - week_offset\n",
    "    date_max_zoom = breaks_rpt[0] + date_offset*1 + week_offset\n",
    "            \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)]\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)]\n",
    "    \n",
    "    \n",
    "    # zoomed in patrons\n",
    "    axs[0,2].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons'])\n",
    "    axs[0,2].set(title=\"Number of patrons (zoomed in)\")\n",
    "    axs[0,2].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "    \n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons'])\n",
    "    axs[0,3].set(title=\"Number of patrons (zoomed in)\")\n",
    "    axs[0,3].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "            \n",
    "    # plot patreon earnings (twice the same plot: 1 for each column)\n",
    "    axs[1,2].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning'])\n",
    "    axs[1,2].set(title=\"Patreon earnings per month (zoomed in)\")\n",
    "    axs[1,2].set_ylabel(\"Earnings\")    \n",
    "    \n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning'])\n",
    "    axs[1,3].set(title=\"Patreon earnings per month (zoomed in)\")\n",
    "    axs[1,3].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    \n",
    "    # zoomed in youtube videos (delta)\n",
    "    # axs[2,0].plot(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], 'r')\n",
    "    # axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=3, marker='o')\n",
    "    axs[2,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,2].set(title=\"YouTube delta videos per week (zoomed in)\")\n",
    "    axs[2,2].set_ylabel(\"Δ Videos\")\n",
    "\n",
    "    # zoomed in youtube videos (cumulative)\n",
    "    axs[2,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['videos'], 'r')\n",
    "    axs[2,3].set(title=\"YouTube cumulative videos (zoomed in)\")\n",
    "    axs[2,3].set_ylabel(\"# Videos\")\n",
    "\n",
    "    \n",
    "    # zoomed in youtube views (delta)\n",
    "    # axs[3,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], 'g')\n",
    "    axs[3,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,2].set(title=\"YouTube delta views per week (zoomed in)\")\n",
    "    axs[3,2].set_ylabel(\"Δ Views\")\n",
    "\n",
    "    # zoomed in youtube views (cumulative)\n",
    "    axs[3,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['views'], 'g')\n",
    "    axs[3,3].set(title=\"YouTube cumulative views (zoomed in)\")\n",
    "    axs[3,3].set_ylabel(\"# Views\")\n",
    "\n",
    "    \n",
    "    # zoomed in youtube subs (delta)\n",
    "    # axs[4,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], 'm')\n",
    "    axs[4,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', s=30, marker='+')\n",
    "\n",
    "    axs[4,2].set(title=\"YouTube delta subscriptions per week (zoomed in)\")\n",
    "    axs[4,2].set_ylabel(\"Δ Subscriptions\")\n",
    "\n",
    "    # zoomed in youtube subs (cumulative)\n",
    "    axs[4,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['subs'], 'm')\n",
    "    axs[4,3].set(title=\"YouTube cumulative subscriptions (zoomed in)\")\n",
    "    axs[4,3].set_ylabel(\"# Subscriptions\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    # format the axes\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if j < 2:\n",
    "                axs[i,j].set_xlim([date_min, date_max])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            if j >= 2:\n",
    "                axs[i,j].set_xlim([date_min_zoom, date_max_zoom])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "                # axs[i,j].xaxis.set_minor_locator(mdates.WeekdayLocator())\n",
    "            axs[i,j].xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            \n",
    "            axs[i,j].yaxis.set_major_formatter(KM_formatter)\n",
    "            \n",
    "            \n",
    "            \n",
    "    # print YT channels URL, patreon ids, and titles of plots\n",
    "    ch_ids = tmp_df_yt['channel'].unique()\n",
    "\n",
    "    print(f\"\\t\\033[1m Rank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "    print(f\"\\t https://www.{patreon}\")\n",
    "    print(f\"\\t https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    \n",
    "    # print channel(s) related to this patreon account\n",
    "    for ch_id in ch_ids:\n",
    "        print(f\"\\t https://youtube.com/channel/{ch_id}\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "#     nb_patrons_bkpoint =        tmp_df_pt[tmp_df_pt['date'] == breakpoint].patrons.values[0] \n",
    "#     nb_patrons_bkpoint_befor = tmp_df_pt[tmp_df_pt['date'] == breakpoint - date_offset].patrons.values[0] \n",
    "#     nb_patrons_bkpoint_after = tmp_df_pt[tmp_df_pt['date'] == breakpoint + date_offset].patrons.values[0] \n",
    "\n",
    "#     diff_after_break = nb_patrons_bkpoint_after - nb_patrons_bkpoint\n",
    "#     diff_before_break = nb_patrons_bkpoint - nb_patrons_bkpoint_befor\n",
    "\n",
    "#     print(\"Number of patrons 1 month before breakpoint:    {:2}\".format(nb_patrons_bkpoint_befor))\n",
    "#     print(\"Increase of patrons 1 month before breakpoint:  {:2}\".format(diff_before_break))\n",
    "#     print(\"----------------------------------------------------\")\n",
    "#     print(\"Number of patrons at breakpoint:                {:2}\".format(nb_patrons_bkpoint))\n",
    "#     print(\"Increase of patrons 1 month after breakpoint:   {:2}\".format(diff_after_break))\n",
    "#     print(\"----------------------------------------------------\")\n",
    "#     print(\"Number of patrons 1 month after breakpoint:     {:2}\".format(nb_patrons_bkpoint_after))\n",
    "#     print()\n",
    "#     print(\"change of increase between 1 month before and 1 month after: {:.1%}\".format((diff_after_break/diff_before_break) - 1))\n",
    "\n",
    "\n",
    "    tmp_df_pt_prior = tmp_df_pt[(tmp_df_pt['date'] >= breakpoint - 2*date_offset) & (tmp_df_pt['date'] <= breakpoint - date_offset)]\n",
    "    # display(tmp_df_pt_prior)\n",
    "\n",
    "    tmp_df_pt_befor = tmp_df_pt[(tmp_df_pt['date'] >= breakpoint - date_offset) & (tmp_df_pt['date'] <= breakpoint)]\n",
    "    # display(tmp_df_pt_befor)\n",
    "\n",
    "    tmp_df_pt_after = tmp_df_pt[(tmp_df_pt['date'] >= breakpoint) & (tmp_df_pt['date'] <= breakpoint + date_offset)]\n",
    "    # display(tmp_df_pt_after)\n",
    "\n",
    "    patrons_prior_mean = tmp_df_pt_prior['patrons'].mean()\n",
    "    patrons_befor_mean = tmp_df_pt_befor['patrons'].mean()\n",
    "    patrons_after_mean = tmp_df_pt_after['patrons'].mean()\n",
    "    \n",
    "    # plot the means   \n",
    "    axs[0,2].plot(tmp_df_pt_prior['date'].mean(), patrons_prior_mean, marker='o', color='pink')\n",
    "    axs[0,2].plot(tmp_df_pt_befor['date'].mean(), patrons_befor_mean, marker='o', color='green')\n",
    "    axs[0,2].plot(tmp_df_pt_after['date'].mean(), patrons_after_mean, marker='o', color='orange')\n",
    "    \n",
    "    diff_mean_prior_to_befor = patrons_befor_mean - patrons_prior_mean\n",
    "    diff_mean_befor_to_after = patrons_after_mean - patrons_befor_mean\n",
    "\n",
    "    print(\"\\t Mean Number of patrons PRIOR to BEFORE:                   {:5.1f}\".format(patrons_prior_mean))\n",
    "    print(\"\\t Increase of patrons from PRIOR mean to BEFORE mean        {:5.1f}\".format(diff_mean_prior_to_befor))\n",
    "    print(\"\\t ------------------------------------------------------------------\")\n",
    "    print(\"\\t Mean Number of patrons BEFORE breakpoint:                 {:5.1f}\".format(patrons_befor_mean))\n",
    "    print(\"\\t Increase of patrons from BEFORE mean to AFTER mean        {:5.1f}\".format(diff_mean_befor_to_after))\n",
    "    print(\"\\t ------------------------------------------------------------------\")\n",
    "    print(\"\\t Mean Number of patrons AFTER breakpoint:                  {:5.1f}\".format(patrons_after_mean))\n",
    "    print()\n",
    "    print(\"\\t Percentage of increase between PRIOR to BEFORE and BEFORE to AFTER: {:.1%}\".format((diff_mean_befor_to_after/diff_mean_prior_to_befor) - 1))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    \n",
    "    # fig.suptitle(f'{patreon} (rank {idx+1}) \\nyoutube.com/channel/{ch_id}', fontweight=\"bold\")\n",
    "    fig.tight_layout(w_pad=0)\n",
    "    plt.show()\n",
    "    print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example\n",
    "data = {'channel': [\"baba\", \"baba\", \"baba\", \"baba\", \"baba\", \"baba\", \"baba\", \"baba\"], 'time': [1, 2, 3, 4, 5, 6, 7, 8], 'patrons': [2,4,6,8,10,12,14,16]}\n",
    "df = pd.DataFrame(data)\n",
    "    \n",
    "breakpoint_date = 5\n",
    "    \n",
    "plt.plot(data['time'], data['patrons'], marker='o')\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"# patrons\")\n",
    "\n",
    "plt.axvline(breakpoint_date, color='red', linestyle='--', label='breakpoint', linewidth=2.5)\n",
    "plt.axvline(breakpoint_date-2, color='green', linestyle='--', label='-2 month', linewidth=2.5)\n",
    "plt.axvline(breakpoint_date+2, color='orange', linestyle='--', label='+ 2 month', linewidth=2.5)\n",
    "# plt.fill_between(df['time'], 4, 12, color='C0', alpha=0.3)\n",
    "\n",
    "plt.fill_betweenx(df['patrons'], breakpoint_date-2, breakpoint_date, facecolor='green', alpha=0.3)\n",
    "plt.fill_betweenx(df['patrons'], breakpoint_date+2, breakpoint_date, facecolor='orange', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Break detection toy example\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patrons_bkpoint = df[df['time'] == breakpoint_date].patrons.values[0] \n",
    "patrons_bkpoint_plus_1 = df[df['time'] == breakpoint_date+2].patrons.values[0] \n",
    "patrons_bkpoint_minu_1 = df[df['time'] == breakpoint_date-2].patrons.values[0] \n",
    "\n",
    "diff_after_break = patrons_bkpoint_plus_1 - patrons_bkpoint\n",
    "diff_before_break = patrons_bkpoint - patrons_bkpoint_minu_1\n",
    "\n",
    "print(\"Number of patrons 1 month before breakpoint:    {:2}\".format(patrons_bkpoint_minu_1))\n",
    "print(\"Increase of patrons 1 month before breakpoint:  {:2}\".format(diff_before_break))\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Number of patrons at breakpoint:                {:2}\".format(patrons_bkpoint))\n",
    "print(\"Increase of patrons 1 month after breakpoint:   {:2}\".format(diff_after_break))\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Number of patrons 1 month after breakpoint:     {:2}\".format(patrons_bkpoint_plus_1))\n",
    "print()\n",
    "print(\"change of increase between 1 month before and 1 month after: {:.1%}\".format((diff_after_break/diff_before_break) - 1))\n",
    "\n",
    "\n",
    "df_prior = df[(df['time'] >= breakpoint_date-4) & (df['time'] <= breakpoint_date-2)]\n",
    "display(df_prior)\n",
    "\n",
    "df_befor = df[(df['time'] >= breakpoint_date-2) & (df['time'] <= breakpoint_date)]\n",
    "display(df_befor)\n",
    "\n",
    "df_after = df[(df['time'] >= breakpoint_date) & (df['time'] <= breakpoint_date+2)]\n",
    "display(df_after)\n",
    "\n",
    "patrons_prior_mean = df_prior['patrons'].mean()\n",
    "patrons_befor_mean = df_befor['patrons'].mean()\n",
    "patrons_after_mean = df_after['patrons'].mean()\n",
    "\n",
    "diff_mean_prior_to_befor = patrons_befor_mean - patrons_prior_mean\n",
    "diff_mean_befor_to_after = patrons_after_mean - patrons_befor_mean\n",
    "\n",
    "print(\"Mean Number of patrons PRIOR to BEFORE:                   {:5.1f}\".format(patrons_prior_mean))\n",
    "print(\"Increase of patrons from PRIOR mean to BEFORE mean        {:5.1f}\".format(diff_mean_prior_to_befor))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Mean Number of patrons BEFORE breakpoint:                 {:5.1f}\".format(patrons_befor_mean))\n",
    "print(\"Increase of patrons from BEFORE mean to AFTER mean        {:5.1f}\".format(diff_mean_befor_to_after))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Mean Number of patrons AFTER breakpoint:                  {:5.1f}\".format(patrons_after_mean))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Percentage of increase between BEFORE and AFTER: {:.1%}\".format((diff_mean_befor_to_after/diff_mean_prior_to_befor) - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example\n",
    "data = {'channel': [\"baba\", \"baba\", \"baba\", \"baba\", \"baba\", \"baba\", \"baba\", \"baba\"], 'time': [1, 2, 3, 4, 5, 6, 7, 8], 'patrons': [2,4,6,8,10,18,14,16]}\n",
    "df = pd.DataFrame(data)\n",
    "    \n",
    "breakpoint_date = 5\n",
    "    \n",
    "plt.plot(data['time'], data['patrons'], marker='o')\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"# patrons\")\n",
    "\n",
    "plt.axvline(breakpoint_date, color='red', linestyle='--', label='breakpoint', linewidth=2.5)\n",
    "plt.axvline(breakpoint_date-2, color='green', linestyle='--', label='-2 month', linewidth=2.5)\n",
    "plt.axvline(breakpoint_date+2, color='orange', linestyle='--', label='+ 2 month', linewidth=2.5)\n",
    "# plt.fill_between(df['time'], 4, 12, color='C0', alpha=0.3)\n",
    "\n",
    "plt.fill_betweenx(df['patrons'], breakpoint_date-2, breakpoint_date, facecolor='green', alpha=0.3)\n",
    "plt.fill_betweenx(df['patrons'], breakpoint_date+2, breakpoint_date, facecolor='orange', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Break detection toy example\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patrons_bkpoint = df[df['time'] == breakpoint_date].patrons.values[0] \n",
    "patrons_bkpoint_plus_1 = df[df['time'] == breakpoint_date+2].patrons.values[0] \n",
    "patrons_bkpoint_minu_1 = df[df['time'] == breakpoint_date-2].patrons.values[0] \n",
    "\n",
    "diff_after_break = patrons_bkpoint_plus_1 - patrons_bkpoint\n",
    "diff_before_break = patrons_bkpoint - patrons_bkpoint_minu_1\n",
    "\n",
    "print(\"Number of patrons 1 month before breakpoint:    {:2}\".format(patrons_bkpoint_minu_1))\n",
    "print(\"Increase of patrons 1 month before breakpoint:  {:2}\".format(diff_before_break))\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Number of patrons at breakpoint:                {:2}\".format(patrons_bkpoint))\n",
    "print(\"Increase of patrons 1 month after breakpoint:   {:2}\".format(diff_after_break))\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Number of patrons 1 month after breakpoint:     {:2}\".format(patrons_bkpoint_plus_1))\n",
    "print()\n",
    "print(\"change of increase between 1 month before and 1 month after: {:.1%}\".format((diff_after_break/diff_before_break) - 1))\n",
    "\n",
    "\n",
    "df_prior = df[(df['time'] >= breakpoint_date-4) & (df['time'] <= breakpoint_date-2)]\n",
    "display(df_prior)\n",
    "\n",
    "df_befor = df[(df['time'] >= breakpoint_date-2) & (df['time'] <= breakpoint_date)]\n",
    "display(df_befor)\n",
    "\n",
    "df_after = df[(df['time'] >= breakpoint_date) & (df['time'] <= breakpoint_date+2)]\n",
    "display(df_after)\n",
    "\n",
    "patrons_prior_mean = df_prior['patrons'].mean()\n",
    "patrons_befor_mean = df_befor['patrons'].mean()\n",
    "patrons_after_mean = df_after['patrons'].mean()\n",
    "\n",
    "diff_mean_prior_to_befor = patrons_befor_mean - patrons_prior_mean\n",
    "diff_mean_befor_to_after = patrons_after_mean - patrons_befor_mean\n",
    "\n",
    "print(\"Mean Number of patrons PRIOR to BEFORE:                   {:5.1f}\".format(patrons_prior_mean))\n",
    "print(\"Increase of patrons from PRIOR mean to BEFORE mean        {:5.1f}\".format(diff_mean_prior_to_befor))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Mean Number of patrons BEFORE breakpoint:                 {:5.1f}\".format(patrons_befor_mean))\n",
    "print(\"Increase of patrons from BEFORE mean to AFTER mean        {:5.1f}\".format(diff_mean_befor_to_after))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Mean Number of patrons AFTER breakpoint:                  {:5.1f}\".format(patrons_after_mean))\n",
    "print()\n",
    "\n",
    "print(\"Percentage of increase between BEFORE and AFTER: {:.1%}\".format((diff_mean_befor_to_after/diff_mean_prior_to_befor) - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Plots (II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Plot YouTube timeseries for channels matching top Patreon accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matching dataframe\n",
    "df_matched_channel_patreon = pd.read_csv(LOCAL_DATA_FOLDER+\"df_matched_channel_patreon.tsv.gz\", sep=\"\\t\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add patreon_id column to YT timeseries\n",
    "df_yt_timeseries_filt4_merged = df_yt_timeseries_filt4.merge(df_matched_channel_patreon, left_on='channel', right_on='channel_id')\n",
    "df_yt_timeseries_filt4_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter channels matching top patreon accounts\n",
    "# df_yt_timeseries_top_pt = df_yt_timeseries_filt4_merged[df_yt_timeseries_filt4_merged['patreon_id'].isin(top_patreons)]\n",
    "# top_yt_patreons = df_yt_timeseries_top_pt.patreon_id.unique()\n",
    "# top_yt_patreons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_ts_top_views = df_yt_timeseries_filt4_merged.groupby(['patreon_id', 'channel_id'])\\\n",
    "                                                     .agg(datetime_cnt=('datetime', 'count'),\n",
    "                                                          views_max=('views', 'max'),\n",
    "                                                          subs_date=('subs', 'max'),\n",
    "                                                          videos_max=('videos', 'mean'))\\\n",
    "                                                     .sort_values(by=['videos_max'], ascending=False)\\\n",
    "                                                     .reset_index()[:10]\n",
    "df_yt_ts_top_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 youtube channels in terms of views\n",
    "top_channel_ids = df_yt_ts_top_views['channel_id'].unique()\n",
    "top_channel_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot YT timeseries for top youtube accounts\n",
    "fig, axs = plt.subplots(int(math.ceil(len(top_channel_ids)/2)), 2, figsize=(12, len(top_channel_ids)*1.2), sharey=False, sharex=False)\n",
    "for idx, chan_id in enumerate(top_channel_ids):\n",
    "    row = math.floor(idx/2)\n",
    "    col = idx % 2\n",
    "    sbplt = axs[row, col]\n",
    "\n",
    "    tmp_df = df_yt_timeseries_filt4_merged[df_yt_timeseries_filt4_merged['channel_id'] == chan_id]\n",
    "\n",
    "    sbplt.plot(tmp_df['datetime'], tmp_df['views'])\n",
    "    sbplt.set(title=tmp_df['channel'].iloc[0])\n",
    "    sbplt.xaxis.set_major_locator(years)\n",
    "    sbplt.xaxis.set_major_formatter(years_fmt)\n",
    "    sbplt.xaxis.set_minor_locator(months)\n",
    "    \n",
    "    \n",
    "fig.suptitle(f'Timeseries of the YouTube channels with most views \\n (views per week)', fontweight=\"bold\")\n",
    "fig.text(0.5,0, 'Week')\n",
    "fig.text(0,0.5, 'Views', rotation = 90)\n",
    "fig.tight_layout(pad=3, w_pad=5, h_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.5.2 Plot Patreon Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-declare global variable for size of original GT dataset\n",
    "GT_final_processed_file_ROWS = 232_269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dailyGraph_earningsSeries.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file from disk\n",
    "df_dailyGraph_earningsSeries = pd.read_csv(LOCAL_DATA_FOLDER+\"/dailyGraph_earningsSeries.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_dailyGraph_earningsSeries.date = pd.to_datetime(df_dailyGraph_earningsSeries.date, unit='ms')\n",
    "df_dailyGraph_earningsSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restrict patreons corresponding to the top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP_CNT = 10\n",
    "# # group by patreon account\n",
    "# dailyGraph_grp_patreon = df_dailyGraph_earningsSeries.groupby('patreon')\\\n",
    "#                                                      .agg(date_cnt=('date', 'count'),\n",
    "#                                                           earliest_date=('date', 'min'),\n",
    "#                                                           lastest_date=('date', 'max'),\n",
    "#                                                           daily_earning_mean=('earning', 'mean'),\n",
    "#                                                           daily_earning_max=('earning', 'max'))\\\n",
    "#                                                      .sort_values(by=['daily_earning_max'], ascending=False)\\\n",
    "#                                                      .reset_index()\\\n",
    "#                                                      .round(2)\n",
    "\n",
    "# # remove hours from dates\n",
    "# dailyGraph_grp_patreon.earliest_date = dailyGraph_grp_patreon.earliest_date.dt.date\n",
    "# dailyGraph_grp_patreon.lastest_date = dailyGraph_grp_patreon.lastest_date.dt.date\n",
    "\n",
    "# dailyGraph_grp_patreon\n",
    "\n",
    "# # extract the top 10 most profitable patreon accounts\n",
    "# top_patreons = dailyGraph_grp_patreon[:TOP_CNT]['patreon']\n",
    "\n",
    "# print(\"[Graphtreon Timeseries] Total number of patreon ids (original file):            {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "# print(\"[Graphtreon Timeseries] Nb of patreon ids in dailyGraph earnings time series:   {:>9,} ({:.1%} of original dataset)\".format(len(dailyGraph_grp_patreon), len(dailyGraph_grp_patreon)/GT_final_processed_file_ROWS))\n",
    "\n",
    "# print()\n",
    "\n",
    "# dailyGraph_grp_patreon[:TOP_CNT].style.set_caption(f\"Top {TOP_CNT} highest-earning Patreon accounts (sorted by max daily earnings)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to get the corresponding channel_id\n",
    "df_dailyGraph_earningsSeries_merged = df_dailyGraph_earningsSeries.merge(df_matched_channel_patreon, left_on='patreon', right_on='patreon_id')\n",
    "df_dailyGraph_earningsSeries_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_channel_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dailyGraph_earningsSeries_merged.groupby(['channel_id', 'patreon_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Patreon daily earningsSeriesData for top patreon accounts\n",
    "fig, axs = plt.subplots(int(len(top_channel_ids)/2), 2, figsize=(12, len(top_channel_ids)*1.2), sharey=False, sharex=False)\n",
    "for idx, chan_id in enumerate(top_channel_ids):\n",
    "    row = math.floor(idx/2)\n",
    "    col = idx % 2\n",
    "    sbplt = axs[row, col]\n",
    "\n",
    "    tmp_df = df_dailyGraph_earningsSeries_merged[df_dailyGraph_earningsSeries_merged['channel_id'] == chan_id]\n",
    "    display(tmp_df)\n",
    "    sbplt.plot(tmp_df['date'], tmp_df['earning'])\n",
    "    sbplt.set(title=chan_id)\n",
    "    sbplt.xaxis.set_major_locator(years)\n",
    "    sbplt.xaxis.set_major_formatter(years_fmt)\n",
    "    sbplt.xaxis.set_minor_locator(months)\n",
    "    \n",
    "    \n",
    "fig.suptitle(f'Timeseries of the Patreon accounts corresponding to YT channel with most views\\n (earnings per day in dollars)', fontweight=\"bold\")\n",
    "fig.text(0.5,0, 'Day')\n",
    "fig.text(0,0.5, 'Earnings per day ($)', rotation = 90)\n",
    "fig.tight_layout(pad=3, w_pad=5, h_pad=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analyse 1 account in detail\n",
    "# patreon_account = 'patreon.com/tinymeatgang'\n",
    "\n",
    "# with pd.option_context('display.max_rows', 90, 'display.min_rows', 90):\n",
    "#     display(df_top_pt_daily_earnings[(df_top_pt_daily_earnings['patreon'] == patreon_account) \n",
    "#                                      # & (df_top_pt_daily_earnings['date'] > pd.Timestamp('2021-01-01'))\n",
    "#                                     ].head(90))\n",
    "        \n",
    "# df_top_pt_daily_earnings.dtypes\n",
    "\n",
    "# # check for NaN values\n",
    "# df_top_pt_daily_earnings[df_top_pt_daily_earnings.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3 Compare YouTube and top Patreon timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare YouTube and Patreon timeseries for top patreon accounts\n",
    "for idx, patreon in enumerate(top_patreons):\n",
    "    \n",
    "    fig, axs = plt.subplots(4, 1, figsize=(6, 8), sharey=False, sharex=True)\n",
    "\n",
    "    # patreon earnings\n",
    "    tmp_df_pt = df_top_pt_daily_earnings[df_top_pt_daily_earnings['patreon'] == patreon]\n",
    "\n",
    "    axs[0].plot(tmp_df_pt['date'], tmp_df_pt['earning'])\n",
    "    axs[0].set(title=\"Patreon earnings\")\n",
    "    axs[0].set_ylabel(\"Earnings ($)\")\n",
    "    \n",
    "    axs[0].xaxis.set_major_locator(years)\n",
    "    axs[0].xaxis.set_major_formatter(years_fmt)\n",
    "    axs[0].xaxis.set_minor_locator(months)\n",
    "    \n",
    "\n",
    "    # youtube views\n",
    "    tmp_df_yt = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon]\n",
    "\n",
    "    axs[1].plot(tmp_df_yt['datetime'], tmp_df_yt['views'])\n",
    "    axs[1].set(title=\"YouTube views\")\n",
    "    axs[1].set_ylabel(\"Views\")\n",
    "\n",
    "    axs[1].xaxis.set_major_locator(years)\n",
    "    axs[1].xaxis.set_major_formatter(years_fmt)\n",
    "    axs[1].xaxis.set_minor_locator(months)\n",
    "    \n",
    "    \n",
    "    # youtube subs\n",
    "    tmp_df3 = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon]\n",
    "\n",
    "    axs[2].plot(tmp_df3['datetime'], tmp_df3['subs'])\n",
    "    axs[2].set(title=\"YouTube subscriptions\")\n",
    "    axs[2].set_ylabel(\"Subscriptions\")\n",
    "\n",
    "    axs[2].xaxis.set_major_locator(years)\n",
    "    axs[2].xaxis.set_major_formatter(years_fmt)\n",
    "    axs[2].xaxis.set_minor_locator(months)\n",
    "    \n",
    "    \n",
    "    # youtube videos\n",
    "    tmp_df4 = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon]\n",
    "\n",
    "    axs[3].plot(tmp_df4['datetime'], tmp_df4['videos'])\n",
    "    axs[3].set(title=\"YouTube videos\")\n",
    "    axs[3].set_ylabel(\"Videos\")\n",
    "\n",
    "    axs[3].xaxis.set_major_locator(years)\n",
    "    axs[3].xaxis.set_major_formatter(years_fmt)\n",
    "    axs[3].xaxis.set_minor_locator(months)\n",
    "    \n",
    "    ch_id = tmp_df_yt['channel'].unique()[0]\n",
    "    fig.suptitle(f'{patreon}\\nchannel id(s): {ch_id}', fontweight=\"bold\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
