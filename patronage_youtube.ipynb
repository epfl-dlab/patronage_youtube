{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Characterizing Patronage on YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libaries imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from tqdm.notebook import tqdm\n",
    "from tableone import TableOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths to data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder paths\n",
    "DATA_FOLDER = \"/dlabdata1/youtube_large/\"\n",
    "LOCAL_DATA_FOLDER = \"local_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Original datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the list of the original YouNiverse and Graphteon datasets:\n",
    "\n",
    "**YouNiverse dataset:**\n",
    "\n",
    "- `df_channels_en.tsv.gz`: channel metadata.\n",
    "- `df_timeseries_en.tsv.gz`: channel-level time-series.\n",
    "- `yt_metadata_en.jsonl.gz`: raw video metadata.\n",
    "\n",
    "**Graphteon dataset:**\n",
    "- `final_processed_file.jsonl.gz` all graphteon time-series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "#### Preprocessed datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing the above datasets in the _preprocessing_ notebook, we obtain the following preprocessed datasets, which we'll use in this notebook:\n",
    "\n",
    "**YouNiverse dataset:**\n",
    "\n",
    "- `df_yt_timeseries_restricted.tsv.gz`: channel-level time-series after having applied 4 filters:\n",
    "    - _YouTube channels that are linked to a Patreon account_; _at least two years between first and last video_; _at least 20 videos with Patreon ids_; and _at least 250k subscribers at data crawling time_\n",
    "- `yt_metadata_en.jsonl.gz`: raw video metadata restricted to videos referencing Patreon accounts, and channels that exist in the restricted YouTube time series\n",
    "\n",
    "**Graphteon dataset:**\n",
    "- `dailyGraph_patrons_and_earnings_Series.tsv.gz` extracted patrons and earnings time series.\n",
    "\n",
    "**New dataset:**\n",
    "- `df_linked_channels_patreons`: matches exactly 1 Patreon id to 1 YouTube channel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1. Load YouTube metadata\n",
    "_In the preprocessing phase, we extracted Patreon urls from YouTube metadata description (if they existed) and kept only those rows_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YT metadata containing patreon ids in description\n",
    "!ls -lh {LOCAL_DATA_FOLDER}yt_metadata_en_restricted.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filtered and restricted youtube metadata file (takes about 10 seconds)\n",
    "yt_metadata_en_restricted = pd.read_csv(LOCAL_DATA_FOLDER+\"yt_metadata_en_restricted.tsv.gz\", sep=\"\\t\", lineterminator='\\n', compression='gzip') \n",
    "yt_metadata_en_restricted['crawl_date'] = pd.to_datetime(yt_metadata_en_restricted['crawl_date'])\n",
    "yt_metadata_en_restricted['upload_date'] = pd.to_datetime(yt_metadata_en_restricted['upload_date'])\n",
    "yt_metadata_en_restricted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTube Metadata statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original YT dataset\n",
    "DF_YT_METADATA_ROWS = 72_924_794\n",
    "\n",
    "# stats \n",
    "print(\"Videos:\")\n",
    "print(\"[Original YouTube metadata] Total number of videos:                                             {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "print(\"[Filtered YouTube metadata] Number of videos that contain a valid patreon link in description \\n \\\n",
    "                           and match the channels of the restricted timeseries:                {:>10,} ({:.1%} of total dataset)\".format(len(yt_metadata_en_restricted), len(yt_metadata_en_restricted)/DF_YT_METADATA_ROWS))\n",
    "\n",
    "\n",
    "# get list of all unique patreon ids in yt_metadata_en_restricted\n",
    "yt_patreon_list = yt_metadata_en_restricted['patreon_id'].unique()\n",
    "yt_pt_channel_list = yt_metadata_en_restricted['channel_id'].unique()\n",
    "print(\"\\nChannels:\")\n",
    "print(\"[Filtered YouTube metadata] Total number of unique patreon ids:                                 {:>9,}\".format(len(yt_patreon_list)))\n",
    "print(\"[Filtered YouTube metadata] Number of unique channels that contain a valid patreon account:     {:>9,}\".format(len(yt_pt_channel_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 Load \"Link\" dataframe (channel/patreon)\n",
    "This dataframe gives the correspondence between `channel_id` and `patreon_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_linked_channels_patreons.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linked_channels_patreons = pd.read_csv(LOCAL_DATA_FOLDER+\"df_linked_channels_patreons.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\n",
    "df_linked_channels_patreons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of linked patreon_ids and channel_ids: {len(df_linked_channels_patreons):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load YouTube timeseries\n",
    "_In the preprocessing phase, the YouTube channels have been restricted / filtered according to the following criteria (filters were applied sequentially):_\n",
    "- Filter 1: Keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account \n",
    "- Filter 2: At least 2 year between first and last video\n",
    "- Filter 3: At least 20 videos with patreon ids\n",
    "- Filter 4: At least 250k subscribers at data crawling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_yt_timeseries_restricted.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load restricted channel-level time-series.\n",
    "df_yt_timeseries_restricted = pd.read_csv(LOCAL_DATA_FOLDER+'df_yt_timeseries_restricted.tsv.gz', sep=\"\\t\", compression='gzip', parse_dates=['datetime'])\n",
    "\n",
    "# replace dates that were collected after 23:00 to their next day, and remove hour (+ backup original column)\n",
    "df_yt_timeseries_restricted['datetime'] = df_yt_timeseries_restricted['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "\n",
    "# remove hours and convert to datetime type\n",
    "df_yt_timeseries_restricted['datetime'] = pd.to_datetime(df_yt_timeseries_restricted['datetime'].dt.date)\n",
    "\n",
    "# add patreon_id column to YT timeseries\n",
    "df_yt_timeseries_restricted_merged = df_yt_timeseries_restricted.merge(df_linked_channels_patreons, left_on='channel', right_on='channel_id')\n",
    "\n",
    "df_yt_timeseries_restricted_merged = df_yt_timeseries_restricted_merged.drop(columns=['channel_id'])\n",
    "df_yt_timeseries_restricted_merged.insert(1, 'patreon_id', df_yt_timeseries_restricted_merged.pop('patreon_id'))\n",
    "df_yt_timeseries_restricted_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTube timeseries statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original YT dataset\n",
    "DF_YT_TIMESERIES_EN_ROWS = 18_872_499\n",
    "DF_YT_TIMESERIES_EN_UNIQUE_CHANNELS = 133_516\n",
    "\n",
    "chan_list_restricted = df_yt_timeseries_restricted['channel'].unique()\n",
    "chan_list_restricted_cnt = len(chan_list_restricted)\n",
    "\n",
    "# Nb of channels of original YT timeseries dataset\n",
    "print(\"[YouTube Timeseries] Number of data points in original dataset:           {:>10,}\".format(DF_YT_TIMESERIES_EN_ROWS))\n",
    "print(\"[YouTube Timeseries] Number of data points after applying filters:        {:>10,} ({:5.1%} of original dataset)\".format(len(df_yt_timeseries_restricted), len(df_yt_timeseries_restricted)/DF_YT_TIMESERIES_EN_ROWS))\n",
    "print()\n",
    "print(\"[YouTube Timeseries] Number of channels of original dataset:              {:>10,}\".format(DF_YT_TIMESERIES_EN_UNIQUE_CHANNELS))\n",
    "print(\"[YouTube Timeseries] Number of channels after applying filters:           {:>10,} ({:5.1%} of original dataset)\".format(chan_list_restricted_cnt, chan_list_restricted_cnt/DF_YT_TIMESERIES_EN_UNIQUE_CHANNELS))\n",
    "print()\n",
    "print('[YouTube Timeseries] Time range after applying filters:               {} and {}'.format(df_yt_timeseries_restricted['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                              df_yt_timeseries_restricted['datetime'].max().strftime('%B %d, %Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4 Load Graphtreon dataset\n",
    "From the original dataset:\n",
    "- Find corresponding YouTube channel for each patreon (lookup in linked patreon/channel dataset)\n",
    "- If patreon_id exist in patreons corresponding to the restricted list of channels in YT time series\n",
    "    - we extract the date and earnings from “dailyGraph_earningsSeriesData” (see preprocessing notebook\n",
    "    - we extract the date and patrons from “dailyGraph_patronSeriesData” (see preprocessing notebook)\n",
    "    - we merge those two time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dailyGraph_patrons_and_earnings_Series.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read merged dailyGraph_patrons_and_earnings_Series from disk\n",
    "df_dailyGraph_patrons_and_earnings_Series = pd.read_csv(LOCAL_DATA_FOLDER+\"dailyGraph_patrons_and_earnings_Series.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_dailyGraph_patrons_and_earnings_Series['date'] = pd.to_datetime(df_dailyGraph_patrons_and_earnings_Series['date'], unit='ms')\n",
    "df_dailyGraph_patrons_and_earnings_Series['patrons'] = df_dailyGraph_patrons_and_earnings_Series['patrons'].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove PT accounts that aren't in linked df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dailyGraph_patrons_and_earnings_Series = df_dailyGraph_patrons_and_earnings_Series[df_dailyGraph_patrons_and_earnings_Series['patreon'].isin(df_linked_channels_patreons['patreon_id'])]\n",
    "df_dailyGraph_patrons_and_earnings_Series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphtreon timeseries statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-declare global variable for size of original GT dataset\n",
    "GT_final_processed_file_ROWS = 232_269\n",
    "\n",
    "# group by patreon account, sort by max number of patrons\n",
    "dailyGraph_grp_patreon = df_dailyGraph_patrons_and_earnings_Series.groupby('patreon').agg(date_cnt=('date', 'count'), earliest_date=('date', 'min'), lastest_date=('date', 'max'),daily_earning_min=('earning', 'min'),daily_earning_max=('earning', 'max'), daily_earning_mean=('earning', 'mean'), daily_patrons_min=('patrons', 'min'), daily_patrons_max=('patrons', 'max'), daily_patrons_mean=('patrons', 'mean')).sort_values(by=['daily_patrons_max'], ascending=False).round(2).reset_index()\n",
    "\n",
    "print(\"[Graphtreon Timeseries] Number of patreon ids in original dataset:                                            {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Number of patreon ids in dailyGraph patreon + earnings time series + linked df:       {:>9,} ({:.1%} of original dataset)\".format(df_dailyGraph_patrons_and_earnings_Series.patreon.nunique(), df_dailyGraph_patrons_and_earnings_Series.patreon.nunique()/GT_final_processed_file_ROWS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['daily_patrons_mean', 'daily_earning_mean']\n",
    "\n",
    "# plot with log scale for x axis \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.histplot(data=dailyGraph_grp_patreon[col], stat='count', ax=ax, bins=50, kde=False, cumulative=False, color=f'C{i}')\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(\"Number of patron accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot with log scale for x axis \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n",
    "xlabels = [r'$\\log_{10}($daily_patrons_mean$)$', r'$\\log_{10}($daily_earning_mean$)$']\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.histplot(data=np.log10(dailyGraph_grp_patreon[col]), stat='count', ax=ax, bins=50, kde=False, cumulative=False, color=f'C{i}')\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_xlabel(xlabels[i])\n",
    "    ax.set_ylabel(\"Number of patron accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "dailyGraph_grp_patreon.describe()[['daily_patrons_mean', 'daily_earning_mean']].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Select top patreon accounts (> 200 patrons)\n",
    "Restrict patreons accounts with mean number of patrons > 200 patrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_MAX_PATRONS = 200\n",
    "\n",
    "# remove patrons accounts that have less than MIN_MAX_PATRONS patrons average\n",
    "top_patreons_df = dailyGraph_grp_patreon[dailyGraph_grp_patreon['daily_patrons_max'] > MIN_MAX_PATRONS]\n",
    "top_patreons = top_patreons_df['patreon']\n",
    "df_top_pt_accts = df_dailyGraph_patrons_and_earnings_Series[df_dailyGraph_patrons_and_earnings_Series['patreon'].isin(top_patreons)]\n",
    "\n",
    "print(\"[Graphtreon Timeseries] Filtered \\\"top\\\" patrons (> {} patrons):  {:>9,} ({:.1%} of original dataset)\".format(MIN_MAX_PATRONS, len(top_patreons), len(top_patreons)/GT_final_processed_file_ROWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter YT timeseries channels matching top patreon accounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter YT channels matching top patreon accounts\n",
    "df_yt_timeseries_top_pt = df_yt_timeseries_restricted_merged[df_yt_timeseries_restricted_merged['patreon_id'].isin(top_patreons)].copy()\n",
    "\n",
    "print(f\"Number of YouTube channels before filtering by top patreon accounts: {len(df_yt_timeseries_restricted_merged.patreon_id.unique()):>5}\" )\n",
    "print(f\"Number of YouTube channels after  filtering by top patreon accounts: {df_yt_timeseries_top_pt.patreon_id.nunique():>5}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter YT metadata channels matching top Patreon accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter accounts that match selected Patreon ids\n",
    "df_yt_metadata_pt_filtered = yt_metadata_en_restricted[yt_metadata_en_restricted['patreon_id'].isin(top_patreons)].copy()\n",
    "print(f'Nuber of YouTube metadata videos that match top Patreon ids: {len(df_yt_metadata_pt_filtered):,} ({len(df_yt_metadata_pt_filtered)/len(yt_metadata_en_restricted):.1%} of restricted videos) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Jointly examine time series related to patronage and YouTube statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Detecting bursts (breakpoints) of incoming patrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_breakpoint_v3(df, column, ratio_threshold):\n",
    "    \"\"\"\n",
    "    Scan column of the dataframe until it finds a breakpoint (= increase larger than threshold)\n",
    "    \n",
    "    :param df: dataframe\n",
    "    :param column: column to scan\n",
    "    :param ratio_threshold: minimum increase ratio threshold \n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    df_len = len(df)\n",
    "    moving_avg_half = 15\n",
    "\n",
    "    # scan dataset for increase larger than threshold\n",
    "    for date_index, row in ts_pt_df.iterrows():\n",
    "        if (i >= (30 + moving_avg_half) and i < df_len-90):\n",
    "            sub30 = df.iloc[i-30][column]\n",
    "            point = df.iloc[i][column]\n",
    "            add30 = df.iloc[i+30][column]\n",
    "            add60 = df.iloc[i+60][column]\n",
    "            add90 = df.iloc[i+90][column]\n",
    "\n",
    "            d1 = point - sub30\n",
    "            d2 = add30 - point            \n",
    "            d3 = add60 - add30            \n",
    "            d4 = add90 - add60            \n",
    "            \n",
    "            # avoid  weird ratios obtained by diving by a difference between -1 and 1 \n",
    "            if (0 <= d1 < 1):\n",
    "                d1 = 1\n",
    "            elif (-1 < d1 < 0):\n",
    "                d1 = -1\n",
    "        \n",
    "            r_d1_d2 = d2 / d1\n",
    "\n",
    "            # at least 10 patrons in the prior period\n",
    "            if (d1 > 10) & (d2 > d1) & (r_d1_d2 >= ratio_threshold):\n",
    "                bkpnt_dict = {\n",
    "                    \"bkpt_date\"         : df.iloc[i]['date'],\n",
    "                    \"bkpt_date_sub30\"   : df.iloc[i-30]['date'],\n",
    "                    \"bkpt_date_add30\"   : df.iloc[i+30]['date'],\n",
    "                    \"bkpt_date_add60\"   : df.iloc[i+60]['date'],\n",
    "                    \"bkpt_date_add90\"   : df.iloc[i+90]['date'],\n",
    "                    \"avg_patrons_bkpnt\" : point,\n",
    "                    \"avg_patrons_sub30\" : sub30,\n",
    "                    \"avg_patrons_add30\" : add30,\n",
    "                    \"avg_patrons_add60\" : add60,\n",
    "                    \"avg_patrons_add90\" : add90,\n",
    "                    \"d1\"                : d1,\n",
    "                    \"d2\"                : d2,\n",
    "                    \"d3\"                : d3,\n",
    "                    \"d4\"                : d4,\n",
    "                    \"r_d1_d2\"           : r_d1_d2\n",
    "                }\n",
    "                \n",
    "                return bkpnt_dict\n",
    "        i = i + 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_acct_and_sort_df(df, patreon_col, patreon_id, date_col):\n",
    "    \"\"\"\n",
    "    Restrict time series to 1 Patreon account only, drop duplicate rows, and sort by ascending dates\n",
    "    :param df: Time series dataframe with multiple Patreon accounts\n",
    "    :param patreon_col: Name of column storing Patreon ids\n",
    "    :param patreon_id: Patreon id we want to keep\n",
    "    :param date_col: Date column of the dataframe\n",
    "    :return: Time series with rows corresponding to selected patreon_id only\n",
    "    \"\"\"\n",
    "    restr_df = df[df[patreon_col] == patreon_id].copy()\n",
    "    restr_df = restr_df.sort_values(by=[date_col])\n",
    "    restr_df = restr_df.drop_duplicates()\n",
    "    return restr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_to_overlapping_dates(df1, df2, df1_date_col, df2_date_col):\n",
    "    \"\"\"\n",
    "    Restrict two dataframes to their overlapping dates only\n",
    "    \n",
    "    :param df1: Patreon time series\n",
    "    :param df2: YouTube time series\n",
    "    :param df1_date_col: Patreon time series date column name\n",
    "    :param df2_date_col: YouTube time series date column name\n",
    "    :return: df1 and df2 restricted to overlapping dates only and raise Exception if none exist\n",
    "    \"\"\"\n",
    "\n",
    "   # find min and max dates of both dataframes   \n",
    "    date_min = max([df1[df1_date_col].min(), df2[df2_date_col].min()])\n",
    "    date_max = min([df1[df1_date_col].max(), df2[df2_date_col].max()])\n",
    "    \n",
    "    # if no overlap period between YT and Patreon datasets, raise exception\n",
    "    if date_max < date_min:\n",
    "        raise Exception(f\"No overlapping period between timeseries\")\n",
    "\n",
    "    # restrict datasets between min and max dates\n",
    "    df1 = df1[(df1[df1_date_col] >= date_min) & (df1[df1_date_col] <= date_max)]\n",
    "    df2 = df2[(df2[df2_date_col] >= date_min) & (df2[df2_date_col] <= date_max)]\n",
    "    \n",
    "    # align both dataframes since youtube starts once a week\n",
    "    df1 = df1[df1[df1_date_col] >= df2[df2_date_col].min()]\n",
    "    \n",
    "    return df1, df2, date_min, date_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find breakpoints for top PT account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCR_RATIO_THRESH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Find breakpoints and store Patreon breakpoints + PT and YT statistics in \"df_treated\"\n",
    "\n",
    "# # variables declaration\n",
    "# ROLLING_AVG_WINDOW = 30\n",
    "# treated_tuples = []\n",
    "# no_bkpnt_cnt = 0\n",
    "# no_overlap = 0\n",
    "\n",
    "\n",
    "\n",
    "# print(f'Find breakpoints for {len(top_patreons)} patreon accounts...')\n",
    "\n",
    "# # LOOP OVER TOP PATREON ACCOUNTS\n",
    "# for idx, patreon in enumerate(tqdm(top_patreons)):\n",
    "#     treat = None\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#     ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "#     # patreon earnings and users\n",
    "#     tmp_df_pt = restrict_acct_and_sort_df(df_top_pt_accts, 'patreon', patreon, 'date')\n",
    "\n",
    "#     # youtube videos\n",
    "#     tmp_df_yt = restrict_acct_and_sort_df(df_yt_timeseries_top_pt, 'patreon_id', patreon, 'datetime')\n",
    "\n",
    "#     # youtube metadata\n",
    "#     tmp_df_yt_meta = restrict_acct_and_sort_df(df_yt_metadata_pt_filtered, 'patreon_id', patreon, 'upload_date')\n",
    "    \n",
    "    \n",
    "#     ########################## RESTRICT PATREON AND YOUTUBE TIME SERIES TO OVERLAPPING DATES ##########################\n",
    "    \n",
    "#    # if no overlap period between YT and Patreon datasets, raise exception and skip account\n",
    "#     try: \n",
    "#         tmp_df_pt, tmp_df_yt, date_min, date_max = restrict_to_overlapping_dates(tmp_df_pt, tmp_df_yt, 'date', 'datetime')\n",
    "#     except Exception as e: \n",
    "#         print(f\"Exception: {patreon} and {yt_channel_id}: {e} --> skipping account\")        \n",
    "#         no_overlap += 1\n",
    "#         continue\n",
    "    \n",
    "#     ########################## PRINT TITLES ##########################\n",
    "#     # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "#     yt_channel_id = tmp_df_yt['channel'].unique()[0]\n",
    "#     # print(f\"\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "    \n",
    "    \n",
    "#     ########################## PATREON: CALCULATE MOVING AVERAGE ##########################\n",
    "    \n",
    "#     tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "#     tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "#     ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "\n",
    "    \n",
    "#     ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "#     bkpnt_dict = find_breakpoint_v3(tmp_df_pt, 'patrons_ma', INCR_RATIO_THRESH)\n",
    "#     # print(\"bkpnt_dict: \", bkpnt_dict)\n",
    "    \n",
    "#     if bkpnt_dict == None:\n",
    "#         no_bkpnt_cnt += 1        \n",
    "#         # print(\"No breakpoint for this account...\")\n",
    "#         continue\n",
    "#     else: \n",
    "#         treat = 1\n",
    "\n",
    "\n",
    "    \n",
    "#     ########################## PATREON: CALCULATE WEEKLY DELTAS ##########################\n",
    "\n",
    "#     # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "#     ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "#     ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "#     ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "#     ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "#     tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "#     # reorder columns to have deltas columns next to their respective columns\n",
    "#     patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "#     ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "#     # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "#     ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "    \n",
    "    \n",
    "#     ################################### CALCULATE MEANS OF DELTA VALUES BEFORE AND AFTER BKPOINT ###################################  \n",
    "\n",
    "\n",
    "#     ##### PATREON #####\n",
    "#     tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_sub30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date'])]\n",
    "#     tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add30'])]\n",
    "#     tmp_df_PT_add60 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_add30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add60'])]\n",
    "#     tmp_df_PT_add90 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_add60']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add90'])]\n",
    "\n",
    "#     # delta patrons\n",
    "#     mean_delta_patrons_sub30 = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "#     mean_delta_patrons_add30 = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "#     mean_delta_patrons_add60 = tmp_df_PT_add60['delta_patrons'].mean()\n",
    "#     mean_delta_patrons_add90 = tmp_df_PT_add90['delta_patrons'].mean()\n",
    "        \n",
    "#     # delta earnings\n",
    "#     mean_delta_earnings_sub30 = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "#     mean_delta_earnings_add30 = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "#     mean_delta_earnings_add60 = tmp_df_PT_add60['delta_earning'].mean()  \n",
    "#     mean_delta_earnings_add90 = tmp_df_PT_add90['delta_earning'].mean()  \n",
    "\n",
    "    \n",
    "#     ##### YOUTUBE TIME SERIES #####\n",
    "#     tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date']      )]\n",
    "#     tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "#     tmp_df_YT_add60 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_add30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add60'])]\n",
    "#     tmp_df_YT_add90 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_add60']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add90'])]\n",
    "    \n",
    "#     # delta videos\n",
    "#     mean_delta_videos_sub30 = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "#     mean_delta_videos_add30 = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "#     mean_delta_videos_add60 = tmp_df_YT_add60['delta_videos'].mean()  \n",
    "#     mean_delta_videos_add90 = tmp_df_YT_add90['delta_videos'].mean()  \n",
    "\n",
    "#     # delta views\n",
    "#     mean_delta_views_sub30 = tmp_df_YT_sub30['delta_views'].mean()\n",
    "#     mean_delta_views_add30 = tmp_df_YT_add30['delta_views'].mean()  \n",
    "#     mean_delta_views_add60 = tmp_df_YT_add60['delta_views'].mean()  \n",
    "#     mean_delta_views_add90 = tmp_df_YT_add90['delta_views'].mean()  \n",
    "\n",
    "#     # delta subscriptions\n",
    "#     mean_delta_subs_sub30 = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "#     mean_delta_subs_add30 = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "#     mean_delta_subs_add60 = tmp_df_YT_add60['delta_subs'].mean()  \n",
    "#     mean_delta_subs_add90 = tmp_df_YT_add90['delta_subs'].mean()  \n",
    "\n",
    "    \n",
    "#     ##### YOUTUBE METADATA #####\n",
    "#     tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date']      )]\n",
    "#     tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "#     tmp_df_YT_META_add60 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_add30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add60'])]\n",
    "#     tmp_df_YT_META_add90 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_add60']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add90'])]\n",
    "        \n",
    "#     # durations\n",
    "#     mean_duration_sub30 = tmp_df_YT_META_sub30['duration'].mean()\n",
    "#     mean_duration_add30 = tmp_df_YT_META_add30['duration'].mean()      \n",
    "#     mean_duration_add60 = tmp_df_YT_META_add60['duration'].mean()      \n",
    "#     mean_duration_add90 = tmp_df_YT_META_add90['duration'].mean()      \n",
    "        \n",
    "#     # likes\n",
    "#     mean_likes_sub30 = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "#     mean_likes_add30 = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "#     mean_likes_add60 = tmp_df_YT_META_add60['like_count'].mean()      \n",
    "#     mean_likes_add90 = tmp_df_YT_META_add90['like_count'].mean()      \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "#     treated_tuples.append(\n",
    "#         (          \n",
    "#             patreon, \n",
    "#             yt_channel_id,   \n",
    "#             treat,\n",
    "#             bkpnt_dict[\"d1\"], \n",
    "#             bkpnt_dict[\"d2\"], \n",
    "#             bkpnt_dict[\"d3\"], \n",
    "#             bkpnt_dict[\"d4\"], \n",
    "            \n",
    "#             bkpnt_dict[\"r_d1_d2\"],\n",
    "            \n",
    "#             bkpnt_dict[\"bkpt_date\"], \n",
    "#             bkpnt_dict[\"bkpt_date_sub30\"], \n",
    "#             bkpnt_dict[\"bkpt_date_add30\"],\n",
    "#             bkpnt_dict[\"bkpt_date_add60\"],\n",
    "#             bkpnt_dict[\"bkpt_date_add90\"],\n",
    "            \n",
    "#             bkpnt_dict[\"avg_patrons_bkpnt\"], \n",
    "#             bkpnt_dict[\"avg_patrons_sub30\"], \n",
    "#             bkpnt_dict[\"avg_patrons_add30\"], \n",
    "#             bkpnt_dict[\"avg_patrons_add60\"], \n",
    "#             bkpnt_dict[\"avg_patrons_add90\"], \n",
    "            \n",
    "#             # delta patrons\n",
    "#             mean_delta_patrons_sub30,\n",
    "#             mean_delta_patrons_add30,\n",
    "#             mean_delta_patrons_add60,\n",
    "#             mean_delta_patrons_add90,\n",
    "\n",
    "#             # delta earnings\n",
    "#             mean_delta_earnings_sub30,\n",
    "#             mean_delta_earnings_add30,\n",
    "#             mean_delta_earnings_add60, \n",
    "#             mean_delta_earnings_add90,\n",
    "\n",
    "#             # delta videos\n",
    "#             mean_delta_videos_sub30,\n",
    "#             mean_delta_videos_add30,\n",
    "#             mean_delta_videos_add60,\n",
    "#             mean_delta_videos_add90,\n",
    "\n",
    "#             # delta views\n",
    "#             mean_delta_views_sub30,\n",
    "#             mean_delta_views_add30,\n",
    "#             mean_delta_views_add60,\n",
    "#             mean_delta_views_add90,\n",
    "\n",
    "#             # delta subscriptions\n",
    "#             mean_delta_subs_sub30,\n",
    "#             mean_delta_subs_add30,\n",
    "#             mean_delta_subs_add60,\n",
    "#             mean_delta_subs_add90,\n",
    "\n",
    "#             # durations\n",
    "#             mean_duration_sub30,\n",
    "#             mean_duration_add30,     \n",
    "#             mean_duration_add60,    \n",
    "#             mean_duration_add90, \n",
    "\n",
    "#             # likes\n",
    "#             mean_likes_sub30,\n",
    "#             mean_likes_add30,  \n",
    "#             mean_likes_add60,   \n",
    "#             mean_likes_add90\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "            \n",
    "# df_treated = pd.DataFrame(treated_tuples, columns = [\n",
    "#     'patreon_id',\n",
    "#     'yt_channel_id',\n",
    "#     'treat',\n",
    "#     'd1', \n",
    "#     'd2', \n",
    "#     'd3',\n",
    "#     'd4',\n",
    "    \n",
    "#     'ratio_d1_d2',\n",
    "#     'bkpt_date',     \n",
    "#     'bkpt_date_sub30', \n",
    "#     'bkpt_date_add30', \n",
    "#     'bkpt_date_add60',\n",
    "#     'bkpt_date_add90',\n",
    "    \n",
    "#     'avg_patrons_bkpnt', \n",
    "#     'avg_patrons_sub30', \n",
    "#     'avg_patrons_add30', \n",
    "#     'avg_patrons_add60',\n",
    "#     'avg_patrons_add90',\n",
    "    \n",
    "#     # delta patrons\n",
    "#     'mean_delta_patrons_sub30',\n",
    "#     'mean_delta_patrons_add30',\n",
    "#     'mean_delta_patrons_add60',\n",
    "#     'mean_delta_patrons_add90',\n",
    "\n",
    "#     # delta earnings\n",
    "#     'mean_delta_earnings_sub30',\n",
    "#     'mean_delta_earnings_add30',\n",
    "#     'mean_delta_earnings_add60',\n",
    "#     'mean_delta_earnings_add90',\n",
    "\n",
    "#     # delta videos\n",
    "#     'mean_delta_videos_sub30',\n",
    "#     'mean_delta_videos_add30',\n",
    "#     'mean_delta_videos_add60',\n",
    "#     'mean_delta_videos_add90',\n",
    "\n",
    "#     # delta views\n",
    "#     'mean_delta_views_sub30',\n",
    "#     'mean_delta_views_add30',\n",
    "#     'mean_delta_views_add60',\n",
    "#     'mean_delta_views_add90',\n",
    "\n",
    "#     # delta subscriptions\n",
    "#     'mean_delta_subs_sub30',\n",
    "#     'mean_delta_subs_add30',\n",
    "#     'mean_delta_subs_add60',\n",
    "#     'mean_delta_subs_add90',\n",
    "#     # durations\n",
    "#     'mean_duration_sub30',\n",
    "#     'mean_duration_add30',\n",
    "#     'mean_duration_add60',\n",
    "#     'mean_duration_add90',\n",
    "\n",
    "#     # likes\n",
    "#     'mean_likes_sub30',\n",
    "#     'mean_likes_add30',\n",
    "#     'mean_likes_add60',\n",
    "#     'mean_likes_add90'\n",
    "# ])\n",
    "\n",
    "\n",
    "# print(f'Patreon accounts added to the treated group (increase ratio >= {INCR_RATIO_THRESH}):                      {len(df_treated):>3}')\n",
    "# print(f'Patreon accounts with no breakpoints found:                                             {no_bkpnt_cnt:>3}')\n",
    "# print(f'Patreon accounts with no overlapping period between YouTube and Patreon datasets found: {no_overlap:>3}')\n",
    "\n",
    "\n",
    "# df_treated['bkpt_date'] = pd.to_datetime(df_treated['bkpt_date'])\n",
    "# df_treated['bkpt_date_sub30'] = pd.to_datetime(df_treated['bkpt_date_sub30'])\n",
    "# df_treated['bkpt_date_add30'] = pd.to_datetime(df_treated['bkpt_date_add30'])\n",
    "# df_treated['bkpt_date_add60'] = pd.to_datetime(df_treated['bkpt_date_add60'])\n",
    "# df_treated['bkpt_date_add90'] = pd.to_datetime(df_treated['bkpt_date_add90'])\n",
    "# df_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"df_treated\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"df_treated.tsv.gz\"\n",
    "# df_treated.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_treated.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated = pd.read_csv(LOCAL_DATA_FOLDER+\"df_treated.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_treated['bkpt_date'] = pd.to_datetime(df_treated['bkpt_date'])\n",
    "df_treated['bkpt_date_sub30'] = pd.to_datetime(df_treated['bkpt_date_sub30'])\n",
    "df_treated['bkpt_date_add30'] = pd.to_datetime(df_treated['bkpt_date_add30'])\n",
    "df_treated['bkpt_date_add60'] = pd.to_datetime(df_treated['bkpt_date_add60'])\n",
    "df_treated['bkpt_date_add90'] = pd.to_datetime(df_treated['bkpt_date_add90'])\n",
    "print(f\"Number of treated subjects (increase ratio >= {INCR_RATIO_THRESH}): {len(df_treated)}\")\n",
    "\n",
    "df_treated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of ratios\n",
    "sns.histplot(data=df_treated['ratio_d1_d2'], stat='count', bins=50, kde=False, cumulative=False, color=f'C{i}')\n",
    "plt.title(\"Distribution of increase ratios of the treated group\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "# plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series plots (treated accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = mdates.YearLocator()\n",
    "months = mdates.MonthLocator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_neg_pos(ax, x, y):\n",
    "    \"\"\"\n",
    "    draw a horizontal line at y=0\n",
    "    fill negative values in red and positive values in green\n",
    "    \"\"\"\n",
    "    if y.isnull().all():\n",
    "        return\n",
    "    if (y.min() < 0):\n",
    "        ax.fill_between(x, y.min(), 0, color='red', alpha=0.05)\n",
    "        ax.axhline(y=0, linestyle='solid', color='black', linewidth=0.5)\n",
    "    # ax.fill_between(x, 0, y.max(), color='green', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KM(x, pos):\n",
    "    \"\"\"\n",
    "    format numbers in thousands (k) or in millions (M) on y axis\n",
    "    \"\"\"\n",
    "    'The two args are the value and tick position'\n",
    "    if x > 999_999:\n",
    "        return '%2.1fM' % (x * 1e-6)\n",
    "    elif x > 999:\n",
    "        return '%2.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%3.0f ' % (x)\n",
    "KM_formatter = FuncFormatter(KM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_ACCOUNTS_TO_PLOT = 4\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in tqdm(df_treated[:NUMBER_OF_ACCOUNTS_TO_PLOT].iterrows(), total=df_treated[:NUMBER_OF_ACCOUNTS_TO_PLOT].shape[0]):\n",
    "    patreon = row['patreon_id']\n",
    "    \n",
    "    fig, axs = plt.subplots(7, 4, figsize=(26, 10), sharey=False, sharex=False)\n",
    "        \n",
    "    \n",
    "    ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = restrict_acct_and_sort_df(df_top_pt_accts, 'patreon', patreon, 'date')\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = restrict_acct_and_sort_df(df_yt_timeseries_top_pt, 'patreon_id', patreon, 'datetime')\n",
    "\n",
    "    # youtube metadata\n",
    "    tmp_df_yt_meta = restrict_acct_and_sort_df(df_yt_metadata_pt_filtered, 'patreon_id', patreon, 'upload_date')\n",
    "    \n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    ch_ids = tmp_df_yt['channel'].unique()\n",
    "    print(f\"\\n\\n\\n\\033[1m {idx+1}: {patreon[12:]} (treat = {row['treat']})\\033[0m\")\n",
    "    print(f\"https://www.{patreon}\")\n",
    "    print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    for ch_id in ch_ids:\n",
    "        print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "   \n",
    "    print(f'\\nYouTube Metadata: ')\n",
    "    \n",
    "    if not (tmp_df_yt_meta.empty):\n",
    "        print('• YT videos were uploaded between {} and {}'.format(tmp_df_yt_meta['upload_date'].min().strftime('%B %d, %Y'),\n",
    "                                                                 tmp_df_yt_meta['upload_date'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "        print('• YT metadata was crawled between {} and {}'.format(tmp_df_yt_meta['crawl_date'].min().strftime('%B %d, %Y'),\n",
    "                                                                 tmp_df_yt_meta['crawl_date'].max().strftime('%B %d, %Y')))\n",
    "    else:\n",
    "        print('• No metadata available for this acount')\n",
    "\n",
    "       \n",
    "    ########################## RESTRICT PATREON AND YOUTUBE TIME SERIES TO OVERLAPPING DATES ##########################\n",
    "    \n",
    "   # if no overlap period between YT and Patreon datasets, raise exception and skip account\n",
    "    try: \n",
    "        tmp_df_pt, tmp_df_yt, date_min, date_max = restrict_to_overlapping_dates(tmp_df_pt, tmp_df_yt, 'date', 'datetime')\n",
    "    except Exception as e: \n",
    "        print(f\"Exception: {patreon} and {yt_channel_id}: {e} --> skipping account\")        \n",
    "        no_overlap += 1\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "              \n",
    "    ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "    # breakpoint_date = find_breakpoint_v2(tmp_df_pt, 'patrons_ma')\n",
    "    print(\"Breakpoint date: \", breakpoint_date.date())\n",
    "\n",
    "    # check that dates prior and after breakpoint exist\n",
    "    if not (((breakpoint_date - 1*MONTH_OFFSET)) in ts_pt_df.index and ((breakpoint_date + 1*MONTH_OFFSET) in ts_pt_df.index)):\n",
    "        print(f\"ERROR: Breakpoint too close to edge of patreon time series or missing data\\n\")\n",
    "        plt.figure().clear(); plt.close(); plt.cla(); plt.clf(); plt.show()\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    avg_patrons_add60 = row['avg_patrons_add60']\n",
    "    avg_patrons_add90 = row['avg_patrons_add90']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    bkpt_date_add60 = row['bkpt_date_add60']\n",
    "    bkpt_date_add90 = row['bkpt_date_add90']\n",
    "\n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "    d3 = row['d3']\n",
    "    d4 = row['d4']\n",
    "\n",
    "    \n",
    "    r_d1_d2 = row['ratio_d1_d2']\n",
    "\n",
    "    print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "    print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "    print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "    print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    print(f'• At breakpoint + 60days ({bkpt_date_add60.date()}): {avg_patrons_add60:,.1f}')\n",
    "    print(f'• At breakpoint + 90days ({bkpt_date_add90.date()}): {avg_patrons_add90:,.1f}')\n",
    "    \n",
    "    print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "    print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date_add30.date()} to {bkpt_date_add60.date()}:        d3  = {d3:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date_add60.date()} to {bkpt_date_add90.date()}:        d4  = {d4:>+6.1f} patrons\")\n",
    "    \n",
    "    print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "    print(f\"• Ratio between 2 increases:                            d2/d1  = {r_d1_d2:.2f}\")\n",
    "    print(f\"• Percentage increase:                              d2/d1*100  = {r_d1_d2:>+.0%}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ################################### GET DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "       \n",
    "    \n",
    "    mean_date_sub30 = row['bkpt_date_sub30'] + (row['bkpt_date']       - row['bkpt_date_sub30'])/2\n",
    "    mean_date_add30 = row['bkpt_date']       + (row['bkpt_date_add30'] - row['bkpt_date'])/2\n",
    "    mean_date_add60 = row['bkpt_date_add30'] + (row['bkpt_date_add60'] - row['bkpt_date_add30'])/2\n",
    "    mean_date_add90 = row['bkpt_date_add60'] + (row['bkpt_date_add90'] - row['bkpt_date_add60'])/2\n",
    "  \n",
    "\n",
    "    mean_dates = [\n",
    "        mean_date_sub30, \n",
    "        mean_date_add30, \n",
    "        mean_date_add60, \n",
    "        mean_date_add90\n",
    "    ]  \n",
    "    \n",
    "    mean_delta_patrons = [\n",
    "        row['mean_delta_patrons_sub30'],\n",
    "        row['mean_delta_patrons_add30'],\n",
    "        row['mean_delta_patrons_add60'],\n",
    "        row['mean_delta_patrons_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_earnings = [\n",
    "        row['mean_delta_earnings_sub30'],\n",
    "        row['mean_delta_earnings_add30'],\n",
    "        row['mean_delta_earnings_add60'],\n",
    "        row['mean_delta_earnings_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_videos = [\n",
    "        row['mean_delta_videos_sub30'],\n",
    "        row['mean_delta_videos_add30'],\n",
    "        row['mean_delta_videos_add60'],\n",
    "        row['mean_delta_videos_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_views = [\n",
    "        row['mean_delta_views_sub30'],\n",
    "        row['mean_delta_views_add30'],\n",
    "        row['mean_delta_views_add60'],\n",
    "        row['mean_delta_views_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_subscriptions = [\n",
    "        row['mean_delta_subs_sub30'],\n",
    "        row['mean_delta_subs_add30'],\n",
    "        row['mean_delta_subs_add60'],\n",
    "        row['mean_delta_subs_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_durations = [\n",
    "        row['mean_duration_sub30'],\n",
    "        row['mean_duration_add30'],\n",
    "        row['mean_duration_add60'],\n",
    "        row['mean_duration_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_likes = [\n",
    "        row['mean_likes_sub30'],\n",
    "        row['mean_likes_add30'],\n",
    "        row['mean_likes_add60'],\n",
    "        row['mean_likes_add90']\n",
    "    ]\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    # patreons\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_patrons):\n",
    "        axs[0,2].plot(mean_date, mean_value, marker='o', color='orange', markersize=10)\n",
    "\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_earnings):\n",
    "        axs[1,2].plot(mean_date, mean_value, marker='o', color='royalblue', markersize=10)\n",
    "\n",
    "\n",
    "    # youtube\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_videos):\n",
    "        axs[2,2].plot(mean_date, mean_value, marker='o', color='r', markersize=10)\n",
    "\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_views):\n",
    "        axs[3,2].plot(mean_date, mean_value, marker='o', color='g', markersize=10)\n",
    "\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_subscriptions):\n",
    "        axs[4,2].plot(mean_date, mean_value, marker='o', color='m', markersize=10)\n",
    "        \n",
    "\n",
    "    # youtube metadata\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_durations):\n",
    "        axs[5,2].plot(mean_date, mean_value, marker='o', color='brown', markersize=10)\n",
    "        \n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_likes):\n",
    "        axs[6,2].plot(mean_date, mean_value, marker='o', color='lightblue', markersize=10)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     # plot horizontal lines for means\n",
    "#     mean_befor_list = [mean_delta_patrons_befor, mean_delta_earnings_befor, mean_delta_videos_befor, mean_delta_views_befor, mean_delta_subs_befor, mean_duration_befor, mean_likes_befor]\n",
    "#     mean_afer_list = [mean_delta_patrons_after, mean_delta_earnings_after, mean_delta_videos_after, mean_delta_views_after, mean_delta_subs_after, mean_duration_after, mean_likes_after]\n",
    "       \n",
    "#     for idx, mean in enumerate(mean_befor_list):\n",
    "#             if not math.isnan(mean):\n",
    "#                 axs[idx,2].hlines(y=mean, xmin=bkpt_date_sub30, xmax=bkpt_date      , linewidth=2, linestyle='--', color='green')\n",
    "\n",
    "#     for idx, mean in enumerate(mean_afer_list):\n",
    "#             if not math.isnan(mean):\n",
    "#                 axs[idx,2].hlines(y=mean, xmin=bkpt_date,       xmax=bkpt_date_add30, linewidth=2, linestyle='--', color='orange')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    ################################### ZOOM OUT PLOTS ###################################\n",
    "    \n",
    "    # number of patrons (delta)\n",
    "    axs[0,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,0].set(title=\"Delta patrons per week\")\n",
    "    axs[0,0].set_ylabel(\"Δ Patrons\")    \n",
    "    color_neg_pos(axs[0,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'])\n",
    "\n",
    "    # number of patrons (cumulative)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons'], alpha=0.2)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons_ma'])\n",
    "    axs[0,1].set(title=\"Number of patrons\")\n",
    "    axs[0,1].set_ylabel(\"# Patrons\")\n",
    "\n",
    "    # patreon earnings (delta)\n",
    "    axs[1,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,0].set(title=\"Patreon delta earnings per week\")\n",
    "    axs[1,0].set_ylabel(\"Δ Earnings\") \n",
    "    color_neg_pos(axs[1,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'])\n",
    "\n",
    "    # patreon earnings (cumulative)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning'], alpha=0.2)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning_ma'], color='royalblue')\n",
    "    axs[1,1].set(title=\"Patreon earnings per month\")\n",
    "    axs[1,1].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # youtube videos (delta)\n",
    "    axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,0].set(title=\"YouTube delta videos per week\")\n",
    "    axs[2,0].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,0], tmp_df_yt['datetime'], tmp_df_yt['delta_videos'])\n",
    "\n",
    "    # youtube videos (cumulative)\n",
    "    axs[2,1].plot(tmp_df_yt['datetime'], tmp_df_yt['videos'], 'r')\n",
    "    axs[2,1].set(title=\"YouTube cumulative videos\")\n",
    "    axs[2,1].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # youtube views (delta)\n",
    "    axs[3,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,0].set(title=\"YouTube delta views per week\")\n",
    "    axs[3,0].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,0], tmp_df_yt['datetime'], tmp_df_yt['delta_views'])\n",
    "\n",
    "    # youtube views (cumulative)\n",
    "    axs[3,1].plot(tmp_df_yt['datetime'], tmp_df_yt['views'], 'g')\n",
    "    axs[3,1].set(title=\"YouTube cumulative views\")\n",
    "    axs[3,1].set_ylabel(\"# Views\")\n",
    "\n",
    "    # youtube subs (delta)\n",
    "    axs[4,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,0].set(title=\"YouTube delta subscriptions per week\")\n",
    "    axs[4,0].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,0], tmp_df_yt['datetime'], tmp_df_yt['delta_subs'])\n",
    "\n",
    "    # youtube subs (cumulative)\n",
    "    axs[4,1].plot(tmp_df_yt['datetime'], tmp_df_yt['subs'], 'm')\n",
    "    axs[4,1].set(title=\"YouTube cumulative subscriptions\")\n",
    "    axs[4,1].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,0].set(title=\"YouTube videos durations\")\n",
    "    axs[5,0].set_ylabel(\"Duration\")\n",
    "    \n",
    "    \n",
    "    # youtube likes at crawl date\n",
    "    axs[6,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,0].set(title=\"YouTube likes (plotted against upload date)\")\n",
    "    axs[6,0].set_ylabel(\"Likes\")\n",
    "    \n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = bkpt_date_sub30 - (1 * MONTH_OFFSET)\n",
    "    date_max_zoom = bkpt_date_add90 + (1 * MONTH_OFFSET)\n",
    "    \n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_meta_zoomed = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min_zoom) & (tmp_df_yt_meta['upload_date'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "    \n",
    "   ################################### ZOOM IN PLOTS  ###################################\n",
    "\n",
    "    # zoomed in patron numbers (delta)\n",
    "    axs[0,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', alpha=0.3)\n",
    "    axs[0,2].set(title=\"Delta patrons per week\")\n",
    "    axs[0,2].set_ylabel(\"Δ Patrons\")\n",
    "    color_neg_pos(axs[0,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'])\n",
    "    \n",
    "    # zoomed in patron numbers (cumulative)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons'], alpha=0.2)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons_ma'])\n",
    "    axs[0,3].set(title=\"Number of patrons (zoomed in)\")\n",
    "    axs[0,3].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "    # zoomed in patron earnings (delta)\n",
    "    axs[1,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', alpha=0.3)\n",
    "    axs[1,2].set(title=\"Delta Patreon earnings per week (zoomed in)\")\n",
    "    axs[1,2].set_ylabel(\"Earnings\")  \n",
    "    color_neg_pos(axs[1,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_earning'])\n",
    "\n",
    "    # zoomed in patron earnings (cumulative)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning'], alpha=0.2)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning_ma'], color='royalblue')\n",
    "    axs[1,3].set(title=\"Patreon earnings per month (zoomed in)\")\n",
    "    axs[1,3].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # zoomed in youtube videos (delta)\n",
    "    axs[2,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', alpha=0.3)\n",
    "    axs[2,2].set(title=\"YouTube delta videos per week (zoomed in)\")\n",
    "    axs[2,2].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_videos'])\n",
    "\n",
    "    # zoomed in youtube videos (cumulative)\n",
    "    axs[2,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['videos'], 'r')\n",
    "    axs[2,3].set(title=\"YouTube cumulative videos (zoomed in)\")\n",
    "    axs[2,3].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # zoomed in youtube views (delta)\n",
    "    axs[3,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', alpha=0.3)\n",
    "    axs[3,2].set(title=\"YouTube delta views per week (zoomed in)\")\n",
    "    axs[3,2].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_views'])\n",
    "\n",
    "    # zoomed in youtube views (cumulative)\n",
    "    axs[3,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['views'], 'g')\n",
    "    axs[3,3].set(title=\"YouTube cumulative views (zoomed in)\")\n",
    "    axs[3,3].set_ylabel(\"# Views\")\n",
    "    \n",
    "    # zoomed in youtube subs (delta)\n",
    "    axs[4,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', alpha=0.3)\n",
    "    axs[4,2].set(title=\"YouTube delta subscriptions per week (zoomed in)\")\n",
    "    axs[4,2].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_subs'])\n",
    "\n",
    "    # zoomed in youtube subs (cumulative)\n",
    "    axs[4,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['subs'], 'm')\n",
    "    axs[4,3].set(title=\"YouTube cumulative subscriptions (zoomed in)\")\n",
    "    axs[4,3].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', alpha=0.3)\n",
    "    axs[5,2].set(title=\"YouTube videos durations (zoomed in)\")\n",
    "    axs[5,2].set_ylabel(\"Duration\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'])\n",
    "    \n",
    "        \n",
    "   # youtube likes per uploads\n",
    "    axs[6,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', alpha=0.3)\n",
    "    axs[6,2].set(title=\"YouTube likes (plotted against upload date) (zoomed in)\")\n",
    "    axs[6,2].set_ylabel(\"Likes\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['crawl_date'], tmp_df_yt_meta_zoomed['like_count'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################### FORMAT AXES ###################################\n",
    "\n",
    "    # format the axes\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if j < 2:\n",
    "                axs[i,j].set_xlim([date_min, date_max])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            if j >= 2:\n",
    "                axs[i,j].set_xlim([date_min_zoom, date_max_zoom])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "                # axs[i,j].xaxis.set_minor_locator(mdates.WeekdayLocator())\n",
    "            axs[i,j].xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.set_major_formatter(KM_formatter)\n",
    "            \n",
    "            \n",
    "    ################################### PLOT BREAKPOINT LINES AND POINTS ###################################\n",
    "\n",
    "    # plot vertical lines for breakpoint, breakpoint-1month, breakpoint+1month\n",
    "    print_legend = True\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if print_legend:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', label='breakpoint', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)          \n",
    "                axs[i,j].axvline(breakpoint_date + 2*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)          \n",
    "                axs[i,j].axvline(breakpoint_date + 3*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)          \n",
    "                # print_legend = False\n",
    "            else:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + 2*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + 3*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "    # axs[0,0].legend()\n",
    "    axs[0,1].legend()\n",
    "\n",
    "    # plot point for mean nb of patrons for breakpoint, breakpoint-1month, breakpoint+1month    \n",
    "    axs[0,3].plot(breakpoint_date - MONTH_OFFSET, ts_pt_df.at[(breakpoint_date - MONTH_OFFSET), 'patrons_ma'], marker='o', color='green')\n",
    "    axs[0,3].plot(breakpoint_date,               ts_pt_df.at[breakpoint_date              , 'patrons_ma'], marker='o', color='red')    \n",
    "    axs[0,3].plot(breakpoint_date + MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "    axs[0,3].plot(breakpoint_date + 2*MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + 2*MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "    axs[0,3].plot(breakpoint_date + 3*MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + 3*MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')   \n",
    "\n",
    "    fig.tight_layout(w_pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Granger causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Compute Granger Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precedence links we will explore\n",
    "granger_columns = [\n",
    "'pt_delta_patrons->yt_delta_videos',\n",
    "'pt_delta_patrons->yt_delta_views',\n",
    "'pt_delta_patrons->yt_delta_subs',\n",
    "'yt_delta_videos->pt_delta_patrons',\n",
    "'yt_delta_views->pt_delta_patrons',\n",
    "'yt_delta_subs->pt_delta_patrons'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare YouTube and Patreon timeseries for top patreon accounts with rolling average - MANUAL VERSION 2\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# WEEK_OFFSET = pd.DateOffset(weeks=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# variables for Granger Tests\n",
    "MAXLAG = 2\n",
    "granger_dict = {} # dictionary with  keys (cause --> effect) and values with list of corresponding patreon account(s)\n",
    "not_granger = []\n",
    "YT_variables = ['yt_delta_videos', 'yt_delta_views', 'yt_delta_subs']\n",
    "# PT_variables = ['pt_delta_patrons', 'pt_delta_earning']\n",
    "PT_variables = ['pt_delta_patrons']\n",
    "\n",
    "df_granger = df_treated.copy()\n",
    "\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in tqdm(df_granger.iterrows(), total=df_granger.shape[0]):   \n",
    "    patreon = row['patreon_id']\n",
    "\n",
    "     ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = restrict_acct_and_sort_df(df_top_pt_accts, 'patreon', patreon, 'date')\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = restrict_acct_and_sort_df(df_yt_timeseries_top_pt, 'patreon_id', patreon, 'datetime')\n",
    "\n",
    "    # youtube metadata\n",
    "    tmp_df_yt_meta = restrict_acct_and_sort_df(df_yt_metadata_pt_filtered, 'patreon_id', patreon, 'upload_date')\n",
    "    \n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    # ch_ids = tmp_df_yt['channel'].unique()\n",
    "    # print(f\"\\n\\n\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "    # print(f\"https://www.{patreon}\")\n",
    "    # print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    # for ch_id in ch_ids:\n",
    "    #     print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "\n",
    "    \n",
    "#     ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "#     # set min and max dates for plots   \n",
    "#     date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "#     date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "#     if date_max < date_min:\n",
    "#         print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "#         continue\n",
    "    \n",
    "#     # restrict datasets between min and max dates\n",
    "#     tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "#     tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    \n",
    "#     # align both dataframes since youtube starts once a week\n",
    "#     tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    ########################## RESTRICT PATREON AND YOUTUBE TIME SERIES TO OVERLAPPING DATES ##########################\n",
    "    \n",
    "   # if no overlap period between YT and Patreon datasets, raise exception and skip account\n",
    "    try: \n",
    "        tmp_df_pt, tmp_df_yt, date_min, date_max = restrict_to_overlapping_dates(tmp_df_pt, tmp_df_yt, 'date', 'datetime')\n",
    "    except Exception as e: \n",
    "        print(f\"Exception: {patreon} and {yt_channel_id}: {e} --> skipping account\")        \n",
    "        no_overlap += 1\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "               \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    \n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "\n",
    "    \n",
    "    r = row['ratio_d1_d2']\n",
    "\n",
    "#     print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "#     print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "#     print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "#     print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    \n",
    "#     print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "#     print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "#     print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    \n",
    "#     print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "#     print(f\"• Ratio between 2 increases:                            d2/d1  = {r:.2f}\")\n",
    "#     print(f\"• Percentage increase:                            |d2/d1|*100  = {abs(r):>+.0%}\")\n",
    "    \n",
    "\n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = breakpoint_date - (2 * MONTH_OFFSET)\n",
    "    date_max_zoom = breakpoint_date + (2 * MONTH_OFFSET)\n",
    "            \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "\n",
    "\n",
    "    ################################### GRANGER CAUSALITY TESTS ###################################\n",
    "\n",
    "    # create a new dataframe with merged columns (the dates might have a day difference)\n",
    "    selected_pt_columns  = ['delta_earning', 'delta_patrons']\n",
    "    df_pt = ts_pt_weekly_avg_df_zoomed\n",
    "    df_pt = df_pt[selected_pt_columns].reset_index().add_prefix('pt_')\n",
    "\n",
    "    selected_yt_columns = ['datetime', 'delta_views', 'delta_subs', 'delta_videos']\n",
    "    df_yt = tmp_df_yt_zoomed\n",
    "    df_yt = df_yt[selected_yt_columns].reset_index().add_prefix('yt_')\n",
    "\n",
    "    # concatenated 2 dfs and select and reorder columns\n",
    "    df_concat = pd.concat([df_pt, df_yt], axis=1)\n",
    "    concat_columns = ['pt_date', 'yt_datetime', 'pt_delta_earning', 'pt_delta_patrons', 'yt_delta_views', 'yt_delta_subs', 'yt_delta_videos']\n",
    "    df_concat = df_concat[concat_columns]\n",
    "    # df_concat['dates_match'] = df_concat['pt_date'] == df_concat['yt_datetime']\n",
    "    \n",
    "    # display(df_concat.round())\n",
    "    # display(df_concat.style.set_caption(f\"df_concat\"))\n",
    "    \n",
    "    \n",
    "    # print(f\"\\nGranger Causality Tests:\")\n",
    "    \n",
    "    granger_causal_link = False\n",
    "    for pt_var in PT_variables:\n",
    "        for yt_var in YT_variables:\n",
    "            \n",
    "            # if nan values in this df, skip\n",
    "            if df_concat[[yt_var, pt_var]].isna().values.any():\n",
    "                continue\n",
    "                \n",
    "            pvalue_fwd = {}\n",
    "            pvalue_rev = {}\n",
    "            \n",
    "            try:\n",
    "                granger_test_fwd = grangercausalitytests(df_concat[[yt_var, pt_var]], maxlag=MAXLAG, verbose=False)  \n",
    "                # print(f'\\n\\n• {pt_var} --> {yt_var}')\n",
    "                # for lag, tests_results in granger_test_fwd.items():\n",
    "                #     print(f\"\\nlag={lag}\")                    \n",
    "                #     for test_name, tests_result in tests_results[0].items():\n",
    "                #         # test_results is the tuple: (test statistic, pvalues, degrees of freedom, [degrees of freedom])\n",
    "                #         print(f\"{test_name}: \\t{tests_result}\")\n",
    "\n",
    "                granger_test_rev = grangercausalitytests(df_concat[[pt_var, yt_var]], maxlag=MAXLAG, verbose=False) \n",
    "                # print(f'\\n\\n• {yt_var} --> {pt_var}')\n",
    "                # for lag, tests_results in granger_test_rev.items():\n",
    "                #     print(f\"\\nlag={lag}\")                    \n",
    "                #     for test_name, tests_result in tests_results[0].items():\n",
    "                #         # test_results is the tuple: (test statistic, pvalues, degrees of freedom, [degrees of freedom])\n",
    "                #         print(f\"{test_name}: \\t{tests_result}\")\n",
    "            except Exception:\n",
    "                print(\"Granger Causality Exception\")\n",
    "                continue\n",
    "\n",
    "            # for each lag, get the p-value for the sum of squared residuals (SSR) of the F-test\n",
    "            for lag in range(1, MAXLAG+1):        \n",
    "                pvalue_fwd[lag] = granger_test_fwd[lag][0]['ssr_ftest'][1]\n",
    "                pvalue_rev[lag] = granger_test_rev[lag][0]['ssr_ftest'][1]\n",
    "                \n",
    "            \n",
    "            \n",
    "            # for the forward tests, get the minimum p-value of the different lags and compare it to our significance level\n",
    "            min_pvalue_fwd = min(pvalue_fwd.values())\n",
    "            if min_pvalue_fwd < 0.05:\n",
    "                granger_causal_link = True\n",
    "                # get the lag of the minimum p-value for that account\n",
    "                min_lag_fwd = [k for k, v in pvalue_fwd.items() if v == min_pvalue_fwd][0]\n",
    "                # print(f'• {pt_var} --> {yt_var} (pvalue={min_pvalue_fwd:.3f}, lag={min_lag_fwd})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 1\n",
    "\n",
    "                if (pt_var, yt_var) in granger_dict:                   \n",
    "                    granger_dict[(pt_var, yt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(pt_var, yt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 0\n",
    "                \n",
    "            # repeat for the reverse tests\n",
    "            min_pvalue_rev = min(pvalue_rev.values())\n",
    "            if min_pvalue_rev < 0.05:\n",
    "                granger_causal_link = True\n",
    "                min_lag_rev = [k for k, v in pvalue_rev.items() if v == min_pvalue_rev][0]\n",
    "                # print(f'• {yt_var} --> {pt_var} (pvalue={min_pvalue_rev:.3f}, lag={min_lag_rev})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 1\n",
    "                \n",
    "                if (yt_var, pt_var) in granger_dict:\n",
    "                    granger_dict[(yt_var, pt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(yt_var, pt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 0\n",
    "                \n",
    "\n",
    "    if (granger_causal_link == False):\n",
    "        # print(\"• No Granger causality found for this account\")\n",
    "        not_granger.append(patreon)\n",
    "    \n",
    "    # print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "print(f'\\n\\nGranger tests summary statistics: (with maxlag = {MAXLAG})')\n",
    "    \n",
    "print(f'• Number of patreon accounts analysed (patrons increase ratio > {INCR_RATIO_THRESH}): {len(df_granger)}')\n",
    "print(f'• Number of patreon with no Granger-causal link: {len(not_granger)} ({len(not_granger)/len(df_granger):.0%})')\n",
    "\n",
    "print(f'• Number of patreon accounts per Granger-causal link:')\n",
    "\n",
    "# Converting granger dict into list of tuples (in order to sort it), the 2nd value of the tuple being the count of accounts\n",
    "granger_list = [(k, len(v)) for k, v in granger_dict.items()]\n",
    "# sort by count desc\n",
    "\n",
    "\n",
    "granger_list_desc = sorted(granger_list, key=lambda tup: -tup[1])\n",
    "for (k,v) in granger_list_desc:\n",
    "    print(f'    • {k[0]} \\t--> {k[1]}:\\t {v} ({v/df_granger[granger_columns].sum().sum():.0%})')\n",
    "\n",
    "df_granger[granger_columns] = df_granger[granger_columns].astype('Int64')\n",
    "df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"df_granger\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"df_granger.tsv.gz\"\n",
    "# df_granger.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Granger causality plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_granger.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load granger dataframe\n",
    "df_granger = pd.read_csv(LOCAL_DATA_FOLDER+\"df_granger.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_granger['bkpt_date'] = pd.to_datetime(df_granger['bkpt_date'])\n",
    "df_granger['bkpt_date_sub30'] = pd.to_datetime(df_granger['bkpt_date_sub30'])\n",
    "df_granger['bkpt_date_add30'] = pd.to_datetime(df_granger['bkpt_date_add30'])\n",
    "df_granger['bkpt_date_add60'] = pd.to_datetime(df_granger['bkpt_date_add60'])\n",
    "df_granger['bkpt_date_add90'] = pd.to_datetime(df_granger['bkpt_date_add90'])\n",
    "df_granger[granger_columns] = df_granger[granger_columns].astype('Int64')\n",
    "df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split columns in PT->YT, and reverse YT->PT\n",
    "cols1 = [\n",
    "'pt_delta_patrons->yt_delta_videos',\n",
    "'pt_delta_patrons->yt_delta_views',\n",
    "'pt_delta_patrons->yt_delta_subs'\n",
    "]\n",
    "cols2 = [\n",
    "'yt_delta_videos->pt_delta_patrons',\n",
    "'yt_delta_views->pt_delta_patrons',\n",
    "'yt_delta_subs->pt_delta_patrons'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For different minimum ratios of increase, plot sum of Granger-causal links between Patreon and YouTube time-series (in blue) and vice-versa (in orange)\n",
    "\n",
    "nb_plots = 10\n",
    "sbplt_cols = 5\n",
    "sbplt_rows = int(nb_plots / sbplt_cols)\n",
    "\n",
    "fig, axs = plt.subplots(sbplt_rows, sbplt_cols, figsize=(16,16), sharey=True, sharex=False)\n",
    "for idx in range(0, nb_plots):\n",
    "    \n",
    "    row = math.floor(idx/sbplt_cols)\n",
    "    col = idx % sbplt_cols\n",
    "    sbplt = axs[row, col]\n",
    "    \n",
    "    increase_ratio = idx+INCR_RATIO_THRESH\n",
    "    ratio_df = df_granger[df_granger['ratio_d1_d2'] > increase_ratio]\n",
    "    \n",
    "    # print(f'\\n\\nratio > {increase_ratio}:')\n",
    "    # print(f'total number of accounts: {len(ratio_df)}:')\n",
    "    # no_causal_links_df = ratio_df[ratio_df[cols1 + cols2].sum(axis=1) == 0]\n",
    "    # print(f'nb accts with no Granger-causal links: {len(no_causal_links_df)} ({len(no_causal_links_df)/len(ratio_df):.0%})')\n",
    "    # print(f'\\nPatreon --> YouTube:')\n",
    "    # print(ratio_df[cols1].sum())\n",
    "    # print(f'\\nYouTube --> Patreon:')\n",
    "    # print(ratio_df[cols2].sum())\n",
    "    \n",
    "    granger_series = ratio_df[cols1 + cols2].sum()/ratio_df[cols1 + cols2].sum().sum()\n",
    "    sbplt.bar(granger_series[cols1].index, granger_series[cols1].values, label='PT --> YT')\n",
    "    sbplt.bar(granger_series[cols2].index, granger_series[cols2].values, label='YT --> PT')\n",
    "    sbplt.set_title(f\"ratio > {increase_ratio}\\n # observations = {len(ratio_df)}\")\n",
    "    sbplt.set_xlabel(\"# Granger-causal links\")\n",
    "    sbplt.set_ylabel(\"% of PT accts\")\n",
    "    sbplt.tick_params(axis='x', labelrotation=90)\n",
    "    sbplt.legend()\n",
    "\n",
    "\n",
    "fig.suptitle('Granger-causal links between Patreon and YouTube time-series, for different minimum ratios of increase at breakpoint \\n (one account can have multiple causal-links)', fontweight=\"bold\")\n",
    "fig.text(0.5,0, 'Granger-causal links')\n",
    "fig.text(0,0.5, 'Percentage of Patreon accts ', rotation = 90)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=3, w_pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Although there are granger links in both directions, we notice that there are more granger links going from Patreon --> YouTube, than YouTube --> Patreon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observational Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Select \"treated\" accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change point detection _(see section 2.2)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Select \"control\" accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential \"control\" accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each subject in “Treated” group (`df_treated`):\n",
    "    - For each subject in `top_patreons` Population (except for current Treated subject):\n",
    "        - Run breakpoint detection algorithm up to the Treated breakpoint date\n",
    "            - If we find a breakpoint --> reject \n",
    "            - If we dont find a breakpoint --> add to _potential_ control subjects (aka “Potential Control Subjects”) for this Treated subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_potential_matches = {}  \n",
    "\n",
    "# print(f\"Look for potential matches for {len(df_treated)} treated subjects\")\n",
    "\n",
    "# for idx_treated, treated_subject_row in tqdm(df_treated.iterrows(), total=df_treated.shape[0]):\n",
    "#     treated_subject = treated_subject_row['patreon_id']\n",
    "#     # print(f\"\\ntreated subject: {idx_treated}, {treated_subject}\")\n",
    "    \n",
    "#     # make sure date cant go beyond treated group\n",
    "#     date_max = treated_subject_row['bkpt_date_add30']    \n",
    "#     # print(\"date_max: \", date_max)\n",
    "    \n",
    "#     for idx_potential_control, potential_control_subject in enumerate(top_patreons):\n",
    "#     # print(f\"\\n\\tpotential control subject: {idx_potential_control}, {potential_control_subject}\")        \n",
    "        \n",
    "#         # make sure potential control account is not the same as treated account         \n",
    "#         if (potential_control_subject == treated_subject):\n",
    "#             # print(\"potential_control_subject == treated_subject\")\n",
    "#             continue\n",
    "            \n",
    "#         # eliminate potential control accounts that have a breakpoint date earlier than treated\n",
    "#         if (potential_control_subject not in df_treated['patreon_id'].tolist()):\n",
    "#             # print(\"\\t\\tNo breakpoint for this account...      => KEEP AS A POTENTIAL CONTROL GROUP :)\")\n",
    "#             if treated_subject in dict_potential_matches:\n",
    "#                 dict_potential_matches[treated_subject].append(potential_control_subject)\n",
    "#             else:\n",
    "#                 dict_potential_matches[treated_subject] = [potential_control_subject]\n",
    "#         else: \n",
    "#             # if in the treated group but has a breakpoint after max date, consider it\n",
    "#             if (df_treated[df_treated['patreon_id'] == potential_control_subject].bkpt_date.iloc[0] > date_max):\n",
    "#                 # print(\"\\t\\tBreakpoint after the max date...      => KEEP AS A POTENTIAL CONTROL GROUP :)\")\n",
    "#                 if treated_subject in dict_potential_matches:\n",
    "#                     dict_potential_matches[treated_subject].append(potential_control_subject)\n",
    "#                 else:\n",
    "#                     dict_potential_matches[treated_subject] = [potential_control_subject]\n",
    "#             else:\n",
    "#                 # reject it\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save potential dictionary matches to disk in pickle format\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"dict_potential_matches.pickle\"\n",
    "# with open(output_file_path, 'wb') as f:\n",
    "#     pickle.dump(dict_potential_matches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dict_potential_matches.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOCAL_DATA_FOLDER+\"dict_potential_matches.pickle\", 'rb') as f:\n",
    "    dict_potential_matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of treated subjects: {len(dict_potential_matches)}\")\n",
    "\n",
    "print(\"\\nNumber of potential control subjects for each treated subject (print first 5): \") \n",
    "for idx, (k, v) in enumerate(dict_potential_matches.items()):\n",
    "    if idx <5:\n",
    "        print(f'{k}: {len(v)}')\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudo intervention date (align bkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_breakpoint_dict(df, column, treated_subject_row):\n",
    "    \"\"\"\n",
    "    Mathces the dates of the treated subject\n",
    "    \n",
    "    :param df: dataframe\n",
    "    :param column: column to scan\n",
    "    :param treated_subject_row: treated suject to get the dates from\n",
    "    \"\"\"\n",
    "    \n",
    "    bkpt_date_sub30   = treated_subject_row['bkpt_date_sub30'].iloc[0]\n",
    "    bkpt_date         = treated_subject_row['bkpt_date'].iloc[0]\n",
    "    bkpt_date_add30   = treated_subject_row['bkpt_date_add30'].iloc[0]\n",
    "    bkpt_date_add60   = treated_subject_row['bkpt_date_add60'].iloc[0]\n",
    "    bkpt_date_add90   = treated_subject_row['bkpt_date_add90'].iloc[0]\n",
    "\n",
    "        \n",
    "    # align points from subject conto control group\n",
    "    try:        \n",
    "        sub30 = df[df['date'] == bkpt_date_sub30]['patrons_ma'].iloc[0]\n",
    "        point = df[df['date'] == bkpt_date]['patrons_ma'].iloc[0]\n",
    "        add30 = df[df['date'] == bkpt_date_add30]['patrons_ma'].iloc[0]\n",
    "        add60 = df[df['date'] == bkpt_date_add60]['patrons_ma'].iloc[0]\n",
    "        add90 = df[df['date'] == bkpt_date_add90]['patrons_ma'].iloc[0]\n",
    "    except Exception as e:\n",
    "        # print(\"Exception in align_breakpoint_dict function (date issue): \", e)\n",
    "        raise\n",
    "\n",
    "    d1 = point - sub30\n",
    "    d2 = add30 - point            \n",
    "    d3 = add60 - add30            \n",
    "    d4 = add90 - add60            \n",
    "\n",
    "    # avoid  weird ratios obtained by diving by a difference between -1 and 1 \n",
    "    if (0 <= d1 < 1):\n",
    "        d1 = 1\n",
    "    elif (-1 < d1 < 0):\n",
    "        d1 = -1\n",
    "\n",
    "    r_d1_d2 = d2 / d1\n",
    "\n",
    "    # no condition since it's the control group\n",
    "    bkpnt_dict = {\n",
    "        \"bkpt_date\"         : bkpt_date,\n",
    "        \"bkpt_date_sub30\"   : bkpt_date_sub30,\n",
    "        \"bkpt_date_add30\"   : bkpt_date_add30,\n",
    "        \"bkpt_date_add60\"   : bkpt_date_add60,\n",
    "        \"bkpt_date_add90\"   : bkpt_date_add90,\n",
    "        \"avg_patrons_bkpnt\" : point,\n",
    "        \"avg_patrons_sub30\" : sub30,\n",
    "        \"avg_patrons_add30\" : add30,\n",
    "        \"avg_patrons_add60\" : add60,\n",
    "        \"avg_patrons_add90\" : add90,\n",
    "        \"d1\"                : d1,\n",
    "        \"d2\"                : d2,\n",
    "        \"d3\"                : d3,\n",
    "        \"d4\"                : d4,\n",
    "        \"r_d1_d2\"           : r_d1_d2\n",
    "    }\n",
    "\n",
    "    return bkpnt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`align_breakpoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation pseudo intervention dates on potential control group\n",
    "def align_breakpoints(treated_subject, potential_control_list, patreon_df, youtube_timeseries_df, youtube_meta_df):\n",
    "    \"\"\"\n",
    "    for a treated subject in dict, \n",
    "    get breakpoint dates values of potential control accounts\n",
    "    \"\"\"\n",
    "    # print(f\"\\n matching regions of potential control subjects for treated subject: {treated_subject}....)\")\n",
    "          \n",
    "    # variables declaration\n",
    "    MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "    ROLLING_AVG_WINDOW = 30\n",
    "    potential_control_tuples = []\n",
    "\n",
    "\n",
    "        \n",
    "    treated_subject_row = df_treated[df_treated['patreon_id'] == treated_subject]\n",
    "    bkpt_date_sub30   = treated_subject_row['bkpt_date_sub30'].iloc[0]\n",
    "    bkpt_date         = treated_subject_row['bkpt_date'].iloc[0]\n",
    "    bkpt_date_add30   = treated_subject_row['bkpt_date_add30'].iloc[0]\n",
    "    bkpt_date_add60   = treated_subject_row['bkpt_date_add60'].iloc[0]\n",
    "    bkpt_date_add90   = treated_subject_row['bkpt_date_add90'].iloc[0]\n",
    "    \n",
    "    # print(\"treated_subject_row:\", treated_subject_row)\n",
    "    # print(\"bkpt_date_sub30:\", bkpt_date_sub30)    \n",
    "    \n",
    "    for idx, potential_control in enumerate(tqdm(potential_control_list)):\n",
    "        # print(\"\\t\", idx, potential_control)\n",
    "        treat = 0\n",
    "        \n",
    "\n",
    "        ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "        # restrict to 1 patreon account, sort by date and drop duplicates\n",
    "\n",
    "        # patreon earnings and users\n",
    "        tmp_df_pt = restrict_acct_and_sort_df(patreon_df, 'patreon', potential_control, 'date')\n",
    "\n",
    "        # sanity checks to make sure the treated date range exist in potential_control_df\n",
    "        if (tmp_df_pt[tmp_df_pt['date'] == bkpt_date_sub30].empty) or (tmp_df_pt[tmp_df_pt['date'] == bkpt_date_add90].empty):\n",
    "            # print(\"skip Patreon account as the range dates dont exist\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # youtube videos\n",
    "        tmp_df_yt = restrict_acct_and_sort_df(youtube_timeseries_df, 'patreon_id', potential_control, 'datetime')\n",
    "            \n",
    "\n",
    "        # youtube metadata\n",
    "        tmp_df_yt_meta = restrict_acct_and_sort_df(youtube_meta_df, 'patreon_id', potential_control, 'upload_date')\n",
    "\n",
    "        ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "\n",
    "        # set min and max dates for plots   \n",
    "        date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "        date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "\n",
    "        # if no overlap period between YT and Patreon datasets, skip account\n",
    "        if date_max < date_min:\n",
    "            # print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "            continue\n",
    "\n",
    "        # restrict datasets between min and max dates\n",
    "        tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "        tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "\n",
    "        # align both dataframes since youtube starts once a week\n",
    "        tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "\n",
    "        tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "        tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "        ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "\n",
    "        # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "        ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "        ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "        ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "        ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "        tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "\n",
    "        # reorder columns to have deltas columns next to their respective columns\n",
    "        patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "        ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "\n",
    "        # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "        ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "\n",
    "\n",
    "\n",
    "        ########################## PRINT TITLES ##########################\n",
    "        # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "        # ch_ids = tmp_df_yt['channel'].unique()\n",
    "        # print(f\"\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "\n",
    "        ########################## ALIGN CONTROL ACCOUNT TO TREATED BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "        try:\n",
    "            bkpnt_dict = align_breakpoint_dict(tmp_df_pt, 'patrons_ma', treated_subject_row)\n",
    "\n",
    "        except Exception as e:\n",
    "                print(\"Exception in align_breakpoint_dict function: \", e)\n",
    "                continue            \n",
    "                              \n",
    "                \n",
    "        ################################### CALCULATE DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "\n",
    "        ##### PATREON #####\n",
    "        tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_sub30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date'])]\n",
    "        tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add30'])]\n",
    "        tmp_df_PT_add60 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_add30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add60'])]\n",
    "        tmp_df_PT_add90 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_add60']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add90'])]\n",
    "\n",
    "        # delta patrons\n",
    "        mean_delta_patrons_sub30 = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "        mean_delta_patrons_add30 = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "        mean_delta_patrons_add60 = tmp_df_PT_add60['delta_patrons'].mean()\n",
    "        mean_delta_patrons_add90 = tmp_df_PT_add90['delta_patrons'].mean()\n",
    "\n",
    "        # delta earnings\n",
    "        mean_delta_earnings_sub30 = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "        mean_delta_earnings_add30 = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "        mean_delta_earnings_add60 = tmp_df_PT_add60['delta_earning'].mean()  \n",
    "        mean_delta_earnings_add90 = tmp_df_PT_add90['delta_earning'].mean()  \n",
    "\n",
    "\n",
    "        ##### YOUTUBE TIME SERIES #####\n",
    "        tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date']      )]\n",
    "        tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "        tmp_df_YT_add60 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_add30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add60'])]\n",
    "        tmp_df_YT_add90 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_add60']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add90'])]\n",
    "\n",
    "        # delta videos\n",
    "        mean_delta_videos_sub30 = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "        mean_delta_videos_add30 = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "        mean_delta_videos_add60 = tmp_df_YT_add60['delta_videos'].mean()  \n",
    "        mean_delta_videos_add90 = tmp_df_YT_add90['delta_videos'].mean()  \n",
    "\n",
    "        # delta views\n",
    "        mean_delta_views_sub30 = tmp_df_YT_sub30['delta_views'].mean()\n",
    "        mean_delta_views_add30 = tmp_df_YT_add30['delta_views'].mean()  \n",
    "        mean_delta_views_add60 = tmp_df_YT_add60['delta_views'].mean()  \n",
    "        mean_delta_views_add90 = tmp_df_YT_add90['delta_views'].mean()  \n",
    "\n",
    "        # delta subscriptions\n",
    "        mean_delta_subs_sub30 = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "        mean_delta_subs_add30 = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "        mean_delta_subs_add60 = tmp_df_YT_add60['delta_subs'].mean()  \n",
    "        mean_delta_subs_add90 = tmp_df_YT_add90['delta_subs'].mean()  \n",
    "\n",
    "\n",
    "        ##### YOUTUBE METADATA #####\n",
    "        tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date']      )]\n",
    "        tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "        tmp_df_YT_META_add60 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_add30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add60'])]\n",
    "        tmp_df_YT_META_add90 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_add60']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add90'])]\n",
    "\n",
    "        # durations\n",
    "        mean_duration_sub30 = tmp_df_YT_META_sub30['duration'].mean()\n",
    "        mean_duration_add30 = tmp_df_YT_META_add30['duration'].mean()      \n",
    "        mean_duration_add60 = tmp_df_YT_META_add60['duration'].mean()      \n",
    "        mean_duration_add90 = tmp_df_YT_META_add90['duration'].mean()      \n",
    "\n",
    "        # likes\n",
    "        mean_likes_sub30 = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "        mean_likes_add30 = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "        mean_likes_add60 = tmp_df_YT_META_add60['like_count'].mean()      \n",
    "        mean_likes_add90 = tmp_df_YT_META_add90['like_count'].mean()     \n",
    "\n",
    "\n",
    "        # get youtube channel id name\n",
    "        yt_channel_id = tmp_df_yt['channel'].unique()[0]\n",
    "\n",
    "\n",
    "        potential_control_tuples.append(\n",
    "            (          \n",
    "                potential_control, \n",
    "                yt_channel_id,   \n",
    "                treat,\n",
    "                bkpnt_dict[\"d1\"], \n",
    "                bkpnt_dict[\"d2\"], \n",
    "                bkpnt_dict[\"d3\"], \n",
    "                bkpnt_dict[\"d4\"], \n",
    "\n",
    "                bkpnt_dict[\"r_d1_d2\"],\n",
    "\n",
    "                bkpnt_dict[\"bkpt_date\"], \n",
    "                bkpnt_dict[\"bkpt_date_sub30\"], \n",
    "                bkpnt_dict[\"bkpt_date_add30\"],\n",
    "                bkpnt_dict[\"bkpt_date_add60\"],\n",
    "                bkpnt_dict[\"bkpt_date_add90\"],\n",
    "\n",
    "                bkpnt_dict[\"avg_patrons_bkpnt\"], \n",
    "                bkpnt_dict[\"avg_patrons_sub30\"], \n",
    "                bkpnt_dict[\"avg_patrons_add30\"], \n",
    "                bkpnt_dict[\"avg_patrons_add60\"], \n",
    "                bkpnt_dict[\"avg_patrons_add90\"], \n",
    "\n",
    "                # delta patrons\n",
    "                mean_delta_patrons_sub30,\n",
    "                mean_delta_patrons_add30,\n",
    "                mean_delta_patrons_add60,\n",
    "                mean_delta_patrons_add90,\n",
    "\n",
    "                # delta earnings\n",
    "                mean_delta_earnings_sub30,\n",
    "                mean_delta_earnings_add30,\n",
    "                mean_delta_earnings_add60, \n",
    "                mean_delta_earnings_add90,\n",
    "\n",
    "                # delta videos\n",
    "                mean_delta_videos_sub30,\n",
    "                mean_delta_videos_add30,\n",
    "                mean_delta_videos_add60,\n",
    "                mean_delta_videos_add90,\n",
    "\n",
    "                # delta views\n",
    "                mean_delta_views_sub30,\n",
    "                mean_delta_views_add30,\n",
    "                mean_delta_views_add60,\n",
    "                mean_delta_views_add90,\n",
    "\n",
    "                # delta subscriptions\n",
    "                mean_delta_subs_sub30,\n",
    "                mean_delta_subs_add30,\n",
    "                mean_delta_subs_add60,\n",
    "                mean_delta_subs_add90,\n",
    "\n",
    "                # durations\n",
    "                mean_duration_sub30,\n",
    "                mean_duration_add30,     \n",
    "                mean_duration_add60,    \n",
    "                mean_duration_add90, \n",
    "\n",
    "                # likes\n",
    "                mean_likes_sub30,\n",
    "                mean_likes_add30,  \n",
    "                mean_likes_add60,   \n",
    "                mean_likes_add90\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    df_potential_control = pd.DataFrame(potential_control_tuples, columns = [\n",
    "        'patreon_id',\n",
    "        'yt_channel_id',\n",
    "        'treat',\n",
    "        'd1', \n",
    "        'd2', \n",
    "        'd3',\n",
    "        'd4',\n",
    "\n",
    "        'ratio_d1_d2',\n",
    "        'bkpt_date',     \n",
    "        'bkpt_date_sub30', \n",
    "        'bkpt_date_add30', \n",
    "        'bkpt_date_add60',\n",
    "        'bkpt_date_add90',\n",
    "\n",
    "        'avg_patrons_bkpnt', \n",
    "        'avg_patrons_sub30', \n",
    "        'avg_patrons_add30', \n",
    "        'avg_patrons_add60',\n",
    "        'avg_patrons_add90',\n",
    "\n",
    "        # delta patrons\n",
    "        'mean_delta_patrons_sub30',\n",
    "        'mean_delta_patrons_add30',\n",
    "        'mean_delta_patrons_add60',\n",
    "        'mean_delta_patrons_add90',\n",
    "\n",
    "        # delta earnings\n",
    "        'mean_delta_earnings_sub30',\n",
    "        'mean_delta_earnings_add30',\n",
    "        'mean_delta_earnings_add60',\n",
    "        'mean_delta_earnings_add90',\n",
    "\n",
    "        # delta videos\n",
    "        'mean_delta_videos_sub30',\n",
    "        'mean_delta_videos_add30',\n",
    "        'mean_delta_videos_add60',\n",
    "        'mean_delta_videos_add90',\n",
    "\n",
    "        # delta views\n",
    "        'mean_delta_views_sub30',\n",
    "        'mean_delta_views_add30',\n",
    "        'mean_delta_views_add60',\n",
    "        'mean_delta_views_add90',\n",
    "\n",
    "        # delta subscriptions\n",
    "        'mean_delta_subs_sub30',\n",
    "        'mean_delta_subs_add30',\n",
    "        'mean_delta_subs_add60',\n",
    "        'mean_delta_subs_add90',\n",
    "\n",
    "        # durations\n",
    "        'mean_duration_sub30',\n",
    "        'mean_duration_add30',\n",
    "        'mean_duration_add60',\n",
    "        'mean_duration_add90',\n",
    "\n",
    "        # likes\n",
    "        'mean_likes_sub30',\n",
    "        'mean_likes_add30',\n",
    "        'mean_likes_add60',\n",
    "        'mean_likes_add90'\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Number of Patreon accounts added to the potential_control group:  {len(df_potential_control)}')\n",
    "\n",
    "    df_potential_control['bkpt_date'] = pd.to_datetime(df_potential_control['bkpt_date'])\n",
    "    df_potential_control['bkpt_date_sub30'] = pd.to_datetime(df_potential_control['bkpt_date_sub30'])\n",
    "    df_potential_control['bkpt_date_add30'] = pd.to_datetime(df_potential_control['bkpt_date_add30'])\n",
    "    df_potential_control['bkpt_date_add60'] = pd.to_datetime(df_potential_control['bkpt_date_add60'])\n",
    "    df_potential_control['bkpt_date_add90'] = pd.to_datetime(df_potential_control['bkpt_date_add90'])\n",
    "\n",
    "\n",
    "    df_treat = pd.concat([treated_subject_row, df_potential_control]).reset_index(drop=True)\n",
    "    return df_treat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create DataFrame with potential control subjects per treated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a DF with treated and potential control accounts (takes about 2h15)\n",
    "# exceptions = 0\n",
    "# all_treat_and_potential_control_df = pd.DataFrame()\n",
    "\n",
    "# print(f'Iterate over {len(dict_potential_matches)} treated accounts...')\n",
    "# for idx, (treated_subject, potential_control_list) in enumerate(tqdm(dict_potential_matches.items())):\n",
    "#     # if idx >= 10:\n",
    "#     #     break\n",
    "#     # print(f\"\\nidx: {idx}, treated_subject: {treated_subject}\")\n",
    "#     try: \n",
    "#         df_treat = align_breakpoints(treated_subject, potential_control_list, df_top_pt_accts, df_yt_timeseries_top_pt, df_yt_metadata_pt_filtered)\n",
    "\n",
    "#     except Exception as e: \n",
    "#         exceptions += 1\n",
    "#         # print(\"Exception: \", e)\n",
    "#         continue\n",
    "        \n",
    "#     df_treat['treated_patreon_id'] = treated_subject\n",
    "#     all_treat_and_potential_control_df = pd.concat([all_treat_and_potential_control_df, df_treat])\n",
    "    \n",
    "# print(\"total number of exceptions: \", exceptions)\n",
    "\n",
    "# # move treated_patreon_id column at the beginning of the df\n",
    "# first_column = all_treat_and_potential_control_df.pop('treated_patreon_id')  \n",
    "# all_treat_and_potential_control_df.insert(0, 'treated_patreon_id', first_column)\n",
    "\n",
    "# print(\"all_treat_and_potential_control_df: \")\n",
    "# all_treat_and_potential_control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save \"all_treat_and_potential_control_df\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"all_treat_and_potential_control_df.tsv.gz\"\n",
    "# all_treat_and_potential_control_df.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')\n",
    "# !ls -lh {LOCAL_DATA_FOLDER}all_treat_and_potential_control_df.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3 Naive analysis\n",
    "Compare the distribution of the outcome variables (`mean_delta_videos_add30`, `mean_delta_views_add30`, `mean_delta_subs_add30`, `mean_duration_add30`) between the two groups, using plots and numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_treat_and_potential_control_df = pd.read_csv(LOCAL_DATA_FOLDER+\"all_treat_and_potential_control_df.tsv.gz\", sep=\"\\t\", compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split \"treated\" VS \"non-treated\" Patreon accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the following caracteristics for the Treated vs Non-Treated accounts:\n",
    "\n",
    "    \n",
    "- **Treated** accounts\n",
    "    - accounts for which we have detected breakpoints in section 2.2\n",
    "\n",
    "\n",
    "- **Non-Treated / Control** accounts\n",
    "    - all the other accounts. \n",
    "    \n",
    "**Important Note**: since we have selected multiple potential Control accounts for each Treated account among the same pool of accounts, and we have not done the matching yet, some Control accounts are being duplicated (with different \"alignement\" dates) accross different potential control groups. This results in far more potential Control accounts than Treated accounts (~130 control accounts for 1 treated account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = all_treat_and_potential_control_df[all_treat_and_potential_control_df['treat'] == 1]\n",
    "control = all_treat_and_potential_control_df[all_treat_and_potential_control_df['treat'] == 0]\n",
    "\n",
    "print(f'Total number of accounts (some being duplicated accross potential control groups) : {len(all_treat_and_potential_control_df):>6,}')\n",
    "print(f'Number of treated accounts (with a breakpoint)                                    : {len(treated):>6,}')\n",
    "print(f'Number of potential control accounts (without a breakpoint)                       : {len(control):>6,}')\n",
    "print(f'Avg number of potential control accounts per treated accounts                     ~ {len(control)/len(treated):>6,.0f}')\n",
    "with pd.option_context('display.max_rows', 150, 'display.min_rows', 150, 'display.max_columns', 50):\n",
    "    display(all_treat_and_potential_control_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_variables = ['mean_delta_videos_add30', 'mean_delta_views_add30', 'mean_delta_subs_add30', 'mean_duration_add30']\n",
    "outcome_delta_variables = ['mean_delta_videos_add30', 'mean_delta_views_add30', 'mean_delta_subs_add30', 'mean_duration_add30']\n",
    "print(\"Descriptive statistics of the outcome variables during month after breakpoint\\n\")\n",
    "print(\"Treated group:\")\n",
    "display(treated[outcome_variables].describe().T)\n",
    "\n",
    "print(\"Control group\")\n",
    "display(control[outcome_variables].describe().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore outcome variables' distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in outcome_delta_variables:\n",
    "    ax = sns.kdeplot(treated[column], label='treated')\n",
    "    ax = sns.kdeplot(control[column], label='control')\n",
    "    ax.set(title=f'{column} distribution comparison \\n(during 1st month after breakpoint)', xlabel=column, ylabel='Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'treat','d1', 'd2', 'ratio_d1_d2','bkpt_date', 'avg_patrons_sub30', 'mean_delta_patrons_sub30', 'mean_delta_earnings_sub30', \n",
    "    'mean_delta_videos_sub30', 'mean_delta_views_sub30','mean_delta_subs_sub30', 'mean_duration_sub30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairwise relationships\n",
    "sns.pairplot(all_treat_and_potential_control_df[features + outcome_variables])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAREX = 'row'\n",
    "# SHAREY = 'row'\n",
    "SHAREX = False\n",
    "SHAREY = False\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "selected_cols = ['avg_patrons_sub30', 'mean_delta_patrons_sub30', 'mean_delta_views_sub30']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharex=SHAREX, sharey=SHAREY)\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.boxplot(x='treat', y=col, hue='treat', data=all_treat_and_potential_control_df, ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Propensity Score Matching (PSM)\n",
    "_(this section has been inspired by ADA 2021 Observational study exercise session by Tiziano Piccardi and Kristina Gligoric)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Propensity score model\n",
    "-  Use logistic regression to estimate propensity scores for all Patreon accounts in the dataset. \n",
    "-  Use statsmodels to fit the logistic regression model and apply it to each account to obtain its propensity score.\n",
    "\n",
    "The propensity score of an account represents its probability of receiving the treatment, based on its pre-treatment features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Treatment** (binary variable):\n",
    "    - `treat` = 1 (treatment group), if we detect a breakpoint\n",
    "    - `treat` = 0 (control group), if we don't detect a breakpoint\n",
    "        \n",
    "- **Outcome**\n",
    "    - `mean_delta_videos_after`: YouTube delta views (post-treatment)\n",
    "    \n",
    "- **Observed covariates:**\n",
    "    - `mean_delta_patrons_sub30`: Patreon delta patrons mean (pre-treatment) \n",
    "    - `mean_delta_videos_sub30`: YouTube delta videos mean (pre-treatment) \n",
    "    - `mean_delta_views_sub30`:  YouTube delta views mean (pre-treatment) \n",
    "    - `mean_delta_subs_sub30`:   YouTube delta subs mean (pre-treatment) \n",
    "    - (`mean_duration_sub30`:     YouTube video duration mean (pre-treatment))\n",
    "    \n",
    "    \n",
    "    \n",
    "_Note: \"pre-treatment\" = month (30 days) before breakpoint and \"post-treatement\" = 1 month (30 days) after breakpoint_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Propensity score and match pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\n",
    "    'd1', 'd2', 'd3', 'd4',\n",
    "    'ratio_d1_d2',\n",
    "    'avg_patrons_sub30', 'avg_patrons_bkpnt', 'avg_patrons_add30', 'avg_patrons_add60', 'avg_patrons_add90',\n",
    "    'mean_delta_patrons_sub30', 'mean_delta_patrons_add30', 'mean_delta_patrons_add60', 'mean_delta_patrons_add90',\n",
    "    'mean_delta_earnings_sub30', 'mean_delta_earnings_add30', 'mean_delta_earnings_add60','mean_delta_earnings_add90',\n",
    "    'mean_delta_videos_sub30', 'mean_delta_videos_add30', 'mean_delta_videos_add60', 'mean_delta_videos_add90',\n",
    "    'mean_delta_views_sub30','mean_delta_views_add30', 'mean_delta_views_add60', 'mean_delta_views_add90',\n",
    "    'mean_delta_subs_sub30', 'mean_delta_subs_add30', 'mean_delta_subs_add60', 'mean_delta_subs_add90',\n",
    "    'mean_duration_sub30', 'mean_duration_add30', 'mean_duration_add60', 'mean_duration_add90',\n",
    "    'mean_likes_sub30', 'mean_likes_add30', 'mean_likes_add60', 'mean_likes_add90'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(df, features_list):\n",
    "    # standardize the continuous features\n",
    "    df_std = df.copy()\n",
    "    for feature in features_list:\n",
    "        df_std[feature] = (df_std[feature] - df_std[feature].mean())/df_std[feature].std()\n",
    "    \n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_weight_matching(df):\n",
    "    # Separate the treatment and control groups\n",
    "    treatment_df = df[df['treat'] == 1]\n",
    "    control_df   = df[df['treat'] == 0]\n",
    "\n",
    "    # Create an empty undirected graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Loop through all the pairs of instances\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "        for control_id, control_row in control_df.iterrows():\n",
    "\n",
    "            # Calculate the similarity \n",
    "            similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                        treatment_row['Propensity_score'])\n",
    "\n",
    "            # Add an edge between the two instances weighted by the similarity between them\n",
    "            G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "    # Generate and return the maximum weight matching on the generated graph\n",
    "    matching = nx.max_weight_matching(G)\n",
    "    return matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "all_treat_and_potential_control_df.groupby('treated_patreon_id')['patreon_id'].count()\n",
    "treated_subjects = all_treat_and_potential_control_df.treated_patreon_id.unique()\n",
    "\n",
    "\n",
    "# match pairs\n",
    "exceptions = 0\n",
    "all_pairs_df = pd.DataFrame()\n",
    "match_pairs_dict = {} # treatment: control\n",
    "display_cols = ['patreon_id', 'treat', 'Propensity_score', 'd1', 'd2', 'ratio_d1_d2', 'bkpt_date', 'avg_patrons_sub30', 'mean_delta_videos_sub30', 'mean_delta_views_sub30', 'mean_delta_subs_sub30', 'mean_duration_sub30']\n",
    "non_na_cols  = ['patreon_id', 'treat', 'd1', 'd2', 'ratio_d1_d2', 'bkpt_date', 'avg_patrons_sub30', 'mean_delta_videos_sub30', 'mean_delta_views_sub30', 'mean_delta_subs_sub30']\n",
    "\n",
    "print(f'Iterate over {len(treated_subjects)} treated accounts...')\n",
    "# for idx, (treated_subject, potential_control_list) in enumerate(tqdm(dict_potential_matches.items())):\n",
    "for idx, treated_subject in enumerate(tqdm(treated_subjects)):\n",
    "    # print(f\"\\nidx: {idx}, treated_subject: {treated_subject}\")\n",
    "    \n",
    "    # restrict dataframe to this treated account\n",
    "    df_treat = all_treat_and_potential_control_df[all_treat_and_potential_control_df['treated_patreon_id'] == treated_subject]\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        # drop rows only from control accounts which have NA values in the columns of interest\n",
    "        df_treat = df_treat.drop(df_treat[(df_treat['treat'] == 0) & (df_treat[non_na_cols].isna().any(axis=1))].index)\n",
    "        df_treat = df_treat.reset_index(drop=True)\n",
    "\n",
    "        df_treat_std = standardize_features(df_treat, continuous_features)\n",
    "\n",
    "        #### GET PROPENSITY SCORE ####\n",
    "        # Declare the model\n",
    "        mod = smf.logit(formula='treat ~  mean_delta_patrons_sub30 + mean_delta_videos_sub30 + mean_delta_views_sub30 + mean_delta_subs_sub30', data=df_treat_std)\n",
    "        \n",
    "        # Fits the model (find the optimal coefficients\n",
    "        res = mod.fit(disp=0, warn_convergence=True)\n",
    "\n",
    "        # Extract the estimated propensity scores \n",
    "        df_treat['Propensity_score'] = res.predict()\n",
    "        df_treat_std['Propensity_score'] = res.predict() \n",
    "        # print(res.summary())\n",
    "\n",
    "        #### MATCH PAIRS ####\n",
    "        matching_pair = maximum_weight_matching(df_treat)\n",
    "        matched_list = [i[0] for i in list(matching_pair)] + [i[1] for i in list(matching_pair)]\n",
    "        matched_pair_df = df_treat.iloc[matched_list]\n",
    "\n",
    "\n",
    "        match_pairs_dict[matched_pair_df[matched_pair_df.treat == 1].patreon_id.iloc[0]] = matched_pair_df[matched_pair_df.treat == 0].patreon_id.iloc[0]\n",
    "\n",
    "        all_pairs_df = pd.concat([all_pairs_df, matched_pair_df])\n",
    "    except Exception as e: \n",
    "        exceptions += 1\n",
    "        # print(f\"\\nidx: {idx}, treated_subject: {treated_subject}\")\n",
    "        # print(\"Exception: \", e)\n",
    "        continue\n",
    "\n",
    "    # print(\"\\n\\n ------------------------------------------------------------------------------------------------------------------------------------------------------ \\n\\n\")\n",
    "\n",
    "print(\"\\nnumber of exceptions: \", exceptions)\n",
    "print(\"\\n\\n Dataframe with all pairs: \")\n",
    "all_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_df = all_pairs_df.reset_index(drop=True)\n",
    "all_pairs_df['bkpt_date'] = pd.to_datetime(all_pairs_df['bkpt_date'])\n",
    "all_pairs_df['bkpt_date_sub30'] = pd.to_datetime(all_pairs_df['bkpt_date_sub30'])\n",
    "all_pairs_df['bkpt_date_add30'] = pd.to_datetime(all_pairs_df['bkpt_date_add30'])\n",
    "all_pairs_df['bkpt_date_add60'] = pd.to_datetime(all_pairs_df['bkpt_date_add60'])\n",
    "all_pairs_df['bkpt_date_add90'] = pd.to_datetime(all_pairs_df['bkpt_date_add90'])\n",
    "all_pairs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess balance in covariates using SMD\n",
    "To assess the balance in covariates after PSM, we calculate the standard mean difference (SMD) of a feature between the treatment and control groups\n",
    "(_source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3144483/#s11title_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T = Treatment;\n",
    "C = Control\n",
    "$$ \n",
    "SMD = \\frac{ \\overline{T} - \\overline{C} } {\\sqrt{\\frac{(S_T^2 + S_C^2)}{2}}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_smd =  ['avg_patrons_sub30','mean_delta_patrons_sub30','mean_delta_earnings_sub30','mean_delta_videos_sub30',\n",
    "                 'mean_delta_views_sub30','mean_delta_subs_sub30','mean_duration_sub30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tableone package: https://github.com/tompollard/tableone\n",
    "data=all_pairs_df\n",
    "columns = features_smd\n",
    "groupby = ['treat']\n",
    "labels={'treat'                    : 'Treatment',\n",
    "        'avg_patrons_sub30'        : 'Pre-treatment Avg #Patrons',\n",
    "        'mean_delta_patrons_sub30' : 'Pre-treatment mean ΔPatrons',\n",
    "        'mean_delta_earnings_sub30': 'Pre-treatment mean ΔEarnings',\n",
    "        'mean_delta_videos_sub30'  : 'Pre-treatment mean ΔVideos',\n",
    "        'mean_delta_views_sub30'   : 'Pre-treatment mean ΔViews',\n",
    "        'mean_delta_subs_sub30'    : 'Pre-treatment mean ΔSubscriptions',\n",
    "        'mean_duration_sub30'      : 'Pre-treatment mean ΔVideo durations',\n",
    "       }\n",
    "table1 = TableOne(data, columns=columns, groupby=groupby, rename=labels, pval=True, smd=True)\n",
    "# print(mytable.tabulate(tablefmt = \"fancy_grid\"))\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results to latex format\n",
    "# table1.to_latex('table1LaTeX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore outcome variables' distribution (after matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_balanced = all_pairs_df.loc[all_pairs_df['treat'] == 1] \n",
    "control_balanced = all_pairs_df.loc[all_pairs_df['treat'] == 0] \n",
    "\n",
    "print(f'Total number of accounts                                                          : {len(all_pairs_df):>6,}')\n",
    "print(f'Number of treated accounts (with a breakpoint)                                    : {len(treated):>6,}')\n",
    "print(f'Number of potential control accounts (without a breakpoint)                       : {len(control):>6,}')\n",
    "print(f'Avg number of potential control accounts per treated accounts                     ~ {len(control)/len(treated):>6,.0f}')\n",
    "with pd.option_context('display.max_rows', 150, 'display.min_rows', 150, 'display.max_columns', 50):\n",
    "    display(all_treat_and_potential_control_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive statistics of the \\\"balanced\\\" outcome variables during month after breakpoint\\n\")\n",
    "print(\"\\\"Balanced\\\" Treated group:\")\n",
    "display(treated_balanced[outcome_variables].describe().T)\n",
    "\n",
    "print(\"\\\"Balanced\\\" Control group\")\n",
    "display(control_balanced[outcome_variables].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in outcome_delta_variables:\n",
    "    ax = sns.kdeplot(treated_balanced[column], label='treated (balanced)')\n",
    "    ax = sns.kdeplot(control_balanced[column], label='control (balanced)')\n",
    "    ax.set(title=f'{column} distribution comparison \\n(during 1st month after breakpoint)', xlabel=column, ylabel='Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Compute difference between means of the different periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute increases between means of all 4 regions\n",
    "all_pairs_df['diff_delta_patrons_d1'] = all_pairs_df['mean_delta_patrons_add30'] - all_pairs_df['mean_delta_patrons_sub30']\n",
    "all_pairs_df['diff_delta_patrons_d2'] = all_pairs_df['mean_delta_patrons_add60'] - all_pairs_df['mean_delta_patrons_sub30']\n",
    "all_pairs_df['diff_delta_patrons_d3'] = all_pairs_df['mean_delta_patrons_add90'] - all_pairs_df['mean_delta_patrons_sub30']\n",
    "\n",
    "all_pairs_df['diff_delta_earnings_d1'] = all_pairs_df['mean_delta_earnings_add30'] - all_pairs_df['mean_delta_earnings_sub30']\n",
    "all_pairs_df['diff_delta_earnings_d2'] = all_pairs_df['mean_delta_earnings_add60'] - all_pairs_df['mean_delta_earnings_sub30']\n",
    "all_pairs_df['diff_delta_earnings_d3'] = all_pairs_df['mean_delta_earnings_add90'] - all_pairs_df['mean_delta_earnings_sub30']\n",
    "\n",
    "all_pairs_df['diff_delta_videos_d1'] = all_pairs_df['mean_delta_videos_add30'] - all_pairs_df['mean_delta_videos_sub30']\n",
    "all_pairs_df['diff_delta_videos_d2'] = all_pairs_df['mean_delta_videos_add60'] - all_pairs_df['mean_delta_videos_sub30']\n",
    "all_pairs_df['diff_delta_videos_d3'] = all_pairs_df['mean_delta_videos_add90'] - all_pairs_df['mean_delta_videos_sub30']\n",
    "\n",
    "all_pairs_df['diff_delta_views_d1'] = all_pairs_df['mean_delta_views_add30'] - all_pairs_df['mean_delta_views_sub30']\n",
    "all_pairs_df['diff_delta_views_d2'] = all_pairs_df['mean_delta_views_add60'] - all_pairs_df['mean_delta_views_sub30']\n",
    "all_pairs_df['diff_delta_views_d3'] = all_pairs_df['mean_delta_views_add90'] - all_pairs_df['mean_delta_views_sub30']\n",
    "\n",
    "all_pairs_df['diff_delta_subs_d1'] = all_pairs_df['mean_delta_subs_add30'] - all_pairs_df['mean_delta_subs_sub30']\n",
    "all_pairs_df['diff_delta_subs_d2'] = all_pairs_df['mean_delta_subs_add60'] - all_pairs_df['mean_delta_subs_sub30']\n",
    "all_pairs_df['diff_delta_subs_d3'] = all_pairs_df['mean_delta_subs_add90'] - all_pairs_df['mean_delta_subs_sub30']\n",
    "\n",
    "all_pairs_df['diff_duration_d1'] = all_pairs_df['mean_duration_add30'] - all_pairs_df['mean_duration_sub30']\n",
    "all_pairs_df['diff_duration_d2'] = all_pairs_df['mean_duration_add60'] - all_pairs_df['mean_duration_sub30']\n",
    "all_pairs_df['diff_duration_d3'] = all_pairs_df['mean_duration_add90'] - all_pairs_df['mean_duration_sub30']\n",
    "\n",
    "all_pairs_df['diff_likes_d1'] = all_pairs_df['mean_likes_add30'] - all_pairs_df['mean_likes_sub30']\n",
    "all_pairs_df['diff_likes_d2'] = all_pairs_df['mean_likes_add60'] - all_pairs_df['mean_likes_sub30']\n",
    "all_pairs_df['diff_likes_d3'] = all_pairs_df['mean_likes_add90'] - all_pairs_df['mean_likes_sub30']\n",
    "all_pairs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "pd.set_option('display.max_columns', 70)\n",
    "columns_order = [\n",
    "            'treated_patreon_id', 'patreon_id', 'yt_channel_id', 'treat','Propensity_score',\n",
    "            'd1', 'd2', 'd3', 'd4',\n",
    "            'ratio_d1_d2',\n",
    "            'bkpt_date_sub30', 'bkpt_date', 'bkpt_date_add30', 'bkpt_date_add60', 'bkpt_date_add90',\n",
    "            'avg_patrons_sub30', 'avg_patrons_bkpnt', 'avg_patrons_add30', 'avg_patrons_add60', 'avg_patrons_add90',\n",
    "            'mean_delta_patrons_sub30', 'mean_delta_patrons_add30', 'mean_delta_patrons_add60', 'mean_delta_patrons_add90',\n",
    "            'mean_delta_earnings_sub30', 'mean_delta_earnings_add30', 'mean_delta_earnings_add60','mean_delta_earnings_add90',\n",
    "            'mean_delta_videos_sub30', 'mean_delta_videos_add30', 'mean_delta_videos_add60', 'mean_delta_videos_add90',\n",
    "            'mean_delta_views_sub30','mean_delta_views_add30', 'mean_delta_views_add60', 'mean_delta_views_add90',\n",
    "            'mean_delta_subs_sub30', 'mean_delta_subs_add30', 'mean_delta_subs_add60', 'mean_delta_subs_add90',\n",
    "            'mean_duration_sub30', 'mean_duration_add30', 'mean_duration_add60', 'mean_duration_add90',\n",
    "            'mean_likes_sub30', 'mean_likes_add30', 'mean_likes_add60', 'mean_likes_add90',\n",
    "            'diff_delta_patrons_d1', 'diff_delta_patrons_d2', 'diff_delta_patrons_d3',\n",
    "            'diff_delta_earnings_d1', 'diff_delta_earnings_d2','diff_delta_earnings_d3',\n",
    "            'diff_delta_videos_d1','diff_delta_videos_d2', 'diff_delta_videos_d3',\n",
    "            'diff_delta_views_d1','diff_delta_views_d2', 'diff_delta_views_d3',\n",
    "            'diff_delta_subs_d1','diff_delta_subs_d2', 'diff_delta_subs_d3',\n",
    "            'diff_duration_d1','diff_duration_d2', 'diff_duration_d3',\n",
    "            'diff_likes_d1','diff_likes_d2', 'diff_likes_d3'\n",
    "]\n",
    "\n",
    "all_pairs_df = all_pairs_df[columns_order]\n",
    "all_pairs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter pairs according to increase of the treated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a balanced dataset, and that we know which pairs are matched together, we can change the threshold of increase of the treated pair to restrict the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TREATED_INCR_RATIO_THRESH = 3\n",
    "print(f\"Treated increase threshold: ratio > {TREATED_INCR_RATIO_THRESH}\")\n",
    "\n",
    "# filter pairs according to increase of the treated\n",
    "filtered_list = all_pairs_df[(all_pairs_df['treat'] == 1) & (all_pairs_df['ratio_d1_d2'] > TREATED_INCR_RATIO_THRESH)].treated_patreon_id\n",
    "print(f\"Number of treated accounts:              {len(filtered_list):>3}\")\n",
    "\n",
    "all_pairs_df_filt = all_pairs_df[all_pairs_df['treated_patreon_id'].isin(filtered_list)]\n",
    "print(f\"Number of corresponding matched pairs:   {len(all_pairs_df_filt):>3}\")\n",
    "\n",
    "all_pairs_df_filt.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots\n",
    "diffs of delta means 1, 2 and 3 months after breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "treated_balanced_filt = all_pairs_df_filt.loc[all_pairs_df_filt['treat'] == 1] \n",
    "control_balanced_filt = all_pairs_df_filt.loc[all_pairs_df_filt['treat'] == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "x_values = [0,1]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(6,6), sharex=False, sharey='row')\n",
    "   \n",
    "sns.stripplot(x=\"treat\", y='ratio_d1_d2', data=all_pairs_df_filt, ax=axs)\n",
    "axs.set(title=f'Distribution of ratio_d1_d2')\n",
    "axs.set_ylabel('ratio_d1_d2')\n",
    "axs.plot(x_values, [control_balanced_filt['ratio_d1_d2'].mean(), treated_balanced_filt['ratio_d1_d2'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "\n",
    "# plot a horizontal line at the increase ratio threshold of the treated group\n",
    "# axs.hlines(y=TREATED_INCR_RATIO_THRESH, xmin=-0.1, xmax=1.1, linewidth=2, linestyle='--', color='green')\n",
    "\n",
    "plt.suptitle(\"Sanity check with patreons variables\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(x=\"treat\", y=\"ratio_d1_d2\", data=all_pairs_df_filt, scatter_kws={\"s\": 15}, x_jitter=.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = [0,1]\n",
    "\n",
    "# sanity check\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15,8), sharex=False, sharey='row')\n",
    "\n",
    "# diff delta patrons distribution\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_patrons_d1', data=all_pairs_df_filt, ax=axs[0,0])\n",
    "axs[0,0].plot(x_values, [control_balanced_filt['diff_delta_patrons_d1'].mean(), treated_balanced_filt['diff_delta_patrons_d1'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[0,0].set(title=f'Distribution of diff_delta_patrons_d1')\n",
    "axs[0,0].set_ylabel('diff_delta_patrons_d1')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_patrons_d2', data=all_pairs_df_filt, ax=axs[0,1])\n",
    "axs[0,1].plot(x_values, [control_balanced_filt['diff_delta_patrons_d2'].mean(), treated_balanced_filt['diff_delta_patrons_d2'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[0,1].set(title=f'Distribution of diff_delta_patrons_d2')\n",
    "axs[0,1].set_ylabel('diff_delta_patrons_d2')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_patrons_d3', data=all_pairs_df_filt, ax=axs[0,2])\n",
    "axs[0,2].plot(x_values, [control_balanced_filt['diff_delta_patrons_d3'].mean(), treated_balanced_filt['diff_delta_patrons_d3'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[0,2].set(title=f'Distribution of diff_delta_patrons_d3')\n",
    "axs[0,2].set_ylabel('diff_delta_patrons_d3')\n",
    "\n",
    "\n",
    "# diff delta earnings distribution\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_earnings_d1', data=all_pairs_df_filt, ax=axs[1,0])\n",
    "axs[1,0].plot(x_values, [control_balanced_filt['diff_delta_earnings_d1'].mean(), treated_balanced_filt['diff_delta_earnings_d1'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[1,0].set(title=f'Distribution of diff_delta_earnings_d1')\n",
    "axs[1,0].set_ylabel('diff_delta_earnings_d1')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_earnings_d2', data=all_pairs_df_filt, ax=axs[1,1])\n",
    "axs[1,1].plot(x_values, [control_balanced_filt['diff_delta_earnings_d2'].mean(), treated_balanced_filt['diff_delta_earnings_d2'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[1,1].set(title=f'Distribution of diff_delta_earnings_d2')\n",
    "axs[1,1].set_ylabel('diff_delta_earnings_d2')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_earnings_d3', data=all_pairs_df_filt, ax=axs[1,2])\n",
    "axs[1,2].plot(x_values, [control_balanced_filt['diff_delta_earnings_d3'].mean(), treated_balanced_filt['diff_delta_earnings_d3'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[1,2].set(title=f'Distribution of diff_delta_earnings_d3')\n",
    "axs[1,2].set_ylabel('diff_delta_earnings_d3')\n",
    "\n",
    "\n",
    "plt.suptitle(\"Sanity check with Patreons variables\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['diff_delta_patrons_d1', 'diff_delta_earnings_d1']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.kdeplot(control_balanced_filt[col], label='control', ax=ax)\n",
    "    sns.kdeplot(treated_balanced_filt[col], label='treated', ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(col)\n",
    "    # ax.set(yscale=\"log\")\n",
    "    ax.legend()\n",
    "plt.suptitle(\"Sanity check with Patreons variables\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change Y scale to log scale. Warning, will fail to plot negative values when set to True\n",
    "Y_LOG_SCALE = False\n",
    "\n",
    "# fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(10,10), sharex=False, sharey='row')\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(10,10), sharex=False, sharey=False)\n",
    "\n",
    "\n",
    "# videos\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_videos_d1', data=all_pairs_df_filt, ax=axs[0,0])\n",
    "axs[0,0].plot(x_values, [control_balanced_filt['diff_delta_videos_d1'].mean(), treated_balanced_filt['diff_delta_videos_d1'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[0,0].set(title=f'Difference of $\\Delta$ videos \\n(after 1 month)')\n",
    "axs[0,0].set_ylabel('Diff. $\\Delta$ videos')\n",
    "axs[0,0].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_videos_d2', data=all_pairs_df_filt, ax=axs[0,1])\n",
    "axs[0,1].plot(x_values, [control_balanced_filt['diff_delta_videos_d2'].mean(), treated_balanced_filt['diff_delta_videos_d2'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[0,1].set(title=f'Difference of $\\Delta$ videos \\n(after 2 months)')\n",
    "axs[0,1].set_ylabel('Diff. $\\Delta$ videos')\n",
    "axs[0,1].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_videos_d3', data=all_pairs_df_filt, ax=axs[0,2])\n",
    "axs[0,2].plot(x_values, [control_balanced_filt['diff_delta_videos_d3'].mean(), treated_balanced_filt['diff_delta_videos_d3'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[0,2].set(title=f'Difference of $\\Delta$ videos \\n(after 3 months)')\n",
    "axs[0,2].set_ylabel('Diff. $\\Delta$ videos')\n",
    "axs[0,2].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "# views\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_views_d1', data=all_pairs_df_filt, ax=axs[1,0])\n",
    "axs[1,0].plot(x_values, [control_balanced_filt['diff_delta_views_d1'].mean(), treated_balanced_filt['diff_delta_views_d1'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[1,0].set(title=f'Difference of $\\Delta$ views \\n(after 1 month)')\n",
    "axs[1,0].set_ylabel('Diff. $\\Delta$ views')\n",
    "axs[1,0].set(yscale=\"symlog\") if Y_LOG_SCALE else None\n",
    "\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_views_d2', data=all_pairs_df_filt, ax=axs[1,1])\n",
    "axs[1,1].plot(x_values, [control_balanced_filt['diff_delta_views_d2'].mean(), treated_balanced_filt['diff_delta_views_d2'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[1,1].set(title=f'Difference of $\\Delta$ views \\n(after 2 months)')\n",
    "axs[1,1].set_ylabel('Diff. $\\Delta$ views')\n",
    "axs[1,1].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_views_d3', data=all_pairs_df_filt, ax=axs[1,2])\n",
    "axs[1,2].plot(x_values, [control_balanced_filt['diff_delta_views_d3'].mean(), treated_balanced_filt['diff_delta_views_d3'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[1,2].set(title=f'Difference of $\\Delta$ views \\n(after 3 months)')\n",
    "axs[1,2].set_ylabel('Diff. $\\Delta$ views')\n",
    "axs[1,2].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "\n",
    "\n",
    "# subs\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_subs_d1', data=all_pairs_df_filt, ax=axs[2,0])\n",
    "axs[2,0].plot(x_values, [control_balanced_filt['diff_delta_subs_d1'].mean(), treated_balanced_filt['diff_delta_subs_d1'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[2,0].set(title=f'Difference of $\\Delta$ subscriptions \\n(after 1 month)')\n",
    "axs[2,0].set_ylabel('Diff. $\\Delta$ subs')\n",
    "axs[2,0].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_subs_d2', data=all_pairs_df_filt, ax=axs[2,1])\n",
    "axs[2,1].plot(x_values, [control_balanced_filt['diff_delta_subs_d2'].mean(), treated_balanced_filt['diff_delta_subs_d2'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[2,1].set(title=f'Difference of $\\Delta$ subscriptions \\n(after 2 months)')\n",
    "axs[2,1].set_ylabel('Diff. $\\Delta$ subs')\n",
    "axs[2,1].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_subs_d3', data=all_pairs_df_filt, ax=axs[2,2])\n",
    "axs[2,2].plot(x_values, [control_balanced_filt['diff_delta_subs_d3'].mean(), treated_balanced_filt['diff_delta_subs_d3'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "axs[2,2].set(title=f'Difference of $\\Delta$ subscriptions \\n(after 3 months)')\n",
    "axs[2,2].set_ylabel('Diff. $\\Delta$ subs')\n",
    "axs[2,2].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "\n",
    "# # duration\n",
    "# sns.stripplot(x=\"treat\", y='diff_duration_d1', data=all_pairs_df_filt, ax=axs[3,0])\n",
    "# axs[3,0].plot(x_values, [control_balanced_filt['diff_duration_d1'].mean(), treated_balanced_filt['diff_duration_d1'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "# axs[3,0].set(title=f'Difference of videos durations \\n(after 1 month)')\n",
    "# axs[3,0].set_ylabel('Diff. duration')\n",
    "# axs[3,0].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "# sns.stripplot(x=\"treat\", y='diff_duration_d2', data=all_pairs_df_filt, ax=axs[3,1])\n",
    "# axs[3,1].plot(x_values, [control_balanced_filt['diff_duration_d2'].mean(), treated_balanced_filt['diff_duration_d2'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "# axs[3,1].set(title=f'Difference of videos durations \\n(after 2 months)')\n",
    "# axs[3,1].set_ylabel('Diff. duration')\n",
    "# axs[3,1].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "# sns.stripplot(x=\"treat\", y='diff_duration_d3', data=all_pairs_df_filt, ax=axs[3,2])\n",
    "# axs[3,2].plot(x_values, [control_balanced_filt['diff_duration_d3'].mean(), treated_balanced_filt['diff_duration_d3'].mean()], linestyle=\"-\", color='green', alpha=0.5)\n",
    "# axs[3,2].set(title=f'Difference of videos durations \\n(after 3 months)')\n",
    "# axs[3,2].set_ylabel('Diff. duration')\n",
    "# axs[3,2].set(yscale=\"log\") if Y_LOG_SCALE else None\n",
    "\n",
    "\n",
    "plt.suptitle(f\"Difference between the delta videos, views, subscribers \\n(# patrons increase ratio > {TREATED_INCR_RATIO_THRESH} for treated group) \\n({len(all_pairs_df_filt)} observations)\",fontsize=14)\n",
    "fig.tight_layout(pad=2, w_pad=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNIFICANCE_LEVEL = 0.05\n",
    "\n",
    "# Declare the model\n",
    "predictors = [\n",
    "            # 'diff_delta_patrons_d1', 'diff_delta_patrons_d2', 'diff_delta_patrons_d3',\n",
    "            # 'diff_delta_earnings_d1', 'diff_delta_earnings_d2','diff_delta_earnings_d3',\n",
    "            'diff_delta_videos_d1','diff_delta_videos_d2', 'diff_delta_videos_d3',\n",
    "            'diff_delta_views_d1','diff_delta_views_d2', 'diff_delta_views_d3',\n",
    "            'diff_delta_subs_d1','diff_delta_subs_d2', 'diff_delta_subs_d3',\n",
    "            # 'diff_duration_d1','diff_duration_d2', 'diff_duration_d3'\n",
    "]\n",
    "\n",
    "print(f\"Tests at significance level α={SIGNIFICANCE_LEVEL}:\")\n",
    "for idx, predictor in enumerate(predictors):\n",
    "    formula_string = 'treat ~ '+predictor\n",
    "    mod = smf.ols(formula=formula_string, data=all_pairs_df_filt)\n",
    "    np.random.seed(2)\n",
    "    res = mod.fit()\n",
    "    if idx % 3 == 0:\n",
    "        print(\"\\n\")\n",
    "    significance = \"-> Statistically significant\" if (res.pvalues[1] < 0.05) else \"\"\n",
    "    print(f'{formula_string}\\t: R2={res.rsquared:.2f}, p-value={res.pvalues[1]:.2f} {significance}')\n",
    "    \n",
    "    # print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAREX = 'row'\n",
    "# SHAREY = 'row'\n",
    "SHAREX = False\n",
    "SHAREY = False\n",
    "\n",
    "# diff delta videos\n",
    "selected_cols = ['diff_delta_videos_d1', 'diff_delta_videos_d2', 'diff_delta_videos_d3']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharex=SHAREX, sharey=SHAREY)\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.kdeplot(control_balanced_filt[col], label='control', ax=ax)\n",
    "    sns.kdeplot(treated_balanced_filt[col], label='treated', ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    # ax.set(yscale=\"log\")\n",
    "    ax.legend()\n",
    "plt.suptitle(f\"Distribution of the difference of delta videos after 1, 2 and 3 months \\n(# patrons increase ratio > {TREATED_INCR_RATIO_THRESH} for treated group) \\n({len(all_pairs_df_filt)} observations)\",fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# diff delta views\n",
    "selected_cols = ['diff_delta_views_d1', 'diff_delta_views_d2', 'diff_delta_views_d3']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharex=SHAREX, sharey=SHAREY)\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.kdeplot(control_balanced_filt[col], label='control', ax=ax)\n",
    "    sns.kdeplot(treated_balanced_filt[col], label='treated', ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    # ax.set(yscale=\"log\")\n",
    "    ax.legend()\n",
    "plt.suptitle(f\"Distribution of the difference of delta views after 1, 2 and 3 months \\n(# patrons increase ratio > {TREATED_INCR_RATIO_THRESH} for treated group) \\n({len(all_pairs_df_filt)} observations)\",fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# diff delta subscriptions\n",
    "selected_cols = ['diff_delta_subs_d1', 'diff_delta_subs_d2', 'diff_delta_subs_d3']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharex=SHAREX, sharey=SHAREY)\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.kdeplot(control_balanced_filt[col], label='control', ax=ax)\n",
    "    sns.kdeplot(treated_balanced_filt[col], label='treated', ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    # ax.set(yscale=\"log\")\n",
    "    ax.legend()\n",
    "plt.suptitle(f\"Distribution of the difference of delta subscriptions after 1, 2 and 3 months \\n(# patrons increase ratio > {TREATED_INCR_RATIO_THRESH} for treated group) \\n({len(all_pairs_df_filt)} observations)\",fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAREX = 'row'\n",
    "# SHAREY = 'row'\n",
    "SHAREX = False\n",
    "SHAREY = False\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# diff delta videos\n",
    "selected_cols = ['diff_delta_videos_d1', 'diff_delta_videos_d2', 'diff_delta_videos_d3']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharex=SHAREX, sharey=SHAREY)\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.boxplot(y='treat', x=col, hue='treat', data=all_pairs_df_filt, ax=ax, orient=\"h\")\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.legend()\n",
    "plt.suptitle(f\"Distribution of the difference of delta videos after 1, 2 and 3 months \\n(# patrons increase ratio > {TREATED_INCR_RATIO_THRESH} for treated group) \\n({len(all_pairs_df_filt)} observations)\",fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# diff delta views\n",
    "selected_cols = ['diff_delta_views_d1', 'diff_delta_views_d2', 'diff_delta_views_d3']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharex=SHAREX, sharey=SHAREY)\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.boxplot(y='treat', x=col, hue='treat', data=all_pairs_df_filt, ax=ax, orient=\"h\")\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.legend()\n",
    "plt.suptitle(f\"Distribution of the difference of delta views after 1, 2 and 3 months \\n(# patrons increase ratio > {TREATED_INCR_RATIO_THRESH} for treated group) \\n({len(all_pairs_df_filt)} observations)\",fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# diff delta subscriptions\n",
    "selected_cols = ['diff_delta_subs_d1', 'diff_delta_subs_d2', 'diff_delta_subs_d3']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharex=SHAREX, sharey=SHAREY)\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.boxplot(y='treat', x=col, hue='treat', data=all_pairs_df_filt, ax=ax, orient=\"h\")\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(col)\n",
    "    ax.legend()\n",
    "plt.suptitle(f\"Distribution of the difference of delta subscriptions after 1, 2 and 3 months \\n(# patrons increase ratio > {TREATED_INCR_RATIO_THRESH} for treated group) \\n({len(all_pairs_df_filt)} observations)\",fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series plots (balanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_ACCOUNTS_TO_PLOT = 4\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in tqdm(all_pairs_df[:NUMBER_OF_ACCOUNTS_TO_PLOT].iterrows(), total=all_pairs_df[:NUMBER_OF_ACCOUNTS_TO_PLOT].shape[0]):\n",
    "    patreon = row['patreon_id']\n",
    "    \n",
    "    fig, axs = plt.subplots(7, 4, figsize=(26, 10), sharey=False, sharex=False)\n",
    "        \n",
    "    \n",
    "    ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = restrict_acct_and_sort_df(df_top_pt_accts, 'patreon', patreon, 'date')\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = restrict_acct_and_sort_df(df_yt_timeseries_top_pt, 'patreon_id', patreon, 'datetime')\n",
    "\n",
    "    # youtube metadata\n",
    "    tmp_df_yt_meta = restrict_acct_and_sort_df(df_yt_metadata_pt_filtered, 'patreon_id', patreon, 'upload_date')\n",
    "    \n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    ch_ids = tmp_df_yt['channel'].unique()\n",
    "    print(f\"\\n\\n\\n\\033[1m {idx+1}: {patreon[12:]} (treat = {row['treat']})\\033[0m\")\n",
    "    print(f\"https://www.{patreon}\")\n",
    "    print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    for ch_id in ch_ids:\n",
    "        print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "   \n",
    "    print(f'\\nYouTube Metadata: ')\n",
    "    \n",
    "    if not (tmp_df_yt_meta.empty):\n",
    "        print('• YT videos were uploaded between {} and {}'.format(tmp_df_yt_meta['upload_date'].min().strftime('%B %d, %Y'),\n",
    "                                                                 tmp_df_yt_meta['upload_date'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "        print('• YT metadata was crawled between {} and {}'.format(tmp_df_yt_meta['crawl_date'].min().strftime('%B %d, %Y'),\n",
    "                                                                 tmp_df_yt_meta['crawl_date'].max().strftime('%B %d, %Y')))\n",
    "    else:\n",
    "        print('• No metadata available for this acount')\n",
    "\n",
    "       \n",
    "    ########################## RESTRICT PATREON AND YOUTUBE TIME SERIES TO OVERLAPPING DATES ##########################\n",
    "    \n",
    "   # if no overlap period between YT and Patreon datasets, raise exception and skip account\n",
    "    try: \n",
    "        tmp_df_pt, tmp_df_yt, date_min, date_max = restrict_to_overlapping_dates(tmp_df_pt, tmp_df_yt, 'date', 'datetime')\n",
    "    except Exception as e: \n",
    "        print(f\"Exception: {patreon} and {yt_channel_id}: {e} --> skipping account\")        \n",
    "        no_overlap += 1\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "              \n",
    "    ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "    print(\"Breakpoint date: \", breakpoint_date.date())\n",
    "\n",
    "    # check that dates prior and after breakpoint exist\n",
    "    if not (((breakpoint_date - 1*MONTH_OFFSET)) in ts_pt_df.index and ((breakpoint_date + 4*MONTH_OFFSET) in ts_pt_df.index)):\n",
    "        print(f\"ERROR: Breakpoint too close to edge of patreon time series or missing data\\n\")\n",
    "        plt.figure().clear(); plt.close(); plt.cla(); plt.clf(); plt.show()\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    avg_patrons_add60 = row['avg_patrons_add60']\n",
    "    avg_patrons_add90 = row['avg_patrons_add90']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    bkpt_date_add60 = row['bkpt_date_add60']\n",
    "    bkpt_date_add90 = row['bkpt_date_add90']\n",
    "\n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "    d3 = row['d3']\n",
    "    d4 = row['d4']\n",
    "\n",
    "    \n",
    "    r_d1_d2 = row['ratio_d1_d2']\n",
    "\n",
    "    print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "    print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "    print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "    print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    print(f'• At breakpoint + 60days ({bkpt_date_add60.date()}): {avg_patrons_add60:,.1f}')\n",
    "    print(f'• At breakpoint + 90days ({bkpt_date_add90.date()}): {avg_patrons_add90:,.1f}')\n",
    "    \n",
    "    print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "    print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date_add30.date()} to {bkpt_date_add60.date()}:        d3  = {d3:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date_add60.date()} to {bkpt_date_add90.date()}:        d4  = {d4:>+6.1f} patrons\")\n",
    "    \n",
    "    print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "    print(f\"• Ratio between 2 increases:                            d2/d1  = {r_d1_d2:.2f}\")\n",
    "    print(f\"• Percentage increase:                              d2/d1*100  = {r_d1_d2:>+.0%}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ################################### GET DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "       \n",
    "    \n",
    "    mean_date_sub30 = row['bkpt_date_sub30'] + (row['bkpt_date']       - row['bkpt_date_sub30'])/2\n",
    "    mean_date_add30 = row['bkpt_date']       + (row['bkpt_date_add30'] - row['bkpt_date'])/2\n",
    "    mean_date_add60 = row['bkpt_date_add30'] + (row['bkpt_date_add60'] - row['bkpt_date_add30'])/2\n",
    "    mean_date_add90 = row['bkpt_date_add60'] + (row['bkpt_date_add90'] - row['bkpt_date_add60'])/2\n",
    "  \n",
    "\n",
    "    mean_dates = [\n",
    "        mean_date_sub30, \n",
    "        mean_date_add30, \n",
    "        mean_date_add60, \n",
    "        mean_date_add90\n",
    "    ]  \n",
    "    \n",
    "    mean_delta_patrons = [\n",
    "        row['mean_delta_patrons_sub30'],\n",
    "        row['mean_delta_patrons_add30'],\n",
    "        row['mean_delta_patrons_add60'],\n",
    "        row['mean_delta_patrons_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_earnings = [\n",
    "        row['mean_delta_earnings_sub30'],\n",
    "        row['mean_delta_earnings_add30'],\n",
    "        row['mean_delta_earnings_add60'],\n",
    "        row['mean_delta_earnings_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_videos = [\n",
    "        row['mean_delta_videos_sub30'],\n",
    "        row['mean_delta_videos_add30'],\n",
    "        row['mean_delta_videos_add60'],\n",
    "        row['mean_delta_videos_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_views = [\n",
    "        row['mean_delta_views_sub30'],\n",
    "        row['mean_delta_views_add30'],\n",
    "        row['mean_delta_views_add60'],\n",
    "        row['mean_delta_views_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_subscriptions = [\n",
    "        row['mean_delta_subs_sub30'],\n",
    "        row['mean_delta_subs_add30'],\n",
    "        row['mean_delta_subs_add60'],\n",
    "        row['mean_delta_subs_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_durations = [\n",
    "        row['mean_duration_sub30'],\n",
    "        row['mean_duration_add30'],\n",
    "        row['mean_duration_add60'],\n",
    "        row['mean_duration_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_likes = [\n",
    "        row['mean_likes_sub30'],\n",
    "        row['mean_likes_add30'],\n",
    "        row['mean_likes_add60'],\n",
    "        row['mean_likes_add90']\n",
    "    ]\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    # patreons\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_patrons):\n",
    "        axs[0,2].plot(mean_date, mean_value, marker='o', color='orange', markersize=10)\n",
    "\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_earnings):\n",
    "        axs[1,2].plot(mean_date, mean_value, marker='o', color='royalblue', markersize=10)\n",
    "\n",
    "\n",
    "    # youtube\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_videos):\n",
    "        axs[2,2].plot(mean_date, mean_value, marker='o', color='r', markersize=10)\n",
    "\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_views):\n",
    "        axs[3,2].plot(mean_date, mean_value, marker='o', color='g', markersize=10)\n",
    "\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_subscriptions):\n",
    "        axs[4,2].plot(mean_date, mean_value, marker='o', color='m', markersize=10)\n",
    "        \n",
    "\n",
    "    # youtube metadata\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_durations):\n",
    "        axs[5,2].plot(mean_date, mean_value, marker='o', color='brown', markersize=10)\n",
    "        \n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_likes):\n",
    "        axs[6,2].plot(mean_date, mean_value, marker='o', color='lightblue', markersize=10)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     # plot horizontal lines for means\n",
    "#     mean_befor_list = [mean_delta_patrons_befor, mean_delta_earnings_befor, mean_delta_videos_befor, mean_delta_views_befor, mean_delta_subs_befor, mean_duration_befor, mean_likes_befor]\n",
    "#     mean_afer_list = [mean_delta_patrons_after, mean_delta_earnings_after, mean_delta_videos_after, mean_delta_views_after, mean_delta_subs_after, mean_duration_after, mean_likes_after]\n",
    "       \n",
    "#     for idx, mean in enumerate(mean_befor_list):\n",
    "#             if not math.isnan(mean):\n",
    "#                 axs[idx,2].hlines(y=mean, xmin=bkpt_date_sub30, xmax=bkpt_date      , linewidth=2, linestyle='--', color='green')\n",
    "\n",
    "#     for idx, mean in enumerate(mean_afer_list):\n",
    "#             if not math.isnan(mean):\n",
    "#                 axs[idx,2].hlines(y=mean, xmin=bkpt_date,       xmax=bkpt_date_add30, linewidth=2, linestyle='--', color='orange')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    ################################### ZOOM OUT PLOTS ###################################\n",
    "    \n",
    "    # number of patrons (delta)\n",
    "    axs[0,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,0].set(title=\"Delta patrons per week\")\n",
    "    axs[0,0].set_ylabel(\"Δ Patrons\")    \n",
    "    color_neg_pos(axs[0,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'])\n",
    "\n",
    "    # number of patrons (cumulative)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons'], alpha=0.2)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons_ma'])\n",
    "    axs[0,1].set(title=\"Number of patrons\")\n",
    "    axs[0,1].set_ylabel(\"# Patrons\")\n",
    "\n",
    "    # patreon earnings (delta)\n",
    "    axs[1,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,0].set(title=\"Patreon delta earnings per week\")\n",
    "    axs[1,0].set_ylabel(\"Δ Earnings\") \n",
    "    color_neg_pos(axs[1,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'])\n",
    "\n",
    "    # patreon earnings (cumulative)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning'], alpha=0.2)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning_ma'], color='royalblue')\n",
    "    axs[1,1].set(title=\"Patreon earnings per month\")\n",
    "    axs[1,1].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # youtube videos (delta)\n",
    "    axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,0].set(title=\"YouTube delta videos per week\")\n",
    "    axs[2,0].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,0], tmp_df_yt['datetime'], tmp_df_yt['delta_videos'])\n",
    "\n",
    "    # youtube videos (cumulative)\n",
    "    axs[2,1].plot(tmp_df_yt['datetime'], tmp_df_yt['videos'], 'r')\n",
    "    axs[2,1].set(title=\"YouTube cumulative videos\")\n",
    "    axs[2,1].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # youtube views (delta)\n",
    "    axs[3,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,0].set(title=\"YouTube delta views per week\")\n",
    "    axs[3,0].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,0], tmp_df_yt['datetime'], tmp_df_yt['delta_views'])\n",
    "\n",
    "    # youtube views (cumulative)\n",
    "    axs[3,1].plot(tmp_df_yt['datetime'], tmp_df_yt['views'], 'g')\n",
    "    axs[3,1].set(title=\"YouTube cumulative views\")\n",
    "    axs[3,1].set_ylabel(\"# Views\")\n",
    "\n",
    "    # youtube subs (delta)\n",
    "    axs[4,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,0].set(title=\"YouTube delta subscriptions per week\")\n",
    "    axs[4,0].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,0], tmp_df_yt['datetime'], tmp_df_yt['delta_subs'])\n",
    "\n",
    "    # youtube subs (cumulative)\n",
    "    axs[4,1].plot(tmp_df_yt['datetime'], tmp_df_yt['subs'], 'm')\n",
    "    axs[4,1].set(title=\"YouTube cumulative subscriptions\")\n",
    "    axs[4,1].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,0].set(title=\"YouTube videos durations\")\n",
    "    axs[5,0].set_ylabel(\"Duration\")\n",
    "    \n",
    "    \n",
    "    # youtube likes at crawl date\n",
    "    axs[6,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,0].set(title=\"YouTube likes (plotted against upload date)\")\n",
    "    axs[6,0].set_ylabel(\"Likes\")\n",
    "    \n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = bkpt_date_sub30 - (1 * MONTH_OFFSET)\n",
    "    date_max_zoom = bkpt_date_add90 + (1 * MONTH_OFFSET)\n",
    "    \n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_meta_zoomed = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min_zoom) & (tmp_df_yt_meta['upload_date'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "    \n",
    "   ################################### ZOOM IN PLOTS  ###################################\n",
    "\n",
    "    # zoomed in patron numbers (delta)\n",
    "    axs[0,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', alpha=0.3)\n",
    "    axs[0,2].set(title=\"Delta patrons per week\")\n",
    "    axs[0,2].set_ylabel(\"Δ Patrons\")\n",
    "    color_neg_pos(axs[0,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'])\n",
    "    \n",
    "    # zoomed in patron numbers (cumulative)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons'], alpha=0.2)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons_ma'])\n",
    "    axs[0,3].set(title=\"Number of patrons (zoomed in)\")\n",
    "    axs[0,3].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "    # zoomed in patron earnings (delta)\n",
    "    axs[1,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', alpha=0.3)\n",
    "    axs[1,2].set(title=\"Delta Patreon earnings per week (zoomed in)\")\n",
    "    axs[1,2].set_ylabel(\"Earnings\")  \n",
    "    color_neg_pos(axs[1,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_earning'])\n",
    "\n",
    "    # zoomed in patron earnings (cumulative)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning'], alpha=0.2)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning_ma'], color='royalblue')\n",
    "    axs[1,3].set(title=\"Patreon earnings per month (zoomed in)\")\n",
    "    axs[1,3].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # zoomed in youtube videos (delta)\n",
    "    axs[2,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', alpha=0.3)\n",
    "    axs[2,2].set(title=\"YouTube delta videos per week (zoomed in)\")\n",
    "    axs[2,2].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_videos'])\n",
    "\n",
    "    # zoomed in youtube videos (cumulative)\n",
    "    axs[2,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['videos'], 'r')\n",
    "    axs[2,3].set(title=\"YouTube cumulative videos (zoomed in)\")\n",
    "    axs[2,3].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # zoomed in youtube views (delta)\n",
    "    axs[3,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', alpha=0.3)\n",
    "    axs[3,2].set(title=\"YouTube delta views per week (zoomed in)\")\n",
    "    axs[3,2].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_views'])\n",
    "\n",
    "    # zoomed in youtube views (cumulative)\n",
    "    axs[3,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['views'], 'g')\n",
    "    axs[3,3].set(title=\"YouTube cumulative views (zoomed in)\")\n",
    "    axs[3,3].set_ylabel(\"# Views\")\n",
    "    \n",
    "    # zoomed in youtube subs (delta)\n",
    "    axs[4,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', alpha=0.3)\n",
    "    axs[4,2].set(title=\"YouTube delta subscriptions per week (zoomed in)\")\n",
    "    axs[4,2].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_subs'])\n",
    "\n",
    "    # zoomed in youtube subs (cumulative)\n",
    "    axs[4,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['subs'], 'm')\n",
    "    axs[4,3].set(title=\"YouTube cumulative subscriptions (zoomed in)\")\n",
    "    axs[4,3].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', alpha=0.3)\n",
    "    axs[5,2].set(title=\"YouTube videos durations (zoomed in)\")\n",
    "    axs[5,2].set_ylabel(\"Duration\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'])\n",
    "    \n",
    "        \n",
    "   # youtube likes per uploads\n",
    "    axs[6,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', alpha=0.3)\n",
    "    axs[6,2].set(title=\"YouTube likes (plotted against upload date) (zoomed in)\")\n",
    "    axs[6,2].set_ylabel(\"Likes\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['crawl_date'], tmp_df_yt_meta_zoomed['like_count'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################### FORMAT AXES ###################################\n",
    "\n",
    "    # format the axes\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if j < 2:\n",
    "                axs[i,j].set_xlim([date_min, date_max])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            if j >= 2:\n",
    "                axs[i,j].set_xlim([date_min_zoom, date_max_zoom])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "                # axs[i,j].xaxis.set_minor_locator(mdates.WeekdayLocator())\n",
    "            axs[i,j].xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.set_major_formatter(KM_formatter)\n",
    "            \n",
    "            \n",
    "    ################################### PLOT BREAKPOINT LINES AND POINTS ###################################\n",
    "\n",
    "    # plot vertical lines for breakpoint, breakpoint-1month, breakpoint+1month\n",
    "    print_legend = True\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if print_legend:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', label='breakpoint', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)          \n",
    "                axs[i,j].axvline(breakpoint_date + 2*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)          \n",
    "                axs[i,j].axvline(breakpoint_date + 3*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)          \n",
    "                # print_legend = False\n",
    "            else:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + 2*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + 3*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "    # axs[0,0].legend()\n",
    "    axs[0,1].legend()\n",
    "\n",
    "    # plot point for mean nb of patrons for breakpoint, breakpoint-1month, breakpoint+1month    \n",
    "    axs[0,3].plot(breakpoint_date - MONTH_OFFSET, ts_pt_df.at[(breakpoint_date - MONTH_OFFSET), 'patrons_ma'], marker='o', color='green')\n",
    "    axs[0,3].plot(breakpoint_date,               ts_pt_df.at[breakpoint_date              , 'patrons_ma'], marker='o', color='red')    \n",
    "    axs[0,3].plot(breakpoint_date + MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "    axs[0,3].plot(breakpoint_date + 2*MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + 2*MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "    axs[0,3].plot(breakpoint_date + 3*MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + 3*MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')   \n",
    "\n",
    "    fig.tight_layout(w_pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
