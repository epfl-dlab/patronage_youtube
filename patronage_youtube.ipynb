{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Characterizing Patronage on YouTube\n",
    "\n",
    "## 0. Files and brief explanation of those\n",
    "\n",
    "All data is located in `/dlabdata1/youtube_large/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/dlabdata1/youtube_large/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YouNiverse dataset:**\n",
    "\n",
    "- `df_channels_en.tsv.gz`: channel metadata.\n",
    "- `df_timeseries_en.tsv.gz`: channel-level time-series.\n",
    "- `yt_metadata_en.jsonl.gz`: raw video metadata.\n",
    "- `youtube_comments.tsv.gz`: user-comment matrices.\n",
    "- `youtube_comments.ndjson.zst`: raw comments — this is a HUGE file.\n",
    "\n",
    "**Graphteon dataset:**\n",
    "- `creators.csv` list with all creator names.\n",
    "- `final_processed_file.jsonl.gz` all graphteon time-series.\n",
    "- `pages.zip` raw html of the pages in graphteon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libaries imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import zstandard\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in current directory\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list all files in DATA_FOLDER\n",
    "!ls -lh /dlabdata1/youtube_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. YouNiverse dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Channel metadata\n",
    "Metadata associated with the 136,470 channels: **channel ID**, **join date**, **country**, **number of subscribers**, **most frequent category**, and the **channel’s position** in socialblade.com’s subscriber ranking. \\\n",
    "The number of subscribers is provided both as obtained from channelcrawler.com (between 2019-09-12 and 2019-09-17) and as crawled from socialblade.com (2019-09-27). Additionally, we also provide a set of **weights** (derived from socialblade.com’s subscriber rankings) that can be used to partially correct sample biases in our dataset.\n",
    "\n",
    "- `category_cc`: category of the channel (majority based)\n",
    "- `join_date`: join date of the channel\n",
    "- `channel`: channel id\n",
    "- `name_cc`: name of the channel.\n",
    "- `subscribers_cc`: number of subscribers\n",
    "- `videos_cc`: number of videos\n",
    "- `subscriber_rank_sb`: rank in terms of number of subscribers (channel’s position in socialblade.com’s subscriber ranking)\n",
    "- `weights`: weights cal (Set of weights derived from socialblade.com’s subscriber rankings. Can be used to partially correct sample biases in our dataset. -> correction for representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/df_channels_en.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel metadata\n",
    "df_yt_channels = pd.read_csv(DATA_FOLDER+'df_channels_en.tsv.gz', sep=\"\\t\", compression='gzip')\n",
    "df_yt_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facts about this data (taken from [YouNiverse github page](https://github.com/epfl-dlab/YouNiverse)) \n",
    "\n",
    "- This dataframe has 136,470 rows, where each one corresponds to a different channel.\n",
    "- We obtained all channels with >10k subscribers and >10 videos from channelcrawler.com in the 27 October 2019.\n",
    "- Additionally we filtered all channels that were not in english given their video metadata (see `Raw Channels')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of unique categories:         {:,}'.format(df_yt_channels['category_cc'].nunique()))\n",
    "print('Number of unique channels:      {:,}'.format(df_yt_channels['channel'].nunique()))\n",
    "print('Number of unique channel names: {:,}'.format(df_yt_channels['name_cc'].nunique()))\n",
    "\n",
    "print('\\nNote: there are more unique channels than unique names, so some channels might have the same name!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of videos and subscribers per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['videos_cc', 'subscribers_cc']\n",
    "\n",
    "# plot with linear scale for x axis and log scale for y axis\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.histplot(data=df_yt_channels[col], ax=ax, bins=50, kde=False, color=f'C{i}')\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(\"Count - number of channels (log scale)\")\n",
    "    ax.set(yscale=\"log\")\n",
    "    # ax.set(xscale=\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot with log scale for x axis \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "xlabels = [r'$\\log_{10}(videos)$', r'$\\log_{10}(subscribers)$']\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.histplot(data=np.log10(df_yt_channels[col]), ax=ax, bins=50, kde=False, cumulative=False, color=f'C{i}')\n",
    "\n",
    "    ax.set(title=f'Distribution of {col} (log-log scale)')\n",
    "    ax.set_xlabel(xlabels[i])\n",
    "    ax.set_ylabel(\"Count - number of channels\")\n",
    "\n",
    "    # ax.set(yscale=\"log\")\n",
    "    # ax.set(xscale=\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot with linear scale for both axes \n",
    "# fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "# for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "#     sns.histplot(data=df_yt_channels[col], ax=ax, bins=50, kde=False, color=f'C{i}')\n",
    "#     ax.set(title=f'Distribution of {col}')\n",
    "#     ax.set_ylabel(\"Count - number of channels\")\n",
    "#     # ax.set(yscale=\"log\")\n",
    "#     ax.set(xscale=\"log\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # plot with log scale for x axis (distplot)\n",
    "# fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "# for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "#     sns.distplot(np.log10(df_yt_channels[col]), hist_kws=kwargs, kde=False, kde_kws=kwargs, ax=ax, norm_hist=True)\n",
    "\n",
    "#     ax.set(title=f'Distribution of {col} (log-log scale)')\n",
    "#     ax.set_ylabel(\"Count - number of channels\")\n",
    "#     # ax.set(yscale=\"log\")\n",
    "#     # ax.set(xscale=\"log\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# descriptive statistics table\n",
    "df_yt_channels[selected_cols].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "From the above graphs and table, we can see that _videos_ and _subscribers_ distributions among YouTube channels follow a **power law**, meaning that most channels have a only a few videos and a few subscribers, but a few of them have a lot of videos and a lot of subscribers.\n",
    "\n",
    "More specifically:\n",
    "- 50% of the YouTube channels have less than 175 videos\n",
    "- 50% of the YouTube channels have less than 42,400 subscribers\n",
    "\n",
    "_Note: only channels with at least 10 videos and 10,000 subscribers were considered for this study._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_cat_chan = df_yt_channels.groupby(['category_cc', 'channel'])[['videos_cc', 'subscribers_cc']].agg(['max'])\n",
    "\n",
    "# set the columns to the top level of the multi-index\n",
    "data_per_cat_chan.columns = data_per_cat_chan.columns.get_level_values(0)\n",
    "data_per_cat_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_cat_chan.reset_index(inplace=True)\n",
    "data_per_cat_chan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of channels per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_per_cat = data_per_cat_chan.groupby('category_cc')[['channel']].count().sort_values('channel', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_per_cat.plot(kind='bar')\n",
    "plt.title(\"Number of channels per category\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Number of channels\")\n",
    "plt.show()\n",
    "chan_per_cat['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_per_cat = data_per_cat_chan.groupby('category')['videos_cc','subscribers_cc'].agg(['min', 'max', 'count', 'sum'])\n",
    "data_per_cat = data_per_cat_chan.groupby('category_cc')[['videos_cc','subscribers_cc']].agg(['sum'])\n",
    "data_per_cat.columns = data_per_cat.columns.get_level_values(0)\n",
    "data_per_cat = data_per_cat.add_suffix('_sum')\n",
    "data_per_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of videos per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_cat['videos_cc_sum'].sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title(\"Number of videos per category\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Number of videos\")\n",
    "plt.show()\n",
    "\n",
    "data_per_cat['videos_cc_sum'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of subscribers per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_cat['subscribers_cc_sum'].sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title(\"Number of subscribers per category\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Number of videos\")\n",
    "plt.show()\n",
    "\n",
    "data_per_cat['subscribers_cc_sum'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1.2 Channel time-series data\n",
    "Weekly number of viewers and subscribers. We have a data point for each channel and each week.\n",
    "\n",
    "Time series of channel activity at **weekly granularity**. The span of time series varies by channel depending on when socialblade.com started tracking the channel. On average, it contains **2.8 years of data per channel** for **133k channels** (notice that this means there are roughly 4k channels for which there is no time-series data). \\\n",
    "Each data point includes the **number of views** (`views`) and **subscribers** (`subs`) obtained in the given week, as well as the **number of videos** (`videos`) posted by the **channel** (`channel`). The number of videos is calculated using the video upload dates in our video metadata, such that videos that were unavailable at crawl time are not accounted for. \n",
    "\n",
    "---\n",
    "\n",
    "Time series related to each channel.\\\n",
    "These come from a mix of YouTube data and time series crawled from [socialblade.com](https://socialblade.com/):\n",
    "- From the former (YouTube data): derived weekly time series indicating **how many videos each channel had posted per week**. \n",
    "- From the latter (socialblade.com): crawled weekly statistics on the **number of viewers** `views` and **subscribers** `subs` per channel `channel`. This data was available for around 153k channels.\n",
    "\n",
    "    - `channel`: unique channel ID, which is the numbers and letters at the end of the URL.\n",
    "    - `category`: category of the channel as assigned by [socialblade.com](https://socialblade.com/) according to the last 10 videos at time of crawl (categories organize channels and videos on YouTube and help creators, advertisers, and channel managers identify with content and audiences they wish to associate with).\n",
    "    - `datetime`: First day of the week related to the data point\n",
    "    - `views`: Total number of views the channel had this week.\n",
    "    - `delta_views`: Delta views obtained this week (difference of nb of views between current and former week). (Interpolation)\n",
    "    - `subs`: Total number of subscribers the channel had this week.\n",
    "    - `delta_subs`: Delta subscribers obtained this week (difference of nb of subscribers between current and former week)\n",
    "    - `videos`: Number of videos posted by the channel up to date\n",
    "    - `delta_videos`:  Delta videos obtained this week (difference of number of videos posted by the channel between current and former week).\n",
    "    - `activity`: Number of videos published in the last 15 days.\n",
    "    \n",
    "    \n",
    "Note: Can view the channel by appending the channel id to the url, e.g.  https://www.youtube.com/channel/UCBJuEqXfXTdcPSbGO9qqn1g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dlabdata1/youtube_large/df_timeseries_en.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel-level time-series.\n",
    "df_yt_timeseries = pd.read_csv(DATA_FOLDER+'df_timeseries_en.tsv.gz', sep=\"\\t\", compression='gzip', parse_dates=['datetime'])\n",
    "df_yt_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_yt_timeseries.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Timeseries data was gathered between {} and {}'.format(df_yt_timeseries['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                         df_yt_timeseries['datetime'].max().strftime('%B %d, %Y')))\n",
    "print('Total number of datapoints accross all channels: {:>12,}'.format(len(df_yt_timeseries)))\n",
    "data_points_dist = df_yt_timeseries['channel'].value_counts()\n",
    "print('Average number of datapoints per channel:       {:>12.0f} weeks (≈{:,.1f} years)'.format(data_points_dist.mean(), data_points_dist.mean()/52))\n",
    "print('Number of unique categories:                     {:>12,}'.format(df_yt_timeseries['category'].nunique()))\n",
    "print('Number of unique channels:                       {:>12,}'.format(df_yt_timeseries['channel'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datetime points per channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all channels timeseries start and end at the same time, therefore we have a different amount of datapoints for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_data = df_yt_timeseries.groupby('channel')['datetime'].agg(['min', 'max'])\n",
    "datetime_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datetime points per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_ts_year_cnt = df_yt_timeseries.groupby(df_yt_timeseries.datetime.dt.year).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Timeseries data was gathered between {} and {}'.format(df_yt_timeseries['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                         df_yt_timeseries['datetime'].max().strftime('%B %d, %Y')))\n",
    "yt_ts_year_cnt.plot(kind='bar')\n",
    "plt.title(\"Nb of datapoints per year accross all channels\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count (datapoints)\")\n",
    "plt.show()\n",
    "\n",
    "yt_ts_year_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datetime points per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_ts_month_cnt = df_yt_timeseries.groupby([df_yt_timeseries.datetime.dt.year, df_yt_timeseries.datetime.dt.month]).size()\n",
    "yt_ts_month_cnt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas.Grouper\n",
    "yt_ts_month_cnt_grouper = df_yt_timeseries.groupby(pd.Grouper(key='datetime', freq='M')).count().channel\n",
    "yt_ts_month_cnt_grouper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of datapoints per month\n",
    "plt.figure(figsize=(15,2))\n",
    "yt_ts_month_cnt.plot(kind='bar')\n",
    "plt.title(\"Number of datapoints per month accross all channels (using regular group by method)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count (datapoints)\")\n",
    "plt.show()\n",
    "\n",
    "# plot number of datapoints per month using grouper\n",
    "plt.figure(figsize=(15,2))\n",
    "yt_ts_month_cnt_grouper.plot(kind='bar')\n",
    "plt.title(\"Number of datapoints per month accross all channels (using grouper)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count (datapoints)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider unique values per channel\n",
    "yt_ts_month_unique_cnt = df_yt_timeseries.groupby(pd.Grouper(key='datetime', freq='M')).agg({\"channel\": pd.Series.nunique})\n",
    "yt_ts_month_unique_cnt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_yt_timeseries.groupby(['datetime', 'channel']).count() > 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels with timeseries (only consider unique values per channel) --> see https://stackoverflow.com/questions/38309729/count-unique-values-per-groups-with-pandas\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(7,3), sharey=True, sharex=True,\n",
    "                       gridspec_kw={\"wspace\": 0.05})\n",
    "\n",
    "ax.plot(yt_ts_month_unique_cnt)\n",
    "\n",
    "ax.set(title='Number of channels with timeseries')\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"# channels\")\n",
    "ax.xaxis.set_major_locator(years)\n",
    "ax.xaxis.set_minor_locator(months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datetime points accross channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of datapoints accross channels\n",
    "\n",
    "print('Total number of datapoints accross all channels: {:>12,}'.format(len(df_yt_timeseries)))\n",
    "data_points_dist = df_yt_timeseries['channel'].value_counts()\n",
    "print('Average number of datapoints per channel:        {:>12,.0f} weeks (≈{:,.1f} years)'.format(data_points_dist.mean(), data_points_dist.mean()/52))\n",
    "\n",
    "ax = sns.histplot(data=data_points_dist, bins=50, kde=False, color=f'C{1}')\n",
    "\n",
    "ax.set(title=f'Distribution of datapoints (weeks) accross channels')\n",
    "ax.set_xlabel('number of data points (weeks)')\n",
    "ax.set_ylabel('number of channels')\n",
    "\n",
    "# ax.set(yscale=\"log\")\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregates per channel\n",
    "sel_cols = ['datetime', 'views', 'delta_views', 'subs', 'delta_subs', 'videos', 'delta_videos', 'activity']\n",
    "data_per_channel = df_yt_timeseries.groupby('channel')[sel_cols].agg(['min', 'max', 'count', 'mean'])\n",
    "data_per_channel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Views per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_channel['views'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of total views per channel\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "\n",
    "sns.histplot(data=data_per_channel['views']['max'], ax=axs[0], bins=20, kde=False, color=f'C{1}')\n",
    "axs[0].set(title=f'Distribution of total views per channel')\n",
    "axs[0].set_xlabel('number of views (in billions)')\n",
    "axs[0].set_ylabel('number of channels')\n",
    "axs[0].set(yscale=\"log\")\n",
    "xlabels1 = ['{:,.0f}'.format(x) + 'bn' for x in axs[0].get_xticks()/1_000_000_000]\n",
    "axs[0].set_xticklabels(xlabels1)\n",
    "\n",
    "# Distribution of total views per channel (log scale)\n",
    "sns.histplot(data=data_per_channel['views']['max'], ax=axs[1], bins=1000, kde=False, color=f'C{1}')\n",
    "axs[1].set(title=f'Distribution of total views per channel (log-log scale)')\n",
    "axs[1].set_xlabel('number of views (in millions)')\n",
    "axs[1].set_ylabel('number of channels')\n",
    "axs[1].set(yscale=\"log\")\n",
    "axs[1].set(xscale=\"log\")\n",
    "xlabels2 = ['{:,.0f}'.format(x) + 'M' for x in axs[1].get_xticks()/1_000_000]\n",
    "axs[1].set_xticklabels(xlabels2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "data_per_channel['views'][['max']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 channels with the most total views (in billions):\")\n",
    "\n",
    "for index, value in data_per_channel['views']['max'].sort_values(ascending=False)[:10].items():\n",
    "    print('https://www.youtube.com/channel/{} : {:,.1f} bn views'.format(index, value/1_000_000_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Videos per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_channel['videos'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of total videos per channel\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "sns.histplot(data=data_per_channel['videos']['max'], ax=axs[0], bins=20, kde=False, color=f'C{1}')\n",
    "\n",
    "axs[0].set(title=f'Distribution of total videos per channel')\n",
    "axs[0].set_xlabel('number of videos')\n",
    "axs[0].set_ylabel('number of channels')\n",
    "axs[0].set(yscale=\"log\")\n",
    "\n",
    "# # Distribution of total views per channel (log scale)\n",
    "sns.histplot(data=data_per_channel['videos']['max'], ax=axs[1], bins=100, kde=False, color=f'C{1}')\n",
    "\n",
    "axs[1].set(title=f'Distribution of total videos per channel (log-log scale)')\n",
    "axs[1].set_xlabel('number of videos')\n",
    "axs[1].set_ylabel('number of channels')\n",
    "axs[1].set(yscale=\"log\")\n",
    "axs[1].set(xscale=\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "data_per_channel['videos'][['max']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 channels with the most total videos:\")\n",
    "\n",
    "for index, value in data_per_channel['videos']['max'].sort_values(ascending=False)[:10].items():\n",
    "    print('https://www.youtube.com/channel/{} : {:,.0f} videos'.format(index, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscribers per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_channel['subs'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of total subscribers per channel\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "sns.histplot(data=data_per_channel['subs']['max'], ax=axs[0], bins=20, kde=False, color=f'C{1}')\n",
    "\n",
    "axs[0].set(title=f'Distribution of total subscribers per channel')\n",
    "axs[0].set_xlabel('number of subscribers (in millions)')\n",
    "axs[0].set_ylabel('number of channels')\n",
    "axs[0].set(yscale=\"log\")\n",
    "xlabels0 = ['{:,.0f}'.format(x) + 'M' for x in axs[0].get_xticks()/1_000_000]\n",
    "axs[0].set_xticklabels(xlabels0)\n",
    "\n",
    "# # Distribution of total views per channel (log scale)\n",
    "sns.histplot(data=data_per_channel['subs']['max'], ax=axs[1], bins=500, kde=False, color=f'C{1}')\n",
    "\n",
    "axs[1].set(title=f'Distribution of total subscribers per channel (log-log scale)')\n",
    "axs[1].set_xlabel('number of subscribers')\n",
    "axs[1].set_ylabel('number of channels')\n",
    "axs[1].set(yscale=\"log\")\n",
    "axs[1].set(xscale=\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "data_per_channel['videos'][['max']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_per_channel['subs']['max'].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 channels with the most total subscribers:\")\n",
    "\n",
    "for index, value in data_per_channel['subs']['max'].sort_values(ascending=False)[:10].items():\n",
    "    print('https://www.youtube.com/channel/{} : {:,.1f}M subscribers'.format(index, value/1_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the columns to the top level of the multi-index\n",
    "# data_per_channel.columns = data_per_channel.columns.get_level_values(0)\n",
    "# data_per_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Raw video metadata\n",
    "The file `df_videos_raw.jsonl.gz` contains metadata data related to ~73M videos from ~137k channels. Below we show the data recorded for each of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dlabdata1/youtube_large/yt_metadata_en.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! zcat /dlabdata1/youtube_large/yt_metadata_en.jsonl.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata = pd.read_json(DATA_FOLDER+'yt_metadata_en.jsonl.gz', compression='gzip', lines=True, nrows=2000)\n",
    "df_yt_metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 user-comment matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/youtube_comments.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-comment matrices\n",
    "df_yt_comments = pd.read_csv(DATA_FOLDER+'youtube_comments.tsv.gz', sep=\"\\t\", compression='gzip', nrows=100)\n",
    "df_yt_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5 raw comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/youtube_comments.ndjson.zst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_jsonify(line): \n",
    "    \"\"\"\n",
    "\n",
    "    :param line: string to parse and jsonify\n",
    "    :return: \n",
    "    \"\"\"    \n",
    "    \n",
    "    # add square brackets around line\n",
    "    line = \"[\" + line + \"]\"\n",
    "\n",
    "    # remove quotes before and after square brackets   \n",
    "    line = line.replace(\"\\\"[{\", \"[{\")\n",
    "    line = line.replace(\"}]\\\"\", \"}]\")    \n",
    "    \n",
    "    # replace double double-quotes with single double-quotes\n",
    "    line = line.replace(\"{\\\"\\\"\", \"{\\\"\")\n",
    "    line = line.replace(\"\\\"\\\"}\", \"\\\"}\")\n",
    "    line = line.replace(\"\\\"\\\":\\\"\\\"\", \"\\\":\\\"\")\n",
    "    line = line.replace(\":\\\"\\\"\", \":\\\"\")\n",
    "    line = line.replace(\"\\\"\\\":\", \"\\\":\")\n",
    "    \n",
    "    # line = line.replace(\"\\\"\\\":\", \"\\\":\")\n",
    "    line = line.replace(\"\\\"\\\",\\\"\\\"\", \"\\\",\\\"\")\n",
    "    line = line.replace(\"\\\"\\\",\\\"\\\"\", \"\\\",\\\"\")\n",
    "    line = line.replace(\"\\\\\\\"\\\"\", \"\\\\\\\"\")\n",
    "    line = line.replace(\"\\\\\\\",[\", \"\\\\\\\\ \\\",[\")\n",
    "    \n",
    "    line = re.sub(r',\\\"\\\"(?!\\,)', ',\\\"', line)\n",
    "\n",
    "    line = line.replace(\"true,\\\"\\\"\", \"true,\\\"\")\n",
    "    line = line.replace(\"false,\\\"\\\"\", \"false,\\\"\")\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        '''Init method'''\n",
    "        import codecs\n",
    "        self.fh = open(file,'rb')\n",
    "        print(f\"reading {file} in chunks ...\")\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstandard.ZstdDecompressor(max_window_size=2147483648)\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def readlines(self):\n",
    "        '''Generator method that creates an iterator for each line of JSON'''\n",
    "        nb_chunk = 0\n",
    "        while True:\n",
    "            nb_chunk = nb_chunk + 1\n",
    "            if nb_chunk % 5000 == 0:\n",
    "                print(\"number of chunks read: \", nb_chunk)\n",
    "                \n",
    "            chunk = self.reader.read(self.chunk_size).decode(\"utf-8\", \"replace\")\n",
    "\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            # print(\"lines per chunk: \", len(lines))\n",
    "            # print(lines)\n",
    "            \n",
    "            for line in lines[:-1]:\n",
    "                # print(line)\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_OF_LINES = 350000\n",
    "lines_json = []\n",
    "inp_file = DATA_FOLDER+\"youtube_comments.ndjson.zst\"\n",
    "reader = Zreader(inp_file, chunk_size=4092)\n",
    "\n",
    "for i, line in enumerate(reader.readlines()):\n",
    "    if i > NB_OF_LINES:\n",
    "        # print(line)\n",
    "        break\n",
    "    line_json = json.loads(line_jsonify(line))\n",
    "    lines_json.append(line_json)\n",
    "\n",
    "print(\"==> number of lines read:\", len(lines_json))\n",
    "\n",
    "df_yt_comments_raw = pd.DataFrame(data=lines_json[1:], columns=lines_json[0])\n",
    "df_yt_comments_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Graphtreon dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 List with all creator names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /dlabdata1/youtube_large/creators.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with all creator names.\n",
    "df_gt_creators = pd.read_csv(DATA_FOLDER+'creators.csv')\n",
    "df_gt_creators.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 All graphtreon time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dlabdata1/youtube_large/final_processed_file.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_processed_file.jsonl.gz all graphteon time-series.\n",
    "df_gt_timeseries = pd.read_json(DATA_FOLDER+'final_processed_file.jsonl.gz', compression='gzip', lines=True, nrows=500)\n",
    "# df_gt_timeseries = pd.read_json(DATA_FOLDER+'final_processed_file.jsonl.gz', compression='gzip', lines=True)\n",
    "df_gt_timeseries.head()\n",
    "\n",
    "# get only id and match them first\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries['startDate'] = pd.to_datetime(df_gt_timeseries['startDate'])\n",
    "df_gt_timeseries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of unique creators:         {:,}'.format(df_gt_timeseries['creatorName'].nunique()))\n",
    "print('Number of unique patreon ids:         {:,}'.format(df_gt_timeseries['patreon'].nunique()))\n",
    "\n",
    "print('Timeseries data was gathered between {} and {}'.format(df_gt_timeseries['startDate'].min().strftime('%B %d, %Y'),\n",
    "                                                         df_gt_timeseries['startDate'].max().strftime('%B %d, %Y')))\n",
    "print('Total number of datapoints accross all channels: {:>12,}'.format(len(df_gt_timeseries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Raw html of the pages in graphteon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dlabdata1/youtube_large/pages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages.zip raw html of the pages in graphteon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Match data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Merge channels data with YouTube timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_yt_timeseries.head(1))\n",
    "display(df_yt_channels.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries_merged = df_yt_timeseries.merge(df_yt_channels, how='inner', on='channel')\n",
    "\n",
    "# remove duplicate columns\n",
    "# df_yt_timeseries_merged.drop('category_cc', axis='columns', inplace=True)\n",
    "\n",
    "df_yt_timeseries_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Match YouTube timeseries and Graphtreon timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Filter YouTube metadata containing patreon id\n",
    "(externalized filter function to scripts.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh \"yt_metadata_en_pt.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filtered youtube metadata file (takes about 2 mins)\n",
    "df_yt_metadata_pt = pd.read_csv(\"yt_metadata_en_pt.tsv.gz\", sep=\"\\t\", lineterminator='\\n', compression='gzip') \n",
    "df_yt_metadata_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_YT_METADATA_ROWS = 72_924_794\n",
    "print(\"[YouTube metadata] Total number of videos:                                           {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "print(\"[YouTube metadata] number of videos that contain a patreon link in description:      {:>10,} ({:.1%} of total dataset)\".format(len(df_yt_metadata_pt), len(df_yt_metadata_pt)/DF_YT_METADATA_ROWS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique patreon ids in df_yt_metadata_pt\n",
    "yt_patreon_list = df_yt_metadata_pt.patreon_id.unique()\n",
    "print(\"[Filtered YouTube metadata] total number of unique patreon ids:                       {:>9,}\".format(df_yt_metadata_pt.patreon_id.nunique()))\n",
    "print(\"[Filtered YouTube metadata] number of unique channels that contain a patreon account: {:>9,}\".format(df_yt_metadata_pt.channel_id.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of patreon ids per youtube channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_id_cnt_pr_chan = df_yt_metadata_pt_grp_chan.groupby('channel_id').count()['patreon_id'].sort_values(ascending=False)\n",
    "pt_id_cnt_pr_chan = pt_id_cnt_pr_chan.to_frame(name='patreon_id_count')\n",
    "pt_id_cnt_pr_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with linear scale for both axes\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "\n",
    "sns.histplot(data=pt_id_cnt_pr_chan, ax=axs[0], bins=50, kde=False, legend=False, color=f'C{0}')\n",
    "axs[0].set(title=f'Distribution of patreon ids per channel')\n",
    "axs[0].set_xlabel(\"Number of patreon ids\")\n",
    "axs[0].set_ylabel(\"Count of channels\")\n",
    "\n",
    "# plot with linear scale for x axis and log scale for y axis\n",
    "sns.histplot(data=pt_id_cnt_pr_chan, ax=axs[1], bins=50, kde=False, legend=False, color=f'C{0}')\n",
    "axs[1].set(title=f'Distribution of patreon ids per channel (log scale)')\n",
    "axs[1].set_xlabel(\"Number of patreon ids\")\n",
    "axs[1].set_ylabel(\"Count of channels (log scale)\")\n",
    "axs[1].set(yscale=\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# descriptive statistics table\n",
    "pt_id_cnt_pr_chan.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "We can see that we have almost twice the amount of patreon ids than the amount of channels. This is because some channels use different patreon ids (typically of the form `patreon.com/posts/postid`)for different videos. For example: \n",
    "- the channel [JuliDG](https://www.youtube.com/channel/UCqQbeJvP8rsrJYF83PDPkWQ) (`channel_id` = UCqQbeJvP8rsrJYF83PDPkWQ) uses 689 different patreon ids.\n",
    "- the channel [Marco Cirillo](https://www.youtube.com/channel/UC4T9oHvffAyPsBUDmqE3HYA) (`channel_id` = UC4T9oHvffAyPsBUDmqE3HYA) uses 299 different patreon ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan['channel_id'] == 'UCqQbeJvP8rsrJYF83PDPkWQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan['channel_id'] == 'UC4T9oHvffAyPsBUDmqE3HYA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of videos per patreon id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Filtered YouTube metadata] number of videos per patreon id:\")\n",
    "vids_cnt_per_patreon_id = df_yt_metadata_pt.groupby('patreon_id').count().title.sort_values(ascending=False)\n",
    "vids_cnt_per_patreon_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with linear scale for both axes\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "\n",
    "sns.histplot(data=vids_cnt_per_patreon_id, ax=axs[0], bins=50, kde=False, color=f'C{0}')\n",
    "axs[0].set(title=f'Distribution of videos per patreon id')\n",
    "axs[0].set_xlabel(\"Number of videos\")\n",
    "axs[0].set_ylabel(\"Count of patreon ids\")\n",
    "\n",
    "# plot with linear scale for x axis and log scale for y axis\n",
    "sns.histplot(data=vids_cnt_per_patreon_id, ax=axs[1], bins=50, kde=False, color=f'C{0}')\n",
    "axs[1].set(title=f'Distribution of videos per patreon id (log scale)')\n",
    "axs[1].set_xlabel(\"Number of videos\")\n",
    "axs[1].set_ylabel(\"Count of patreon ids (log scale)\")\n",
    "axs[1].set(yscale=\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# descriptive statistics table\n",
    "vids_cnt_per_patreon_id.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "From the above graphs and table, we can see that the _videos_ distributions among patreon ids follows a **power law**, meaning that most patreon accounts have a only a few videos, but a few of them have a lot of videos.\n",
    "\n",
    "More specifically:\n",
    "- 25% of the Patreon accounts have less than 3 videos\n",
    "- 50% of the Patreon accounts have less than 27 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Filtered YouTube metadata] number of channels per patreon id:\")\n",
    "\n",
    "chan_cnt_per_patreon_id = df_yt_metadata_pt.groupby('patreon_id')[['channel_id']].agg(['count'])\n",
    "chan_cnt_per_patreon_id\n",
    "\n",
    "# set the columns to the top level of the multi-index\n",
    "chan_cnt_per_patreon_id.columns = chan_cnt_per_patreon_id.columns.get_level_values(0)\n",
    "chan_cnt_per_patreon_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_cnt_per_patreon_id.reset_index()\n",
    "# chan_cnt_per_patreon_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Filter Graphtreon to keep only the ones matching patreon id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dlabdata1/youtube_large/final_processed_file.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_escape(str):\n",
    "    \"\"\"\n",
    "    replace new line special character by a space\n",
    "    \"\"\"\n",
    "    return str.replace(\"\\\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # extract patreon accounts from youtube channel descriptions and\n",
    "# # # filter the metadata to retain only the rows which patreon url exists in the filtered YT metadata \n",
    "# input_file_path = DATA_FOLDER+\"/final_processed_file.jsonl.gz\"\n",
    "\n",
    "# # MAX_ITER = 1_000\n",
    "\n",
    "# nb_rows_read = 0\n",
    "# JSONDecodeErrors_cnt = 0 \n",
    "# lines_json = []    \n",
    "\n",
    "# # pattern = re.compile(r'patreon.com/\\w*')\n",
    "\n",
    "# compressed_file_size = os.stat(input_file_path).st_size\n",
    "# print(\"Compressed file size is :                 {:>3,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "# # 12.4\n",
    "# uncompressed_file_size = 13_310_000_000\n",
    "# print(\"Estimated Uncompressed file size is :     {:>3,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "# start = timeit.default_timer()\n",
    "\n",
    "# # Load tqdm with size counter instead of file counter\n",
    "# with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "#     with gzip.open(input_file_path, \"r\") as f:\n",
    "#         for i, line_byte in enumerate(f): \n",
    "\n",
    "#             read_bytes = len(line_byte)\n",
    "#             if read_bytes:\n",
    "#                 pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "#                 pbar.update(read_bytes)\n",
    "\n",
    "#             nb_rows_read += 1\n",
    "            \n",
    "#             # set a maximum iteration for tests\n",
    "#             # if nb_rows_read >= MAX_ITER:\n",
    "#             #     break\n",
    "\n",
    "#             # convert bytes into string\n",
    "#             line_str = line_byte.decode(\"utf-8\")\n",
    "\n",
    "            \n",
    "#             # convert string into json after escaping new line characters\n",
    "#             line_str_esc = json_escape(line_str)\n",
    "#             try:\n",
    "#                 line_json = json.loads(line_str_esc)\n",
    "#             except Exception as e:\n",
    "#                 JSONDecodeErrors_cnt += 1\n",
    "#                 pass\n",
    "           \n",
    "            \n",
    "#             # add line if patreon id is exists in df_yt_metadata_pt\n",
    "#             if line_json['patreon'] in yt_patreon_list:\n",
    "#                 lines_json.append(line_json)\n",
    "\n",
    "# stop = timeit.default_timer()\n",
    "# time_diff = stop - start\n",
    "\n",
    "# print(\"\\n==> total time to read and filter graphtreon time series:        {:>9.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "# print(\"==> number of rows read:                                           {:>9,}\".format(nb_rows_read))\n",
    "# print(\"==> number of patreon ids that exist in both GTts and YT metadata: {:>9,} ({:.2%})\".format(len(lines_json), len(lines_json)/nb_rows_read ))\n",
    "# print(\"==> number of skipped rows (JSONDecodeErrors):                     {:>9,}\".format(JSONDecodeErrors_cnt))\n",
    "\n",
    "# # create new dataframe with the filtered lines\n",
    "# df_gt_timeseries_filtered = pd.DataFrame(data=lines_json)\n",
    "\n",
    "# # calculate memory usage of the new dataframe\n",
    "# mem_cons = df_gt_timeseries_filtered.memory_usage(index=True).sum()\n",
    "# print(\"==> memory usage of new (filtered) dataframe:                      {:12,.2f} MB ({:,} bytes)\".format(mem_cons / 2**20, mem_cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to disk as feather - ERROR\n",
    "# output_file_path = \"df_gt_timeseries_filtered.feather\"\n",
    "# df_gt_timeseries_filtered.to_feather(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to disk as a compressed tsv\n",
    "# output_file_path = \"df_gt_timeseries_filtered.tsv.gz\"\n",
    "# df_gt_timeseries_filtered.to_csv(output_file_path, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh yt_metadata_en_pt.tsv.gz\n",
    "!ls -lh df_gt_timeseries_filtered.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_filtered = pd.read_csv(\"df_gt_timeseries_filtered.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_gt_timeseries_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_processed_file_ROWS = 232_269\n",
    "print(\"[Graphtreon Timeseries] Total number of patreon ids:                                              {:>9,}\".format(final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids that exist in both GT Timeseries and YouTube metadata:   {:>9,} ({:.1%} of Graphtreon timeseries dataset)\".format(len(df_gt_timeseries_filtered), len(df_gt_timeseries_filtered)/final_processed_file_ROWS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique patreon ids in df_gt_timeseries_filtered\n",
    "yt_gt_patreon_list = df_gt_timeseries_filtered.patreon.unique()\n",
    "print(yt_gt_patreon_list)\n",
    "print(len(yt_gt_patreon_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
