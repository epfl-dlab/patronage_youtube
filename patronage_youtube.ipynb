{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Characterizing Patronage on YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libaries imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder paths\n",
    "DATA_FOLDER = \"/dlabdata1/youtube_large/\"\n",
    "LOCAL_DATA_FOLDER = \"local_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original files used in this section. They are preprocessed in the _preprocessing_ notebook.\n",
    "\n",
    "**YouNiverse dataset:**\n",
    "\n",
    "- `df_channels_en.tsv.gz`: channel metadata.\n",
    "- `df_timeseries_en.tsv.gz`: channel-level time-series.\n",
    "- `yt_metadata_en.jsonl.gz`: raw video metadata.\n",
    "\n",
    "**Graphteon dataset:**\n",
    "- `final_processed_file.jsonl.gz` all graphteon time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1. Load YouTube metadata\n",
    "_In the preprocessing phase, we extracted Patreon urls from YouTube metadata description (if they existed) and kept only those rows_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YT metadata containing patreon ids in description\n",
    "!ls -lh {LOCAL_DATA_FOLDER}yt_metadata_en_unique_pt_yt.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filtered youtube metadata file (takes about 1.5 min)\n",
    "df_yt_metadata_pt = pd.read_csv(LOCAL_DATA_FOLDER+\"yt_metadata_en_unique_pt_yt.tsv.gz\", sep=\"\\t\", lineterminator='\\n', compression='gzip') \n",
    "df_yt_metadata_pt['crawl_date'] = pd.to_datetime(df_yt_metadata_pt['crawl_date'])\n",
    "df_yt_metadata_pt['upload_date'] = pd.to_datetime(df_yt_metadata_pt['upload_date'])\n",
    "df_yt_metadata_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTube Metadata statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original YT dataset\n",
    "DF_YT_METADATA_ROWS = 72_924_794\n",
    "\n",
    "# stats \n",
    "print(\"Videos:\")\n",
    "print(\"[Original YouTube metadata] Total number of videos:                                             {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "print(\"[Filtered YouTube metadata] Number of videos that contain a valid patreon link in description:  {:>10,} ({:.1%} of total dataset)\".format(len(df_yt_metadata_pt), len(df_yt_metadata_pt)/DF_YT_METADATA_ROWS))\n",
    "\n",
    "\n",
    "# get list of all unique patreon ids in df_yt_metadata_pt\n",
    "yt_patreon_list = df_yt_metadata_pt['patreon_id'].unique()\n",
    "yt_pt_channel_list = df_yt_metadata_pt['channel_id'].unique()\n",
    "print(\"\\nChannels:\")\n",
    "print(\"[Filtered YouTube metadata] Total number of unique patreon ids:                                 {:>9,}\".format(len(yt_patreon_list)))\n",
    "print(\"[Filtered YouTube metadata] Number of unique channels that contain a valid patreon account:     {:>9,}\".format(len(yt_pt_channel_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 Load \"Link\" dataframe (channel/patreon)\n",
    "This dataframe shows the correspondence between `channel_id` and `patreon_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_linked_channels_patreons.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linked_channels_patreons = pd.read_csv(LOCAL_DATA_FOLDER+\"df_linked_channels_patreons.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\n",
    "df_linked_channels_patreons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of linked patreon_ids and channel_ids: {len(df_linked_channels_patreons):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load YouTube timeseries\n",
    "_In the preprocessing phase, the YouTube channels have been restricted / filtered according to the following criteria (filters were applied sequentially):_\n",
    "- Filter 1: Keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account \n",
    "- Filter 2: At least 2 year between first and last video\n",
    "- Filter 3: At least 20 videos with patreon ids\n",
    "- Filter 4: At least 250k subscribers at data crawling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_yt_timeseries_restricted.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load restricted channel-level time-series.\n",
    "df_yt_timeseries_restricted = pd.read_csv(LOCAL_DATA_FOLDER+'df_yt_timeseries_restricted.tsv.gz', sep=\"\\t\", compression='gzip', parse_dates=['datetime'])\n",
    "\n",
    "# replace dates that were collected after 23:00 to their next day, and remove hour (+ backup original column)\n",
    "df_yt_timeseries_restricted['datetime'] = df_yt_timeseries_restricted['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "\n",
    "# remove hours and convert to datetime type\n",
    "df_yt_timeseries_restricted['datetime'] = pd.to_datetime(df_yt_timeseries_restricted['datetime'].dt.date)\n",
    "df_yt_timeseries_restricted.head()\n",
    "\n",
    "# add patreon_id column to YT timeseries\n",
    "df_yt_timeseries_restricted_merged = df_yt_timeseries_restricted.merge(df_linked_channels_patreons, left_on='channel', right_on='channel_id')\n",
    "\n",
    "df_yt_timeseries_restricted_merged = df_yt_timeseries_restricted_merged.drop(columns=['channel_id'])\n",
    "df_yt_timeseries_restricted_merged.insert(1, 'patreon_id', df_yt_timeseries_restricted_merged.pop('patreon_id'))\n",
    "df_yt_timeseries_restricted_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTube timeseries statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original YT dataset\n",
    "DF_YT_METADATA_ROWS = 18_872_499\n",
    "DF_YT_METADATA_UNIQUE_CHANNELS = 133_516\n",
    "\n",
    "chan_list_restricted = df_yt_timeseries_restricted['channel'].unique()\n",
    "chan_list_restricted_cnt = len(chan_list_restricted)\n",
    "\n",
    "# Nb of channels of original YT timeseries dataset\n",
    "print(\"[YouTube Timeseries] Number of data points in original dataset:           {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "print(\"[YouTube Timeseries] Number of data points after applying filters:        {:>10,} ({:5.1%} of original dataset)\".format(len(df_yt_timeseries_restricted), len(df_yt_timeseries_restricted)/DF_YT_METADATA_ROWS))\n",
    "print()\n",
    "print(\"[YouTube Timeseries] Number of channels of original dataset:              {:>10,}\".format(DF_YT_METADATA_UNIQUE_CHANNELS))\n",
    "print(\"[YouTube Timeseries] Number of channels after applying filters:           {:>10,} ({:5.1%} of original dataset)\".format(chan_list_restricted_cnt, chan_list_restricted_cnt/DF_YT_METADATA_UNIQUE_CHANNELS))\n",
    "print()\n",
    "print('[YouTube Timeseries] Time range after applying filters:               {} and {}'.format(df_yt_timeseries_restricted['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                              df_yt_timeseries_restricted['datetime'].max().strftime('%B %d, %Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4 Load Graphtreon dataset\n",
    "From the original dataset:\n",
    "- Find corresponding YouTube channel for each patreon (lookup in linked patreon/channel dataset)\n",
    "- If patreon_id exist in patreons corresponding to the restricted list of channels in YT time series\n",
    "    - we extract the date and earnings from “dailyGraph_earningsSeriesData” (see preprocessing notebook\n",
    "    - we extract the date and patrons from “dailyGraph_patronSeriesData” (see preprocessing notebook)\n",
    "    - we merge those two time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dailyGraph_patrons_and_earnings_Series.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read merged dailyGraph_patrons_and_earnings_Series from disk\n",
    "df_dailyGraph_patrons_and_earnings_Series = pd.read_csv(LOCAL_DATA_FOLDER+\"dailyGraph_patrons_and_earnings_Series.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_dailyGraph_patrons_and_earnings_Series['date'] = pd.to_datetime(df_dailyGraph_patrons_and_earnings_Series['date'], unit='ms')\n",
    "df_dailyGraph_patrons_and_earnings_Series['patrons'] = df_dailyGraph_patrons_and_earnings_Series['patrons'].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove PT accounts that aren't in linked df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dailyGraph_patrons_and_earnings_Series = df_dailyGraph_patrons_and_earnings_Series[df_dailyGraph_patrons_and_earnings_Series['patreon'].isin(df_linked_channels_patreons['patreon_id'])]\n",
    "df_dailyGraph_patrons_and_earnings_Series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphtreon timeseries statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-declare global variable for size of original GT dataset\n",
    "GT_final_processed_file_ROWS = 232_269\n",
    "\n",
    "# group by patreon account, sort by max number of patrons\n",
    "dailyGraph_grp_patreon = df_dailyGraph_patrons_and_earnings_Series.groupby('patreon').agg(date_cnt=('date', 'count'), earliest_date=('date', 'min'), lastest_date=('date', 'max'),daily_earning_min=('earning', 'min'),daily_earning_max=('earning', 'max'), daily_earning_mean=('earning', 'mean'), daily_patrons_min=('patrons', 'min'), daily_patrons_max=('patrons', 'max'), daily_patrons_mean=('patrons', 'mean')).sort_values(by=['daily_patrons_max'], ascending=False).round(2).reset_index()\n",
    "\n",
    "print(\"[Graphtreon Timeseries] Number of patreon ids in original dataset:                                            {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Number of patreon ids in dailyGraph patreon + earnings time series + linked df:       {:>9,} ({:.1%} of original dataset)\".format(df_dailyGraph_patrons_and_earnings_Series.patreon.nunique(), df_dailyGraph_patrons_and_earnings_Series.patreon.nunique()/GT_final_processed_file_ROWS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['daily_patrons_mean', 'daily_earning_mean']\n",
    "\n",
    "# plot with log scale for x axis \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.histplot(data=dailyGraph_grp_patreon[col], stat='count', ax=ax, bins=50, kde=False, cumulative=False, color=f'C{i}')\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(\"Number of patron accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot with log scale for x axis \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n",
    "xlabels = [r'$\\log_{10}($daily_patrons_mean$)$', r'$\\log_{10}($daily_earning_mean$)$']\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.histplot(data=np.log10(dailyGraph_grp_patreon[col]), stat='count', ax=ax, bins=50, kde=False, cumulative=False, color=f'C{i}')\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_xlabel(xlabels[i])\n",
    "    ax.set_ylabel(\"Number of patron accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "dailyGraph_grp_patreon.describe()[['daily_patrons_mean', 'daily_earning_mean']].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Select top patreon accounts\n",
    "Restrict patreons accounts with mean number of patrons > 200 patrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyGraph_grp_patreon[dailyGraph_grp_patreon['patreon'] == 'patreon.com/adamthewoo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_MAX_PATRONS = 200\n",
    "\n",
    "# remove patrons accounts that have less than MIN_MAX_PATRONS patrons average\n",
    "top_patreons_df = dailyGraph_grp_patreon[dailyGraph_grp_patreon['daily_patrons_max'] > MIN_MAX_PATRONS]\n",
    "top_patreons = top_patreons_df['patreon']\n",
    "df_top_pt_accts = df_dailyGraph_patrons_and_earnings_Series[df_dailyGraph_patrons_and_earnings_Series['patreon'].isin(top_patreons)]\n",
    "\n",
    "print(\"[Graphtreon Timeseries] Filtered \\\"top\\\" patrons (> {} patrons):  {:>9,} ({:.1%} of original dataset)\".format(MIN_MAX_PATRONS, len(top_patreons), len(top_patreons)/GT_final_processed_file_ROWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter YT timeseries channels matching top patreon accounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter YT channels matching top patreon accounts\n",
    "df_yt_timeseries_top_pt = df_yt_timeseries_restricted_merged[df_yt_timeseries_restricted_merged['patreon_id'].isin(top_patreons)].copy()\n",
    "\n",
    "print(f\"Number of YouTube channels before filtering by top patreon accounts: {len(df_yt_timeseries_restricted_merged.patreon_id.unique()):>5}\" )\n",
    "print(f\"Number of YouTube channels after  filtering by top patreon accounts: {df_yt_timeseries_top_pt.patreon_id.nunique():>5}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter YT metadata channels matching top Patreon accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter accounts that match selected Patreon ids\n",
    "df_yt_metadata_pt_filtered = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(top_patreons)].copy()\n",
    "\n",
    "print(f'Filter accounts that match top Patreon ids: {len(df_yt_metadata_pt_filtered):,} ({len(df_yt_metadata_pt_filtered)/len(df_yt_metadata_pt):.1%} of videos containing a PT accounts) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Propensity score matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = mdates.YearLocator()\n",
    "months = mdates.MonthLocator()\n",
    "# years_fmt = mdates.DateFormatter('%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Select \"treated\" accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change point detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"img/increase_decrease_options_051322.jpg\" alt=\"increase_decrease_options_051322.jpg\" width=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find max increase algo V3\n",
    "# def find_breakpoint_v3(df, column, ratio_threshold):\n",
    "#     \"\"\"\n",
    "#     Scan column of the dataframe until it finds a breakpoint (= increase larger than threshold)\n",
    "    \n",
    "#     :param df: dataframe\n",
    "#     :param column: column to scan\n",
    "#     :param ratio_threshold: minimum increase ratio threshold \n",
    "#     \"\"\"\n",
    "#     i = 0\n",
    "#     df_len = len(df)\n",
    "#     moving_avg_half = 15\n",
    "        \n",
    "#     # with pd.option_context('display.max_rows', 70):\n",
    "#     #     display(df.head(65))\n",
    "\n",
    "#     # scan dataset for increase larger than threshold\n",
    "#     for date_index, row in ts_pt_df.iterrows():\n",
    "#         if (i >= (60 + moving_avg_half) and i < df_len):\n",
    "#             sub60 = df.iloc[i-60][column]\n",
    "#             sub30 = df.iloc[i-30][column]\n",
    "#             now = df.iloc[i][column]\n",
    "            \n",
    "        \n",
    "#             d1 = sub30 - sub60\n",
    "#             d2 = now - sub30\n",
    "            \n",
    "#             # avoid  weird ratios obtained by diving by a difference between -1 and 1 \n",
    "#             if (0 <= d1 < 1):\n",
    "#                 d1 = 1\n",
    "#             elif (-1 < d1 < 0):\n",
    "#                 d1 = -1\n",
    "        \n",
    "#             r = d2 / d1\n",
    "\n",
    "#             # at least 10 patrons in the prior period\n",
    "#             if (d1 > 10) & (d2 > d1) & (r >= ratio_threshold):\n",
    "#                 bkpnt_dict = {\n",
    "#                     \"bkpt_date\"         : df.iloc[i-30]['date'],\n",
    "#                     \"bkpt_date_sub30\"   : df.iloc[i-60]['date'],\n",
    "#                     \"bkpt_date_add30\"   : df.iloc[i]['date'],\n",
    "#                     \"avg_patrons_bkpnt\" : sub30,\n",
    "#                     \"avg_patrons_sub30\" : sub60,\n",
    "#                     \"avg_patrons_add30\" : now,\n",
    "#                     \"d1\"                : d1,\n",
    "#                     \"d2\"                : d2,\n",
    "#                     \"r\"                 : r\n",
    "#                 }\n",
    "                \n",
    "#                 return bkpnt_dict\n",
    "#         i = i + 1\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max increase algo V3\n",
    "def find_breakpoint_v3(df, column, ratio_threshold):\n",
    "    \"\"\"\n",
    "    Scan column of the dataframe until it finds a breakpoint (= increase larger than threshold)\n",
    "    \n",
    "    :param df: dataframe\n",
    "    :param column: column to scan\n",
    "    :param ratio_threshold: minimum increase ratio threshold \n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    df_len = len(df)\n",
    "    moving_avg_half = 15\n",
    "        \n",
    "    # with pd.option_context('display.max_rows', 70):\n",
    "    #     display(df.head(65))\n",
    "\n",
    "    # scan dataset for increase larger than threshold\n",
    "    for date_index, row in ts_pt_df.iterrows():\n",
    "        if (i >= (30 + moving_avg_half) and i < df_len-90):\n",
    "            sub30 = df.iloc[i-30][column]\n",
    "            point = df.iloc[i][column]\n",
    "            add30 = df.iloc[i+30][column]\n",
    "            add60 = df.iloc[i+60][column]\n",
    "            add90 = df.iloc[i+90][column]\n",
    "\n",
    "            d1 = point - sub30\n",
    "            d2 = add30 - point            \n",
    "            d3 = add60 - add30            \n",
    "            d4 = add90 - add60            \n",
    "            \n",
    "            # avoid  weird ratios obtained by diving by a difference between -1 and 1 \n",
    "            if (0 <= d1 < 1):\n",
    "                d1 = 1\n",
    "            elif (-1 < d1 < 0):\n",
    "                d1 = -1\n",
    "        \n",
    "            r_d1_d2 = d2 / d1\n",
    "\n",
    "            # at least 10 patrons in the prior period\n",
    "            if (d1 > 10) & (d2 > d1) & (r_d1_d2 >= ratio_threshold):\n",
    "                bkpnt_dict = {\n",
    "                    \"bkpt_date\"         : df.iloc[i]['date'],\n",
    "                    \"bkpt_date_sub30\"   : df.iloc[i-30]['date'],\n",
    "                    \"bkpt_date_add30\"   : df.iloc[i+30]['date'],\n",
    "                    \"bkpt_date_add60\"   : df.iloc[i+60]['date'],\n",
    "                    \"bkpt_date_add90\"   : df.iloc[i+90]['date'],\n",
    "                    \"avg_patrons_bkpnt\" : point,\n",
    "                    \"avg_patrons_sub30\" : sub30,\n",
    "                    \"avg_patrons_add30\" : add30,\n",
    "                    \"avg_patrons_add60\" : add60,\n",
    "                    \"avg_patrons_add90\" : add90,\n",
    "                    \"d1\"                : d1,\n",
    "                    \"d2\"                : d2,\n",
    "                    \"d3\"                : d3,\n",
    "                    \"d4\"                : d4,\n",
    "                    \"r_d1_d2\"           : r_d1_d2\n",
    "                }\n",
    "                \n",
    "                return bkpnt_dict\n",
    "        i = i + 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to 1 patreon account, sort by date and drop duplicates\n",
    "def restrict_acct_and_sort_df(df, patreon_col, patreon_id, date_col):\n",
    "    restr_df = df[df[patreon_col] == patreon_id].copy()  \n",
    "    restr_df = restr_df.sort_values(by=[date_col])\n",
    "    restr_df = restr_df.drop_duplicates()\n",
    "    return restr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCR_RATIO_THRESH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Find breakpoint and store Patreon breakpoint in \"df_treated\"\n",
    "\n",
    "# # variables declaration\n",
    "# MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# ROLLING_AVG_WINDOW = 30\n",
    "# treated_tuples = []\n",
    "# no_bkpnt_cnt = 0\n",
    "# no_overlap = 0\n",
    "\n",
    "\n",
    "\n",
    "# print(f'Iterate over {len(top_patreons)} patreon accounts...')\n",
    "\n",
    "# # LOOP OVER TOP PATREON ACCOUNTS\n",
    "# for idx, patreon in enumerate(tqdm(top_patreons)):\n",
    "#     treat = None\n",
    "    \n",
    "#     ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "#     # patreon earnings and users\n",
    "#     tmp_df_pt = restrict_acct_and_sort_df(df_top_pt_accts, 'patreon', patreon, 'date')\n",
    "\n",
    "#     # youtube videos\n",
    "#     tmp_df_yt = restrict_acct_and_sort_df(df_yt_timeseries_top_pt, 'patreon_id', patreon, 'datetime')\n",
    "\n",
    "#     # youtube metadata\n",
    "#     tmp_df_yt_meta = restrict_acct_and_sort_df(df_yt_metadata_pt_filtered, 'patreon_id', patreon, 'upload_date')\n",
    "    \n",
    "#     ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "#     # set min and max dates for plots   \n",
    "#     date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "#     date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "#     # if no overlap period between YT and Patreon datasets, skip account\n",
    "#     if date_max < date_min:\n",
    "#         no_overlap += 1\n",
    "#         # print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "#         continue\n",
    "        \n",
    "#     # restrict datasets between min and max dates\n",
    "#     tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "#     tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    \n",
    "#     # align both dataframes since youtube starts once a week\n",
    "#     tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "#     tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "#     tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "#     ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "#     # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "#     ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "#     ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "#     ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "#     ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "#     tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "#     # reorder columns to have deltas columns next to their respective columns\n",
    "#     patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "#     ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "#     # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "#     ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "\n",
    "\n",
    "    \n",
    "#     ########################## PRINT TITLES ##########################\n",
    "#     # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "#     # ch_ids = tmp_df_yt['channel'].unique()\n",
    "#     # print(f\"\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "    \n",
    "#     ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "#     bkpnt_dict = find_breakpoint_v3(tmp_df_pt, 'patrons_ma', INCR_RATIO_THRESH)\n",
    "#     # print(\"bkpnt_dict: \", bkpnt_dict)\n",
    "    \n",
    "#     if bkpnt_dict == None:\n",
    "#         no_bkpnt_cnt += 1        \n",
    "#         # print(\"No breakpoint for this account...\")\n",
    "#         continue\n",
    "#     else: \n",
    "#         treat = 1\n",
    "\n",
    "    \n",
    "#     ################################### CALCULATE DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "    \n",
    "#     ##### PATREON #####\n",
    "#     tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_sub30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date'])]\n",
    "#     tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add30'])]\n",
    "#     tmp_df_PT_add60 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_add30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add60'])]\n",
    "#     tmp_df_PT_add90 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_add60']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add90'])]\n",
    "\n",
    "#     # delta patrons\n",
    "#     mean_delta_patrons_sub30 = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "#     mean_delta_patrons_add30 = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "#     mean_delta_patrons_add60 = tmp_df_PT_add60['delta_patrons'].mean()\n",
    "#     mean_delta_patrons_add90 = tmp_df_PT_add90['delta_patrons'].mean()\n",
    "        \n",
    "#     # delta earnings\n",
    "#     mean_delta_earnings_sub30 = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "#     mean_delta_earnings_add30 = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "#     mean_delta_earnings_add60 = tmp_df_PT_add60['delta_earning'].mean()  \n",
    "#     mean_delta_earnings_add90 = tmp_df_PT_add90['delta_earning'].mean()  \n",
    "\n",
    "    \n",
    "#     ##### YOUTUBE TIME SERIES #####\n",
    "#     tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date']      )]\n",
    "#     tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "#     tmp_df_YT_add60 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_add30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add60'])]\n",
    "#     tmp_df_YT_add90 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_add60']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add90'])]\n",
    "    \n",
    "#     # delta videos\n",
    "#     mean_delta_videos_sub30 = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "#     mean_delta_videos_add30 = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "#     mean_delta_videos_add60 = tmp_df_YT_add60['delta_videos'].mean()  \n",
    "#     mean_delta_videos_add90 = tmp_df_YT_add90['delta_videos'].mean()  \n",
    "\n",
    "#     # delta views\n",
    "#     mean_delta_views_sub30 = tmp_df_YT_sub30['delta_views'].mean()\n",
    "#     mean_delta_views_add30 = tmp_df_YT_add30['delta_views'].mean()  \n",
    "#     mean_delta_views_add60 = tmp_df_YT_add60['delta_views'].mean()  \n",
    "#     mean_delta_views_add90 = tmp_df_YT_add90['delta_views'].mean()  \n",
    "\n",
    "#     # delta subscriptions\n",
    "#     mean_delta_subs_sub30 = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "#     mean_delta_subs_add30 = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "#     mean_delta_subs_add60 = tmp_df_YT_add60['delta_subs'].mean()  \n",
    "#     mean_delta_subs_add90 = tmp_df_YT_add90['delta_subs'].mean()  \n",
    "\n",
    "    \n",
    "#     ##### YOUTUBE METADATA #####\n",
    "#     tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date']      )]\n",
    "#     tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "#     tmp_df_YT_META_add60 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_add30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add60'])]\n",
    "#     tmp_df_YT_META_add90 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_add60']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add90'])]\n",
    "        \n",
    "#     # durations\n",
    "#     mean_duration_sub30 = tmp_df_YT_META_sub30['duration'].mean()\n",
    "#     mean_duration_add30 = tmp_df_YT_META_add30['duration'].mean()      \n",
    "#     mean_duration_add60 = tmp_df_YT_META_add60['duration'].mean()      \n",
    "#     mean_duration_add90 = tmp_df_YT_META_add90['duration'].mean()      \n",
    "        \n",
    "#     # likes\n",
    "#     mean_likes_sub30 = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "#     mean_likes_add30 = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "#     mean_likes_add60 = tmp_df_YT_META_add60['like_count'].mean()      \n",
    "#     mean_likes_add90 = tmp_df_YT_META_add90['like_count'].mean()      \n",
    "    \n",
    "#     yt_channel_id = tmp_df_yt['channel'].unique()[0]\n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "#     treated_tuples.append(\n",
    "#         (          \n",
    "#             patreon, \n",
    "#             yt_channel_id,   \n",
    "#             treat,\n",
    "#             bkpnt_dict[\"d1\"], \n",
    "#             bkpnt_dict[\"d2\"], \n",
    "#             bkpnt_dict[\"d3\"], \n",
    "#             bkpnt_dict[\"d4\"], \n",
    "            \n",
    "#             bkpnt_dict[\"r_d1_d2\"],\n",
    "            \n",
    "#             bkpnt_dict[\"bkpt_date\"], \n",
    "#             bkpnt_dict[\"bkpt_date_sub30\"], \n",
    "#             bkpnt_dict[\"bkpt_date_add30\"],\n",
    "#             bkpnt_dict[\"bkpt_date_add60\"],\n",
    "#             bkpnt_dict[\"bkpt_date_add90\"],\n",
    "            \n",
    "#             bkpnt_dict[\"avg_patrons_bkpnt\"], \n",
    "#             bkpnt_dict[\"avg_patrons_sub30\"], \n",
    "#             bkpnt_dict[\"avg_patrons_add30\"], \n",
    "#             bkpnt_dict[\"avg_patrons_add60\"], \n",
    "#             bkpnt_dict[\"avg_patrons_add90\"], \n",
    "            \n",
    "#             # delta patrons\n",
    "#             mean_delta_patrons_sub30,\n",
    "#             mean_delta_patrons_add30,\n",
    "#             mean_delta_patrons_add60,\n",
    "#             mean_delta_patrons_add90,\n",
    "\n",
    "#             # delta earnings\n",
    "#             mean_delta_earnings_sub30,\n",
    "#             mean_delta_earnings_add30,\n",
    "#             mean_delta_earnings_add60, \n",
    "#             mean_delta_earnings_add90,\n",
    "\n",
    "#             # delta videos\n",
    "#             mean_delta_videos_sub30,\n",
    "#             mean_delta_videos_add30,\n",
    "#             mean_delta_videos_add60,\n",
    "#             mean_delta_videos_add90,\n",
    "\n",
    "#             # delta views\n",
    "#             mean_delta_views_sub30,\n",
    "#             mean_delta_views_add30,\n",
    "#             mean_delta_views_add60,\n",
    "#             mean_delta_views_add90,\n",
    "\n",
    "#             # delta subscriptions\n",
    "#             mean_delta_subs_sub30,\n",
    "#             mean_delta_subs_add30,\n",
    "#             mean_delta_subs_add60,\n",
    "#             mean_delta_subs_add90,\n",
    "\n",
    "#             # durations\n",
    "#             mean_duration_sub30,\n",
    "#             mean_duration_add30,     \n",
    "#             mean_duration_add60,    \n",
    "#             mean_duration_add90, \n",
    "\n",
    "#             # likes\n",
    "#             mean_likes_sub30,\n",
    "#             mean_likes_add30,  \n",
    "#             mean_likes_add60,   \n",
    "#             mean_likes_add90\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "            \n",
    "# df_treated = pd.DataFrame(treated_tuples, columns = [\n",
    "#     'patreon_id',\n",
    "#     'yt_channel_id',\n",
    "#     'treat',\n",
    "#     'd1', \n",
    "#     'd2', \n",
    "#     'd3',\n",
    "#     'd4',\n",
    "    \n",
    "#     'ratio_d1_d2',\n",
    "#     'bkpt_date',     \n",
    "#     'bkpt_date_sub30', \n",
    "#     'bkpt_date_add30', \n",
    "#     'bkpt_date_add60',\n",
    "#     'bkpt_date_add90',\n",
    "    \n",
    "#     'avg_patrons_bkpnt', \n",
    "#     'avg_patrons_sub30', \n",
    "#     'avg_patrons_add30', \n",
    "#     'avg_patrons_add60',\n",
    "#     'avg_patrons_add90',\n",
    "    \n",
    "#     # delta patrons\n",
    "#     'mean_delta_patrons_sub30',\n",
    "#     'mean_delta_patrons_add30',\n",
    "#     'mean_delta_patrons_add60',\n",
    "#     'mean_delta_patrons_add90',\n",
    "\n",
    "#     # delta earnings\n",
    "#     'mean_delta_earnings_sub30',\n",
    "#     'mean_delta_earnings_add30',\n",
    "#     'mean_delta_earnings_add60',\n",
    "#     'mean_delta_earnings_add90',\n",
    "\n",
    "#     # delta videos\n",
    "#     'mean_delta_videos_sub30',\n",
    "#     'mean_delta_videos_add30',\n",
    "#     'mean_delta_videos_add60',\n",
    "#     'mean_delta_videos_add90',\n",
    "\n",
    "#     # delta views\n",
    "#     'mean_delta_views_sub30',\n",
    "#     'mean_delta_views_add30',\n",
    "#     'mean_delta_views_add60',\n",
    "#     'mean_delta_views_add90',\n",
    "\n",
    "#     # delta subscriptions\n",
    "#     'mean_delta_subs_sub30',\n",
    "#     'mean_delta_subs_add30',\n",
    "#     'mean_delta_subs_add60',\n",
    "#     'mean_delta_subs_add90',\n",
    "\n",
    "#     # durations\n",
    "#     'mean_duration_sub30',\n",
    "#     'mean_duration_add30',\n",
    "#     'mean_duration_add60',\n",
    "#     'mean_duration_add90',\n",
    "\n",
    "#     # likes\n",
    "#     'mean_likes_sub30',\n",
    "#     'mean_likes_add30',\n",
    "#     'mean_likes_add60',\n",
    "#     'mean_likes_add90'\n",
    "# ])\n",
    "\n",
    "\n",
    "# print(f'Patreon accounts added to the treated group (increase ratio >= {INCR_RATIO_THRESH}):                      {len(df_treated):>3}')\n",
    "# print(f'Patreon accounts with no breakpoints found:                                             {no_bkpnt_cnt:>3}')\n",
    "# print(f'Patreon accounts with overlapping period between YouTube and Patreon datasets found:    {no_overlap:>3}')\n",
    "\n",
    "\n",
    "# df_treated['bkpt_date'] = pd.to_datetime(df_treated['bkpt_date'])\n",
    "# df_treated['bkpt_date_sub30'] = pd.to_datetime(df_treated['bkpt_date_sub30'])\n",
    "# df_treated['bkpt_date_add30'] = pd.to_datetime(df_treated['bkpt_date_add30'])\n",
    "# df_treated['bkpt_date_add60'] = pd.to_datetime(df_treated['bkpt_date_add60'])\n",
    "# df_treated['bkpt_date_add90'] = pd.to_datetime(df_treated['bkpt_date_add90'])\n",
    "# df_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"df_treated\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"df_treated.tsv.gz\"\n",
    "# df_treated.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_treated.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated = pd.read_csv(LOCAL_DATA_FOLDER+\"df_treated.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_treated['bkpt_date'] = pd.to_datetime(df_treated['bkpt_date'])\n",
    "df_treated['bkpt_date_sub30'] = pd.to_datetime(df_treated['bkpt_date_sub30'])\n",
    "df_treated['bkpt_date_add30'] = pd.to_datetime(df_treated['bkpt_date_add30'])\n",
    "df_treated['bkpt_date_add60'] = pd.to_datetime(df_treated['bkpt_date_add60'])\n",
    "df_treated['bkpt_date_add90'] = pd.to_datetime(df_treated['bkpt_date_add90'])\n",
    "print(f\"Number of treated subjects (increase ratio >= {INCR_RATIO_THRESH}): {len(df_treated)}\")\n",
    "\n",
    "df_treated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of ratios\n",
    "sns.histplot(data=df_treated['ratio_d1_d2'], stat='count', bins=50, kde=False, cumulative=False, color=f'C{i}')\n",
    "plt.title(\"Distribution of increase ratios of the treated group\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "# plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot treated accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_neg_pos(ax, x, y):\n",
    "    if y.isnull().all():\n",
    "        return\n",
    "    if (y.min() < 0): \n",
    "        # fill negative values in red and draw a horizontal line at 0\n",
    "        ax.fill_between(x, y.min(), 0, color='red', alpha=0.05)\n",
    "        ax.axhline(y=0, linestyle='solid', color= 'black', linewidth=0.5)\n",
    "    # fill positive values in green\n",
    "    # ax.fill_between(x, 0, y.max(), color='green', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number in thousand (k) or in million (M) on y axis\n",
    "def KM(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    if x > 999_999:\n",
    "        return '%2.1fM' % (x * 1e-6)\n",
    "    elif x > 999:\n",
    "        return '%2.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%3.0f ' % (x)\n",
    "KM_formatter = FuncFormatter(KM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK - \n",
    "\n",
    "\n",
    "# compare Patreon and YouTube timeseries + YouTube metadata\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# WEEK_OFFSET = pd.DateOffset(weeks=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# variables for Granger Tests\n",
    "MAXLAG = 2\n",
    "granger_dict = {} # dictionary with  keys (cause --> effect) and values with list of corresponding patreon account(s)\n",
    "not_granger = []\n",
    "YT_variables = ['yt_delta_videos', 'yt_delta_views', 'yt_delta_subs']\n",
    "PT_variables = ['pt_delta_patrons']\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in tqdm(df_treated[:1].iterrows(), total=df_treated.shape[0]):\n",
    "    patreon = row['patreon_id']\n",
    "    \n",
    "    fig, axs = plt.subplots(7, 4, figsize=(26, 10), sharey=False, sharex=False)\n",
    "        \n",
    "\n",
    "    \n",
    "    ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = df_top_pt_accts[df_top_pt_accts['patreon'] == patreon].copy()  \n",
    "    tmp_df_pt = tmp_df_pt.sort_values(by=['date'])\n",
    "    tmp_df_pt = tmp_df_pt.drop_duplicates()\n",
    "    # print(\"\\ntmp_df_pt:\")\n",
    "    # display(tmp_df_pt)\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon].copy()\n",
    "    tmp_df_yt = tmp_df_yt.sort_values(by=['datetime'])\n",
    "    \n",
    "    # youtube metadata\n",
    "    tmp_df_yt_meta = df_yt_metadata_pt_filtered[df_yt_metadata_pt_filtered['patreon_id'] == patreon].copy()   \n",
    "    tmp_df_yt_meta = tmp_df_yt_meta.sort_values('upload_date')\n",
    "    # tmp_df_yt_meta['upload_date'] = pd.to_datetime(tmp_df_yt_meta['upload_date'])\n",
    "    \n",
    "    # replace dates that were collected after 23:00 to their next day, and remove hour\n",
    "    tmp_df_yt['datetime_original'] = tmp_df_yt['datetime']\n",
    "    tmp_df_yt['datetime'] = tmp_df_yt['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "    \n",
    "    # remove hours and convert to datetime type\n",
    "    tmp_df_yt['datetime'] = pd.to_datetime(tmp_df_yt['datetime'].dt.date)\n",
    "    \n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    ch_ids = tmp_df_yt['channel'].unique()\n",
    "    print(f\"\\n\\n\\n\\033[1m {idx+1}: {patreon[12:]} (treat = {row['treat']})\\033[0m\")\n",
    "    print(f\"https://www.{patreon}\")\n",
    "    print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    for ch_id in ch_ids:\n",
    "        print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "   \n",
    "    print(f'\\nYouTube Metadata: ')\n",
    "    \n",
    "    if not (tmp_df_yt_meta.empty):\n",
    "        print('• YT videos were uploaded between {} and {}'.format(tmp_df_yt_meta['upload_date'].min().strftime('%B %d, %Y'),\n",
    "                                                                 tmp_df_yt_meta['upload_date'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "        print('• YT metadata was crawled between {} and {}'.format(tmp_df_yt_meta['crawl_date'].min().strftime('%B %d, %Y'),\n",
    "                                                                 tmp_df_yt_meta['crawl_date'].max().strftime('%B %d, %Y')))\n",
    "    else:\n",
    "        print('• No metadata available for this acount')\n",
    "\n",
    "    \n",
    "    ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "    # set min and max dates for plots   \n",
    "    date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "    date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "    if date_max < date_min:\n",
    "        print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "        continue\n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "    tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    tmp_df_yt_meta = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min) & (tmp_df_yt_meta['upload_date'] <= date_max)]\n",
    "    \n",
    "    # align both dataframes since youtube starts once a week\n",
    "    tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "              \n",
    "    ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "    # breakpoint_date = find_breakpoint_v2(tmp_df_pt, 'patrons_ma')\n",
    "    print(\"Breakpoint date: \", breakpoint_date.date())\n",
    "\n",
    "    # check that dates prior and after breakpoint exist\n",
    "    if not (((breakpoint_date - 1*MONTH_OFFSET)) in ts_pt_df.index and ((breakpoint_date + 1*MONTH_OFFSET) in ts_pt_df.index)):\n",
    "        print(f\"ERROR: Breakpoint too close to edge of patreon time series or missing data\\n\")\n",
    "        plt.figure().clear(); plt.close(); plt.cla(); plt.clf(); plt.show()\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    avg_patrons_add60 = row['avg_patrons_add60']\n",
    "    avg_patrons_add90 = row['avg_patrons_add90']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    bkpt_date_add60 = row['bkpt_date_add60']\n",
    "    bkpt_date_add90 = row['bkpt_date_add90']\n",
    "\n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "    d3 = row['d3']\n",
    "    d4 = row['d4']\n",
    "\n",
    "    \n",
    "    r_d1_d2 = row['ratio_d1_d2']\n",
    "\n",
    "    print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "    print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "    print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "    print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    print(f'• At breakpoint + 60days ({bkpt_date_add60.date()}): {avg_patrons_add60:,.1f}')\n",
    "    print(f'• At breakpoint + 90days ({bkpt_date_add90.date()}): {avg_patrons_add90:,.1f}')\n",
    "    \n",
    "    print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "    print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date_add30.date()} to {bkpt_date_add60.date()}:        d3  = {d3:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date_add60.date()} to {bkpt_date_add90.date()}:        d4  = {d4:>+6.1f} patrons\")\n",
    "    \n",
    "    print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "    print(f\"• Ratio between 2 increases:                            d2/d1  = {r_d1_d2:.2f}\")\n",
    "    print(f\"• Percentage increase:                              d2/d1*100  = {r_d1_d2:>+.0%}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ################################### GET DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "       \n",
    "    \n",
    "    mean_date_sub30 = row['bkpt_date_sub30'] + (row['bkpt_date']       - row['bkpt_date_sub30'])/2\n",
    "    mean_date_add30 = row['bkpt_date']       + (row['bkpt_date_add30'] - row['bkpt_date'])/2\n",
    "    mean_date_add60 = row['bkpt_date_add30'] + (row['bkpt_date_add60'] - row['bkpt_date_add30'])/2\n",
    "    mean_date_add90 = row['bkpt_date_add60'] + (row['bkpt_date_add90'] - row['bkpt_date_add60'])/2\n",
    "  \n",
    "\n",
    "    mean_dates = [\n",
    "        mean_date_sub30, \n",
    "        mean_date_add30, \n",
    "        mean_date_add60, \n",
    "        mean_date_add90\n",
    "    ]  \n",
    "    \n",
    "    mean_delta_patrons = [\n",
    "        row['mean_delta_patrons_sub30'],\n",
    "        row['mean_delta_patrons_add30'],\n",
    "        row['mean_delta_patrons_add60'],\n",
    "        row['mean_delta_patrons_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_earnings = [\n",
    "        row['mean_delta_earnings_sub30'],\n",
    "        row['mean_delta_earnings_add30'],\n",
    "        row['mean_delta_earnings_add60'],\n",
    "        row['mean_delta_earnings_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_videos = [\n",
    "        row['mean_delta_videos_sub30'],\n",
    "        row['mean_delta_videos_add30'],\n",
    "        row['mean_delta_videos_add60'],\n",
    "        row['mean_delta_videos_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_views = [\n",
    "        row['mean_delta_views_sub30'],\n",
    "        row['mean_delta_views_add30'],\n",
    "        row['mean_delta_views_add60'],\n",
    "        row['mean_delta_views_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_subscriptions = [\n",
    "        row['mean_delta_subs_sub30'],\n",
    "        row['mean_delta_subs_add30'],\n",
    "        row['mean_delta_subs_add60'],\n",
    "        row['mean_delta_subs_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_durations = [\n",
    "        row['mean_duration_sub30'],\n",
    "        row['mean_duration_add30'],\n",
    "        row['mean_duration_add60'],\n",
    "        row['mean_duration_add90']\n",
    "    ]\n",
    "\n",
    "    mean_delta_likes = [\n",
    "        row['mean_likes_sub30'],\n",
    "        row['mean_likes_add30'],\n",
    "        row['mean_likes_add60'],\n",
    "        row['mean_likes_add90']\n",
    "    ]\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    # patreons\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_patrons):\n",
    "        axs[0,2].plot(mean_date, mean_value, marker='o', color='orange', markersize=10)\n",
    "\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_earnings):\n",
    "        axs[1,2].plot(mean_date, mean_value, marker='o', color='royalblue', markersize=10)\n",
    "\n",
    "\n",
    "    # youtube\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_videos):\n",
    "        axs[2,2].plot(mean_date, mean_value, marker='o', color='r', markersize=10)\n",
    "\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_views):\n",
    "        axs[3,2].plot(mean_date, mean_value, marker='o', color='g', markersize=10)\n",
    "\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_subscriptions):\n",
    "        axs[4,2].plot(mean_date, mean_value, marker='o', color='m', markersize=10)\n",
    "        \n",
    "\n",
    "    # youtube metadata\n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_durations):\n",
    "        axs[5,2].plot(mean_date, mean_value, marker='o', color='brown', markersize=10)\n",
    "        \n",
    "    for mean_date, mean_value in zip(mean_dates, mean_delta_likes):\n",
    "        axs[6,2].plot(mean_date, mean_value, marker='o', color='lightblue', markersize=10)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     # plot horizontal lines for means\n",
    "#     mean_befor_list = [mean_delta_patrons_befor, mean_delta_earnings_befor, mean_delta_videos_befor, mean_delta_views_befor, mean_delta_subs_befor, mean_duration_befor, mean_likes_befor]\n",
    "#     mean_afer_list = [mean_delta_patrons_after, mean_delta_earnings_after, mean_delta_videos_after, mean_delta_views_after, mean_delta_subs_after, mean_duration_after, mean_likes_after]\n",
    "       \n",
    "#     for idx, mean in enumerate(mean_befor_list):\n",
    "#             if not math.isnan(mean):\n",
    "#                 axs[idx,2].hlines(y=mean, xmin=bkpt_date_sub30, xmax=bkpt_date      , linewidth=2, linestyle='--', color='green')\n",
    "\n",
    "#     for idx, mean in enumerate(mean_afer_list):\n",
    "#             if not math.isnan(mean):\n",
    "#                 axs[idx,2].hlines(y=mean, xmin=bkpt_date,       xmax=bkpt_date_add30, linewidth=2, linestyle='--', color='orange')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    ################################### ZOOM OUT PLOTS ###################################\n",
    "    \n",
    "    # number of patrons (delta)\n",
    "    axs[0,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,0].set(title=\"Delta patrons per week\")\n",
    "    axs[0,0].set_ylabel(\"Δ Patrons\")    \n",
    "    color_neg_pos(axs[0,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'])\n",
    "\n",
    "    # number of patrons (cumulative)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons'], alpha=0.2)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons_ma'])\n",
    "    axs[0,1].set(title=\"Number of patrons\")\n",
    "    axs[0,1].set_ylabel(\"# Patrons\")\n",
    "\n",
    "    # patreon earnings (delta)\n",
    "    axs[1,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,0].set(title=\"Patreon delta earnings per week\")\n",
    "    axs[1,0].set_ylabel(\"Δ Earnings\") \n",
    "    color_neg_pos(axs[1,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'])\n",
    "\n",
    "    # patreon earnings (cumulative)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning'], alpha=0.2)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning_ma'], color='royalblue')\n",
    "    axs[1,1].set(title=\"Patreon earnings per month\")\n",
    "    axs[1,1].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # youtube videos (delta)\n",
    "    axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,0].set(title=\"YouTube delta videos per week\")\n",
    "    axs[2,0].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,0], tmp_df_yt['datetime'], tmp_df_yt['delta_videos'])\n",
    "\n",
    "    # youtube videos (cumulative)\n",
    "    axs[2,1].plot(tmp_df_yt['datetime'], tmp_df_yt['videos'], 'r')\n",
    "    axs[2,1].set(title=\"YouTube cumulative videos\")\n",
    "    axs[2,1].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # youtube views (delta)\n",
    "    axs[3,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,0].set(title=\"YouTube delta views per week\")\n",
    "    axs[3,0].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,0], tmp_df_yt['datetime'], tmp_df_yt['delta_views'])\n",
    "\n",
    "    # youtube views (cumulative)\n",
    "    axs[3,1].plot(tmp_df_yt['datetime'], tmp_df_yt['views'], 'g')\n",
    "    axs[3,1].set(title=\"YouTube cumulative views\")\n",
    "    axs[3,1].set_ylabel(\"# Views\")\n",
    "\n",
    "    # youtube subs (delta)\n",
    "    axs[4,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,0].set(title=\"YouTube delta subscriptions per week\")\n",
    "    axs[4,0].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,0], tmp_df_yt['datetime'], tmp_df_yt['delta_subs'])\n",
    "\n",
    "    # youtube subs (cumulative)\n",
    "    axs[4,1].plot(tmp_df_yt['datetime'], tmp_df_yt['subs'], 'm')\n",
    "    axs[4,1].set(title=\"YouTube cumulative subscriptions\")\n",
    "    axs[4,1].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,0].set(title=\"YouTube videos durations\")\n",
    "    axs[5,0].set_ylabel(\"Duration\")\n",
    "    \n",
    "    \n",
    "    # youtube likes at crawl date\n",
    "    axs[6,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,0].set(title=\"YouTube likes (plotted against upload date)\")\n",
    "    axs[6,0].set_ylabel(\"Likes\")\n",
    "    \n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = bkpt_date_sub30 - (1 * MONTH_OFFSET)\n",
    "    date_max_zoom = bkpt_date_add90 + (1 * MONTH_OFFSET)\n",
    "    \n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_meta_zoomed = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min_zoom) & (tmp_df_yt_meta['upload_date'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "    \n",
    "   ################################### ZOOM IN PLOTS  ###################################\n",
    "\n",
    "    # zoomed in patron numbers (delta)\n",
    "    axs[0,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', alpha=0.3)\n",
    "    axs[0,2].set(title=\"Delta patrons per week\")\n",
    "    axs[0,2].set_ylabel(\"Δ Patrons\")\n",
    "    color_neg_pos(axs[0,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'])\n",
    "    \n",
    "    # zoomed in patron numbers (cumulative)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons'], alpha=0.2)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons_ma'])\n",
    "    axs[0,3].set(title=\"Number of patrons (zoomed in)\")\n",
    "    axs[0,3].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "    # zoomed in patron earnings (delta)\n",
    "    axs[1,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', alpha=0.3)\n",
    "    axs[1,2].set(title=\"Delta Patreon earnings per week (zoomed in)\")\n",
    "    axs[1,2].set_ylabel(\"Earnings\")  \n",
    "    color_neg_pos(axs[1,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_earning'])\n",
    "\n",
    "    # zoomed in patron earnings (cumulative)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning'], alpha=0.2)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning_ma'], color='royalblue')\n",
    "    axs[1,3].set(title=\"Patreon earnings per month (zoomed in)\")\n",
    "    axs[1,3].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # zoomed in youtube videos (delta)\n",
    "    axs[2,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', alpha=0.3)\n",
    "    axs[2,2].set(title=\"YouTube delta videos per week (zoomed in)\")\n",
    "    axs[2,2].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_videos'])\n",
    "\n",
    "    # zoomed in youtube videos (cumulative)\n",
    "    axs[2,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['videos'], 'r')\n",
    "    axs[2,3].set(title=\"YouTube cumulative videos (zoomed in)\")\n",
    "    axs[2,3].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # zoomed in youtube views (delta)\n",
    "    axs[3,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', alpha=0.3)\n",
    "    axs[3,2].set(title=\"YouTube delta views per week (zoomed in)\")\n",
    "    axs[3,2].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_views'])\n",
    "\n",
    "    # zoomed in youtube views (cumulative)\n",
    "    axs[3,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['views'], 'g')\n",
    "    axs[3,3].set(title=\"YouTube cumulative views (zoomed in)\")\n",
    "    axs[3,3].set_ylabel(\"# Views\")\n",
    "    \n",
    "    # zoomed in youtube subs (delta)\n",
    "    axs[4,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', alpha=0.3)\n",
    "    axs[4,2].set(title=\"YouTube delta subscriptions per week (zoomed in)\")\n",
    "    axs[4,2].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_subs'])\n",
    "\n",
    "    # zoomed in youtube subs (cumulative)\n",
    "    axs[4,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['subs'], 'm')\n",
    "    axs[4,3].set(title=\"YouTube cumulative subscriptions (zoomed in)\")\n",
    "    axs[4,3].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', alpha=0.3)\n",
    "    axs[5,2].set(title=\"YouTube videos durations (zoomed in)\")\n",
    "    axs[5,2].set_ylabel(\"Duration\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'])\n",
    "    \n",
    "        \n",
    "   # youtube likes per uploads\n",
    "    axs[6,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', alpha=0.3)\n",
    "    axs[6,2].set(title=\"YouTube likes (plotted against upload date) (zoomed in)\")\n",
    "    axs[6,2].set_ylabel(\"Likes\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['crawl_date'], tmp_df_yt_meta_zoomed['like_count'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################### FORMAT AXES ###################################\n",
    "\n",
    "    # format the axes\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if j < 2:\n",
    "                axs[i,j].set_xlim([date_min, date_max])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            if j >= 2:\n",
    "                axs[i,j].set_xlim([date_min_zoom, date_max_zoom])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "                # axs[i,j].xaxis.set_minor_locator(mdates.WeekdayLocator())\n",
    "            axs[i,j].xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.set_major_formatter(KM_formatter)\n",
    "            \n",
    "            \n",
    "    ################################### PLOT BREAKPOINT LINES AND POINTS ###################################\n",
    "\n",
    "    # plot vertical lines for breakpoint, breakpoint-1month, breakpoint+1month\n",
    "    print_legend = True\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if print_legend:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', label='breakpoint', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)          \n",
    "                axs[i,j].axvline(breakpoint_date + 2*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)          \n",
    "                axs[i,j].axvline(breakpoint_date + 3*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)          \n",
    "                # print_legend = False\n",
    "            else:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + 2*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + 3*MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "    # axs[0,0].legend()\n",
    "    axs[0,1].legend()\n",
    "\n",
    "    # plot point for mean nb of patrons for breakpoint, breakpoint-1month, breakpoint+1month    \n",
    "    axs[0,3].plot(breakpoint_date - MONTH_OFFSET, ts_pt_df.at[(breakpoint_date - MONTH_OFFSET), 'patrons_ma'], marker='o', color='green')\n",
    "    axs[0,3].plot(breakpoint_date,               ts_pt_df.at[breakpoint_date              , 'patrons_ma'], marker='o', color='red')    \n",
    "    axs[0,3].plot(breakpoint_date + MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "    axs[0,3].plot(breakpoint_date + 2*MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + 2*MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "    axs[0,3].plot(breakpoint_date + 3*MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + 3*MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "\n",
    "\n",
    "    ################################### GRANGER CAUSALITY TESTS ###################################\n",
    "\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    fig.tight_layout(w_pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# print('\\n\\nGranger tests summary statistics:')\n",
    "    \n",
    "# print(f'• Number of patreon accounts analysed (patrons increase ratio > {incr_thresh_ratio}): {len(df_granger)}')\n",
    "# print(f'• Number of patreon with no Granger-causal link: {len(not_granger)} ({len(not_granger)/len(df_granger):.0%})')\n",
    "\n",
    "# print(f'• Number of patreon accounts per Granger-causal link:')\n",
    "# # Converting granger dict into list of tuples (in order to sort it), the 2nd value of the tuple being the count of accounts\n",
    "# granger_list = [(k, len(v)) for k, v in granger_dict.items()]\n",
    "# # sort by count desc\n",
    "# granger_list_desc = sorted(granger_list, key=lambda tup: -tup[1])\n",
    "# for (k,v) in granger_list_desc:\n",
    "#     print(f'    • {k[0]} \\t--> {k[1]}:\\t {v} ({v/len(df_granger):.0%})')\n",
    "\n",
    "\n",
    "# df_granger[columns] = df_granger[columns].astype('Int64')\n",
    "# df_granger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Select \"control\" accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential \"control\" accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each subject in “Treated” group:\n",
    "    - For each subject in Population (except for current Treated subject):\n",
    "        - Run breakpoint detection algorithm up to the Treated breakpoint date\n",
    "            - If we find a breakpoint --> reject \n",
    "            - If we dont find a breakpoint --> add to _potential_ control subjects (aka “Potential Control Subjects”) for this Treated subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_potential_matches = {}  \n",
    "\n",
    "# print(f\"Look for potential matches for {len(df_treated)} treated subjects\")\n",
    "\n",
    "# for idx_treated, treated_subject_row in tqdm(df_treated.iterrows(), total=df_treated.shape[0]):\n",
    "#     treated_subject = treated_subject_row['patreon_id']\n",
    "#     # print(f\"\\ntreated subject: {idx_treated}, {treated_subject}\")\n",
    "    \n",
    "#     # make sure date cant go beyond treated group\n",
    "#     date_max = treated_subject_row['bkpt_date_add30']    \n",
    "#     # print(\"date_max: \", date_max)\n",
    "    \n",
    "#     for idx_potential_control, potential_control_subject in enumerate(top_patreons):\n",
    "#     # print(f\"\\n\\tpotential control subject: {idx_potential_control}, {potential_control_subject}\")        \n",
    "        \n",
    "#         # make sure potential control account is not the same as treated account         \n",
    "#         if (potential_control_subject == treated_subject):\n",
    "#             # print(\"potential_control_subject == treated_subject\")\n",
    "#             continue\n",
    "            \n",
    "#         # eliminate potential control accounts that have a breakpoint date earlier than treated\n",
    "#         if (potential_control_subject not in df_treated['patreon_id'].tolist()):\n",
    "#             # print(\"\\t\\tNo breakpoint for this account...      => KEEP AS A POTENTIAL CONTROL GROUP :)\")\n",
    "#             if treated_subject in dict_potential_matches:\n",
    "#                 dict_potential_matches[treated_subject].append(potential_control_subject)\n",
    "#             else:\n",
    "#                 dict_potential_matches[treated_subject] = [potential_control_subject]\n",
    "#         else: \n",
    "#             # if in the treated group but has a breakpoint after max date, consider it\n",
    "#             if (df_treated[df_treated['patreon_id'] == potential_control_subject].bkpt_date.iloc[0] > date_max):\n",
    "#                 # print(\"\\t\\tBreakpoint after the max date...      => KEEP AS A POTENTIAL CONTROL GROUP :)\")\n",
    "#                 if treated_subject in dict_potential_matches:\n",
    "#                     dict_potential_matches[treated_subject].append(potential_control_subject)\n",
    "#                 else:\n",
    "#                     dict_potential_matches[treated_subject] = [potential_control_subject]\n",
    "#             else:\n",
    "#                 # reject it\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save potential dictionary matches to disk in pickle format\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"dict_potential_matches.pickle\"\n",
    "# with open(output_file_path, 'wb') as f:\n",
    "#     pickle.dump(dict_potential_matches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dict_potential_matches.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOCAL_DATA_FOLDER+\"dict_potential_matches.pickle\", 'rb') as f:\n",
    "    dict_potential_matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of treated subjects: {len(dict_potential_matches)}\")\n",
    "\n",
    "print(\"\\nNumber of potential control subjects for each treated subject (print first 5): \") \n",
    "for idx, (k, v) in enumerate(dict_potential_matches.items()):\n",
    "    if idx <5:\n",
    "        print(f'{k}: {len(v)}')\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudo intervention date (align bkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `align_breakpoint_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_breakpoint_dict(df, column, treated_subject_row):\n",
    "    \"\"\"\n",
    "    Mathces the dates of the treated subject\n",
    "    \n",
    "    :param df: dataframe\n",
    "    :param column: column to scan\n",
    "    :param treated_subject_row: treated suject to get the dates from\n",
    "    \"\"\"\n",
    "    \n",
    "    bkpt_date_sub30   = treated_subject_row['bkpt_date_sub30'].iloc[0]\n",
    "    bkpt_date         = treated_subject_row['bkpt_date'].iloc[0]\n",
    "    bkpt_date_add30   = treated_subject_row['bkpt_date_add30'].iloc[0]\n",
    "    bkpt_date_add60   = treated_subject_row['bkpt_date_add60'].iloc[0]\n",
    "    bkpt_date_add90   = treated_subject_row['bkpt_date_add90'].iloc[0]\n",
    "\n",
    "        \n",
    "    # align points from subject conto control group\n",
    "    try:        \n",
    "        sub30 = df[df['date'] == bkpt_date_sub30]['patrons_ma'].iloc[0]\n",
    "        point = df[df['date'] == bkpt_date]['patrons_ma'].iloc[0]\n",
    "        add30 = df[df['date'] == bkpt_date_add30]['patrons_ma'].iloc[0]\n",
    "        add60 = df[df['date'] == bkpt_date_add60]['patrons_ma'].iloc[0]\n",
    "        add90 = df[df['date'] == bkpt_date_add90]['patrons_ma'].iloc[0]\n",
    "    except Exception as e:\n",
    "        # print(\"Exception in align_breakpoint_dict function (date issue): \", e)\n",
    "        raise\n",
    "\n",
    "    d1 = point - sub30\n",
    "    d2 = add30 - point            \n",
    "    d3 = add60 - add30            \n",
    "    d4 = add90 - add60            \n",
    "\n",
    "    # avoid  weird ratios obtained by diving by a difference between -1 and 1 \n",
    "    if (0 <= d1 < 1):\n",
    "        d1 = 1\n",
    "    elif (-1 < d1 < 0):\n",
    "        d1 = -1\n",
    "\n",
    "    r_d1_d2 = d2 / d1\n",
    "\n",
    "    # no condition since it's the control group\n",
    "    bkpnt_dict = {\n",
    "        \"bkpt_date\"         : bkpt_date,\n",
    "        \"bkpt_date_sub30\"   : bkpt_date_sub30,\n",
    "        \"bkpt_date_add30\"   : bkpt_date_add30,\n",
    "        \"bkpt_date_add60\"   : bkpt_date_add60,\n",
    "        \"bkpt_date_add90\"   : bkpt_date_add90,\n",
    "        \"avg_patrons_bkpnt\" : point,\n",
    "        \"avg_patrons_sub30\" : sub30,\n",
    "        \"avg_patrons_add30\" : add30,\n",
    "        \"avg_patrons_add60\" : add60,\n",
    "        \"avg_patrons_add90\" : add90,\n",
    "        \"d1\"                : d1,\n",
    "        \"d2\"                : d2,\n",
    "        \"d3\"                : d3,\n",
    "        \"d4\"                : d4,\n",
    "        \"r_d1_d2\"           : r_d1_d2\n",
    "    }\n",
    "\n",
    "    return bkpnt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`align_breakpoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation pseudo intervention dates on potential control group\n",
    "def align_breakpoints(treated_subject, potential_control_list, patreon_df, youtube_timeseries_df, youtube_meta_df):\n",
    "    \"\"\"\n",
    "    for a treated subject in dict, \n",
    "    get breakpoint dates values of potential control accounts\n",
    "    \"\"\"\n",
    "    # print(f\"\\n matching regions of potential control subjects for treated subject: {treated_subject}....)\")\n",
    "          \n",
    "    # variables declaration\n",
    "    MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "    ROLLING_AVG_WINDOW = 30\n",
    "    potential_control_tuples = []\n",
    "\n",
    "\n",
    "        \n",
    "    treated_subject_row = df_treated[df_treated['patreon_id'] == treated_subject]\n",
    "    bkpt_date_sub30   = treated_subject_row['bkpt_date_sub30'].iloc[0]\n",
    "    bkpt_date         = treated_subject_row['bkpt_date'].iloc[0]\n",
    "    bkpt_date_add30   = treated_subject_row['bkpt_date_add30'].iloc[0]\n",
    "    bkpt_date_add60   = treated_subject_row['bkpt_date_add60'].iloc[0]\n",
    "    bkpt_date_add90   = treated_subject_row['bkpt_date_add90'].iloc[0]\n",
    "    \n",
    "    # print(\"treated_subject_row:\", treated_subject_row)\n",
    "    # print(\"bkpt_date_sub30:\", bkpt_date_sub30)    \n",
    "    \n",
    "    for idx, potential_control in enumerate(tqdm(potential_control_list)):\n",
    "        # print(\"\\t\", idx, potential_control)\n",
    "        treat = 0\n",
    "        \n",
    "\n",
    "        ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "        # restrict to 1 patreon account, sort by date and drop duplicates\n",
    "\n",
    "        # patreon earnings and users\n",
    "        tmp_df_pt = restrict_acct_and_sort_df(patreon_df, 'patreon', potential_control, 'date')\n",
    "\n",
    "        # sanity checks to make sure the treated date range exist in potential_control_df\n",
    "        if (tmp_df_pt[tmp_df_pt['date'] == bkpt_date_sub30].empty) or (tmp_df_pt[tmp_df_pt['date'] == bkpt_date_add90].empty):\n",
    "            # print(\"skip Patreon account as the range dates dont exist\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # youtube videos\n",
    "        tmp_df_yt = restrict_acct_and_sort_df(youtube_timeseries_df, 'patreon_id', potential_control, 'datetime')\n",
    "            \n",
    "\n",
    "        # youtube metadata\n",
    "        tmp_df_yt_meta = restrict_acct_and_sort_df(youtube_meta_df, 'patreon_id', potential_control, 'upload_date')\n",
    "\n",
    "        ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "\n",
    "        # set min and max dates for plots   \n",
    "        date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "        date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "\n",
    "        # if no overlap period between YT and Patreon datasets, skip account\n",
    "        if date_max < date_min:\n",
    "            # print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "            continue\n",
    "\n",
    "        # restrict datasets between min and max dates\n",
    "        tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "        tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "\n",
    "        # align both dataframes since youtube starts once a week\n",
    "        tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "\n",
    "        tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "        tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "        ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "\n",
    "        # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "        ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "        ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "        ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "        ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "        tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "\n",
    "        # reorder columns to have deltas columns next to their respective columns\n",
    "        patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "        ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "\n",
    "        # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "        ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "\n",
    "\n",
    "\n",
    "        ########################## PRINT TITLES ##########################\n",
    "        # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "        # ch_ids = tmp_df_yt['channel'].unique()\n",
    "        # print(f\"\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "\n",
    "        ########################## ALIGN CONTROL ACCOUNT TO TREATED BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "        try:\n",
    "            bkpnt_dict = align_breakpoint_dict(tmp_df_pt, 'patrons_ma', treated_subject_row)\n",
    "\n",
    "        except Exception as e:\n",
    "                print(\"Exception in align_breakpoint_dict function: \", e)\n",
    "                continue            \n",
    "                              \n",
    "                \n",
    "        ################################### CALCULATE DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "\n",
    "        ##### PATREON #####\n",
    "        tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_sub30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date'])]\n",
    "        tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add30'])]\n",
    "        tmp_df_PT_add60 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_add30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add60'])]\n",
    "        tmp_df_PT_add90 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_add60']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add90'])]\n",
    "\n",
    "        # delta patrons\n",
    "        mean_delta_patrons_sub30 = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "        mean_delta_patrons_add30 = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "        mean_delta_patrons_add60 = tmp_df_PT_add60['delta_patrons'].mean()\n",
    "        mean_delta_patrons_add90 = tmp_df_PT_add90['delta_patrons'].mean()\n",
    "\n",
    "        # delta earnings\n",
    "        mean_delta_earnings_sub30 = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "        mean_delta_earnings_add30 = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "        mean_delta_earnings_add60 = tmp_df_PT_add60['delta_earning'].mean()  \n",
    "        mean_delta_earnings_add90 = tmp_df_PT_add90['delta_earning'].mean()  \n",
    "\n",
    "\n",
    "        ##### YOUTUBE TIME SERIES #####\n",
    "        tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date']      )]\n",
    "        tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "        tmp_df_YT_add60 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_add30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add60'])]\n",
    "        tmp_df_YT_add90 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_add60']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add90'])]\n",
    "\n",
    "        # delta videos\n",
    "        mean_delta_videos_sub30 = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "        mean_delta_videos_add30 = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "        mean_delta_videos_add60 = tmp_df_YT_add60['delta_videos'].mean()  \n",
    "        mean_delta_videos_add90 = tmp_df_YT_add90['delta_videos'].mean()  \n",
    "\n",
    "        # delta views\n",
    "        mean_delta_views_sub30 = tmp_df_YT_sub30['delta_views'].mean()\n",
    "        mean_delta_views_add30 = tmp_df_YT_add30['delta_views'].mean()  \n",
    "        mean_delta_views_add60 = tmp_df_YT_add60['delta_views'].mean()  \n",
    "        mean_delta_views_add90 = tmp_df_YT_add90['delta_views'].mean()  \n",
    "\n",
    "        # delta subscriptions\n",
    "        mean_delta_subs_sub30 = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "        mean_delta_subs_add30 = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "        mean_delta_subs_add60 = tmp_df_YT_add60['delta_subs'].mean()  \n",
    "        mean_delta_subs_add90 = tmp_df_YT_add90['delta_subs'].mean()  \n",
    "\n",
    "\n",
    "        ##### YOUTUBE METADATA #####\n",
    "        tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date']      )]\n",
    "        tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "        tmp_df_YT_META_add60 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_add30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add60'])]\n",
    "        tmp_df_YT_META_add90 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_add60']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add90'])]\n",
    "\n",
    "        # durations\n",
    "        mean_duration_sub30 = tmp_df_YT_META_sub30['duration'].mean()\n",
    "        mean_duration_add30 = tmp_df_YT_META_add30['duration'].mean()      \n",
    "        mean_duration_add60 = tmp_df_YT_META_add60['duration'].mean()      \n",
    "        mean_duration_add90 = tmp_df_YT_META_add90['duration'].mean()      \n",
    "\n",
    "        # likes\n",
    "        mean_likes_sub30 = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "        mean_likes_add30 = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "        mean_likes_add60 = tmp_df_YT_META_add60['like_count'].mean()      \n",
    "        mean_likes_add90 = tmp_df_YT_META_add90['like_count'].mean()     \n",
    "\n",
    "\n",
    "        # get youtube channel id name\n",
    "        yt_channel_id = tmp_df_yt['channel'].unique()[0]\n",
    "\n",
    "\n",
    "        potential_control_tuples.append(\n",
    "            (          \n",
    "                potential_control, \n",
    "                yt_channel_id,   \n",
    "                treat,\n",
    "                bkpnt_dict[\"d1\"], \n",
    "                bkpnt_dict[\"d2\"], \n",
    "                bkpnt_dict[\"d3\"], \n",
    "                bkpnt_dict[\"d4\"], \n",
    "\n",
    "                bkpnt_dict[\"r_d1_d2\"],\n",
    "\n",
    "                bkpnt_dict[\"bkpt_date\"], \n",
    "                bkpnt_dict[\"bkpt_date_sub30\"], \n",
    "                bkpnt_dict[\"bkpt_date_add30\"],\n",
    "                bkpnt_dict[\"bkpt_date_add60\"],\n",
    "                bkpnt_dict[\"bkpt_date_add90\"],\n",
    "\n",
    "                bkpnt_dict[\"avg_patrons_bkpnt\"], \n",
    "                bkpnt_dict[\"avg_patrons_sub30\"], \n",
    "                bkpnt_dict[\"avg_patrons_add30\"], \n",
    "                bkpnt_dict[\"avg_patrons_add60\"], \n",
    "                bkpnt_dict[\"avg_patrons_add90\"], \n",
    "\n",
    "                # delta patrons\n",
    "                mean_delta_patrons_sub30,\n",
    "                mean_delta_patrons_add30,\n",
    "                mean_delta_patrons_add60,\n",
    "                mean_delta_patrons_add90,\n",
    "\n",
    "                # delta earnings\n",
    "                mean_delta_earnings_sub30,\n",
    "                mean_delta_earnings_add30,\n",
    "                mean_delta_earnings_add60, \n",
    "                mean_delta_earnings_add90,\n",
    "\n",
    "                # delta videos\n",
    "                mean_delta_videos_sub30,\n",
    "                mean_delta_videos_add30,\n",
    "                mean_delta_videos_add60,\n",
    "                mean_delta_videos_add90,\n",
    "\n",
    "                # delta views\n",
    "                mean_delta_views_sub30,\n",
    "                mean_delta_views_add30,\n",
    "                mean_delta_views_add60,\n",
    "                mean_delta_views_add90,\n",
    "\n",
    "                # delta subscriptions\n",
    "                mean_delta_subs_sub30,\n",
    "                mean_delta_subs_add30,\n",
    "                mean_delta_subs_add60,\n",
    "                mean_delta_subs_add90,\n",
    "\n",
    "                # durations\n",
    "                mean_duration_sub30,\n",
    "                mean_duration_add30,     \n",
    "                mean_duration_add60,    \n",
    "                mean_duration_add90, \n",
    "\n",
    "                # likes\n",
    "                mean_likes_sub30,\n",
    "                mean_likes_add30,  \n",
    "                mean_likes_add60,   \n",
    "                mean_likes_add90\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    df_potential_control = pd.DataFrame(potential_control_tuples, columns = [\n",
    "        'patreon_id',\n",
    "        'yt_channel_id',\n",
    "        'treat',\n",
    "        'd1', \n",
    "        'd2', \n",
    "        'd3',\n",
    "        'd4',\n",
    "\n",
    "        'ratio_d1_d2',\n",
    "        'bkpt_date',     \n",
    "        'bkpt_date_sub30', \n",
    "        'bkpt_date_add30', \n",
    "        'bkpt_date_add60',\n",
    "        'bkpt_date_add90',\n",
    "\n",
    "        'avg_patrons_bkpnt', \n",
    "        'avg_patrons_sub30', \n",
    "        'avg_patrons_add30', \n",
    "        'avg_patrons_add60',\n",
    "        'avg_patrons_add90',\n",
    "\n",
    "        # delta patrons\n",
    "        'mean_delta_patrons_sub30',\n",
    "        'mean_delta_patrons_add30',\n",
    "        'mean_delta_patrons_add60',\n",
    "        'mean_delta_patrons_add90',\n",
    "\n",
    "        # delta earnings\n",
    "        'mean_delta_earnings_sub30',\n",
    "        'mean_delta_earnings_add30',\n",
    "        'mean_delta_earnings_add60',\n",
    "        'mean_delta_earnings_add90',\n",
    "\n",
    "        # delta videos\n",
    "        'mean_delta_videos_sub30',\n",
    "        'mean_delta_videos_add30',\n",
    "        'mean_delta_videos_add60',\n",
    "        'mean_delta_videos_add90',\n",
    "\n",
    "        # delta views\n",
    "        'mean_delta_views_sub30',\n",
    "        'mean_delta_views_add30',\n",
    "        'mean_delta_views_add60',\n",
    "        'mean_delta_views_add90',\n",
    "\n",
    "        # delta subscriptions\n",
    "        'mean_delta_subs_sub30',\n",
    "        'mean_delta_subs_add30',\n",
    "        'mean_delta_subs_add60',\n",
    "        'mean_delta_subs_add90',\n",
    "\n",
    "        # durations\n",
    "        'mean_duration_sub30',\n",
    "        'mean_duration_add30',\n",
    "        'mean_duration_add60',\n",
    "        'mean_duration_add90',\n",
    "\n",
    "        # likes\n",
    "        'mean_likes_sub30',\n",
    "        'mean_likes_add30',\n",
    "        'mean_likes_add60',\n",
    "        'mean_likes_add90'\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Number of Patreon accounts added to the potential_control group:  {len(df_potential_control)}')\n",
    "\n",
    "    df_potential_control['bkpt_date'] = pd.to_datetime(df_potential_control['bkpt_date'])\n",
    "    df_potential_control['bkpt_date_sub30'] = pd.to_datetime(df_potential_control['bkpt_date_sub30'])\n",
    "    df_potential_control['bkpt_date_add30'] = pd.to_datetime(df_potential_control['bkpt_date_add30'])\n",
    "    df_potential_control['bkpt_date_add60'] = pd.to_datetime(df_potential_control['bkpt_date_add60'])\n",
    "    df_potential_control['bkpt_date_add90'] = pd.to_datetime(df_potential_control['bkpt_date_add90'])\n",
    "\n",
    "\n",
    "    df_treat = pd.concat([treated_subject_row, df_potential_control]).reset_index(drop=True)\n",
    "    return df_treat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DF with potential control subjects per treated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DF with treated and potential control accounts (takes about 2h15)\n",
    "exceptions = 0\n",
    "all_treat_and_potential_control_df = pd.DataFrame()\n",
    "\n",
    "print(f'Iterate over {len(dict_potential_matches)} treated accounts...')\n",
    "for idx, (treated_subject, potential_control_list) in enumerate(tqdm(dict_potential_matches.items())):\n",
    "    if idx >= 10:\n",
    "        break\n",
    "    # print(f\"\\nidx: {idx}, treated_subject: {treated_subject}\")\n",
    "    try: \n",
    "        df_treat = align_breakpoints(treated_subject, potential_control_list, df_top_pt_accts, df_yt_timeseries_top_pt, df_yt_metadata_pt_filtered)\n",
    "\n",
    "    except Exception as e: \n",
    "        exceptions += 1\n",
    "        print(\"Exception: \", e)\n",
    "        continue\n",
    "        \n",
    "    df_treat['treated_patreon_id'] = treated_subject\n",
    "    all_treat_and_potential_control_df = pd.concat([all_treat_and_potential_control_df, df_treat])\n",
    "    \n",
    "print(\"total number of exceptions: \", exceptions)\n",
    "\n",
    "# move treated_patreon_id column at the beginning of the df\n",
    "first_column = all_treat_and_potential_control_df.pop('treated_patreon_id')  \n",
    "all_treat_and_potential_control_df.insert(0, 'treated_patreon_id', first_column)\n",
    "\n",
    "print(\"all_treat_and_potential_control_df: \")\n",
    "all_treat_and_potential_control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save \"all_treat_and_potential_control_df\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"all_treat_and_potential_control_df_060522.tsv.gz\"\n",
    "# all_treat_and_potential_control_df_060522.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save \"all_treat_and_potential_control_df\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"all_treat_and_potential_control_df.tsv.gz\"\n",
    "# all_treat_and_potential_control_df.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}all_treat_and_potential_control_df.tsv.gz\n",
    "!ls -lh {LOCAL_DATA_FOLDER}all_treat_and_potential_control_df_060522.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - CHANGE FILE NAME\n",
    "all_treat_and_potential_control_df = pd.read_csv(LOCAL_DATA_FOLDER+\"all_treat_and_potential_control_df_060522.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "\n",
    "print(f\"Number of \\'treated subject + corresponding potential subjects\\' groups: {all_treat_and_potential_control_df['treated_patreon_id'].nunique()}\")\n",
    "all_treat_and_potential_control_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', 150, 'display.min_rows', 150):\n",
    "#     display(all_treat_and_potential_control_df) #need display to show the dataframe when using with in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verif that all accounts are in top patreons list\n",
    "all_treat_and_potential_control_df[~all_treat_and_potential_control_df['patreon_id'].isin(top_patreons)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Propensity score model\n",
    "- Use logistic regression to estimate propensity scores for all points in the dataset. \n",
    "-  Use statsmodels to fit the logistic regression model and apply it to each data point to obtain propensity scores.\n",
    "\n",
    "The propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Treatment:**\n",
    "    - Increase of Patrons at breakpoint - converted to a binary variable as follows:\n",
    "        - `treat` = 1 (treatment group), if number of patrons increase ratio at breakpoint > threshold\n",
    "        - `treat` = 0 (control group), if number of patrons increase linearly (increase ratio btw 1 and 2)\n",
    "        \n",
    "- **Outcome**\n",
    "    - `mean_delta_videos_after`: YouTube delta views (post-treatment)\n",
    "    \n",
    "- **Observed covariates:**\n",
    "    - `avg_patrons_sub30`: Average number of patrons 30 days before breakpoint\n",
    "    - `mean_delta_videos_befor`: YouTube delta videos (pre-treatment) \n",
    "    - `mean_delta_views_befor`:  YouTube delta views (pre-treatment) \n",
    "    - `mean_delta_subs_befor`:   YouTube delta subs (pre-treatment) \n",
    "    - `mean_duration_befor`:     YouTube video duration (pre-treatment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\n",
    "    'd1', 'd2', 'd3', 'd4',\n",
    "    'ratio_d1_d2',\n",
    "    'avg_patrons_sub30', 'avg_patrons_bkpnt', 'avg_patrons_add30', 'avg_patrons_add60', 'avg_patrons_add90',\n",
    "    'mean_delta_patrons_sub30', 'mean_delta_patrons_add30', 'mean_delta_patrons_add60', 'mean_delta_patrons_add90',\n",
    "    'mean_delta_earnings_sub30', 'mean_delta_earnings_add30', 'mean_delta_earnings_add60','mean_delta_earnings_add90',\n",
    "    'mean_delta_videos_sub30', 'mean_delta_videos_add30', 'mean_delta_videos_add60', 'mean_delta_videos_add90',\n",
    "    'mean_delta_views_sub30','mean_delta_views_add30', 'mean_delta_views_add60', 'mean_delta_views_add90',\n",
    "    'mean_delta_subs_sub30', 'mean_delta_subs_add30', 'mean_delta_subs_add60', 'mean_delta_subs_add90',\n",
    "    'mean_duration_sub30', 'mean_duration_add30', 'mean_duration_add60', 'mean_duration_add90',\n",
    "    'mean_likes_sub30', 'mean_likes_add30', 'mean_likes_add60', 'mean_likes_add90'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(df, features_list):\n",
    "    # standardize the continuous features\n",
    "    df_std = df.copy()\n",
    "    for feature in features_list:\n",
    "        df_std[feature] = (df_std[feature] - df_std[feature].mean())/df_std[feature].std()\n",
    "    \n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_weight_matching(df):\n",
    "    # Separate the treatment and control groups\n",
    "    treatment_df = df[df['treat'] == 1]\n",
    "    control_df   = df[df['treat'] == 0]\n",
    "\n",
    "    # Create an empty undirected graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Loop through all the pairs of instances\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "        for control_id, control_row in control_df.iterrows():\n",
    "\n",
    "            # Calculate the similarity \n",
    "            similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                        treatment_row['Propensity_score'])\n",
    "\n",
    "            # Add an edge between the two instances weighted by the similarity between them\n",
    "            G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "    # Generate and return the maximum weight matching on the generated graph\n",
    "    matching = nx.max_weight_matching(G)\n",
    "    return matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_treat_and_potential_control_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "all_treat_and_potential_control_df.groupby('treated_patreon_id')['patreon_id'].count()\n",
    "treated_subjects = all_treat_and_potential_control_df.treated_patreon_id.unique()\n",
    "\n",
    "\n",
    "# match pairs\n",
    "exceptions = 0\n",
    "all_pairs_df = pd.DataFrame()\n",
    "match_pairs_dict = {} # treatment: control\n",
    "display_cols = ['patreon_id', 'treat', 'Propensity_score', 'd1', 'd2', 'ratio_d1_d2', 'bkpt_date', 'avg_patrons_sub30', 'mean_delta_videos_sub30', 'mean_delta_views_sub30', 'mean_delta_subs_sub30', 'mean_duration_sub30']\n",
    "non_na_cols  = ['patreon_id', 'treat', 'd1', 'd2', 'ratio_d1_d2', 'bkpt_date', 'avg_patrons_sub30', 'mean_delta_videos_sub30', 'mean_delta_views_sub30', 'mean_delta_subs_sub30']\n",
    "\n",
    "print(f'Iterate over {len(treated_subjects)} treated accounts...')\n",
    "# for idx, (treated_subject, potential_control_list) in enumerate(tqdm(dict_potential_matches.items())):\n",
    "for idx, treated_subject in enumerate(tqdm(treated_subjects)):\n",
    "    # print(f\"\\nidx: {idx}, treated_subject: {treated_subject}\")\n",
    "    \n",
    "    # restrict dataframe to this treated account\n",
    "    df_treat = all_treat_and_potential_control_df[all_treat_and_potential_control_df['treated_patreon_id'] == treated_subject]\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        # df_treat = align_breakpoints(treated_subject, potential_control_list, df_top_pt_accts, df_yt_timeseries_top_pt, df_yt_metadata_pt_filtered)\n",
    "\n",
    "        # drop rows only from control accounts which have NA values in the columns of interest\n",
    "        df_treat = df_treat.drop(df_treat[(df_treat['treat'] == 0) & (df_treat[non_na_cols].isna().any(axis=1))].index)\n",
    "        df_treat = df_treat.reset_index(drop=True)\n",
    "\n",
    "        df_treat_std = standardize_features(df_treat, continuous_features)\n",
    "\n",
    "        # propensity score matching\n",
    "        mod = smf.logit(formula='treat ~  avg_patrons_sub30 + mean_delta_patrons_sub30 + mean_delta_videos_sub30 + mean_delta_views_sub30 + mean_delta_subs_sub30', data=df_treat_std)\n",
    "        # mod = smf.logit(formula='treat ~  d1', data=df_treat_std)\n",
    "        res = mod.fit(disp=0, warn_convergence=True)\n",
    "\n",
    "        df_treat['Propensity_score'] = res.predict()\n",
    "        df_treat_std['Propensity_score'] = res.predict() # Extract the estimated propensity scores\n",
    "        # print(res.summary())\n",
    "\n",
    "        matching_pair = maximum_weight_matching(df_treat)\n",
    "        matched_list = [i[0] for i in list(matching_pair)] + [i[1] for i in list(matching_pair)]\n",
    "        matched_pair_df = df_treat.iloc[matched_list]\n",
    "\n",
    "        # print(\"\\n matched_pair_df: \")\n",
    "        # display(matched_pair_df[display_cols])\n",
    "\n",
    "        match_pairs_dict[matched_pair_df[matched_pair_df.treat == 1].patreon_id.iloc[0]] = matched_pair_df[matched_pair_df.treat == 0].patreon_id.iloc[0]\n",
    "\n",
    "        all_pairs_df = pd.concat([all_pairs_df, matched_pair_df])\n",
    "        # .reset_index(drop=True)\n",
    "        # display(all_pairs_df)\n",
    "    except Exception as e: \n",
    "        exceptions += 1\n",
    "        # print(f\"\\nidx: {idx}, treated_subject: {treated_subject}\")\n",
    "        print(\"Exception: \", e)\n",
    "        continue\n",
    "\n",
    "    # print(\"\\n\\n ------------------------------------------------------------------------------------------------------------------------------------------------------ \\n\\n\")\n",
    "\n",
    "print(\"\\nnumber of exceptions: \", exceptions)\n",
    "print(\"\\n\\n Dataframe with all pairs: \")\n",
    "all_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_df = all_pairs_df.reset_index(drop=True)\n",
    "all_pairs_df['bkpt_date'] = pd.to_datetime(all_pairs_df['bkpt_date'])\n",
    "all_pairs_df['bkpt_date_sub30'] = pd.to_datetime(all_pairs_df['bkpt_date_sub30'])\n",
    "all_pairs_df['bkpt_date_add30'] = pd.to_datetime(all_pairs_df['bkpt_date_add30'])\n",
    "all_pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Compute difference between means of regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute increases between means of all 4 regions\n",
    "all_pairs_df['diff_delta_patrons_d1'] = all_pairs_df['mean_delta_patrons_add30'] - all_pairs_df['mean_delta_patrons_sub30']\n",
    "all_pairs_df['diff_delta_patrons_d2'] = all_pairs_df['mean_delta_patrons_add60'] - all_pairs_df['mean_delta_patrons_sub30']\n",
    "all_pairs_df['diff_delta_patrons_d3'] = all_pairs_df['mean_delta_patrons_add90'] - all_pairs_df['mean_delta_patrons_sub30']\n",
    "\n",
    "all_pairs_df['diff_delta_earnings_d1'] = all_pairs_df['mean_delta_earnings_add30'] - all_pairs_df['mean_delta_earnings_sub30']\n",
    "all_pairs_df['diff_delta_earnings_d2'] = all_pairs_df['mean_delta_earnings_add60'] - all_pairs_df['mean_delta_earnings_sub30']\n",
    "all_pairs_df['diff_delta_earnings_d3'] = all_pairs_df['mean_delta_earnings_add90'] - all_pairs_df['mean_delta_earnings_sub30']\n",
    "\n",
    "all_pairs_df['diff_delta_videos_d1'] = all_pairs_df['mean_delta_videos_add30'] - all_pairs_df['mean_delta_videos_sub30']\n",
    "all_pairs_df['diff_delta_videos_d2'] = all_pairs_df['mean_delta_videos_add60'] - all_pairs_df['mean_delta_videos_sub30']\n",
    "all_pairs_df['diff_delta_videos_d3'] = all_pairs_df['mean_delta_videos_add90'] - all_pairs_df['mean_delta_videos_sub30']\n",
    "\n",
    "all_pairs_df['diff_delta_views_d1'] = all_pairs_df['mean_delta_views_add30'] - all_pairs_df['mean_delta_views_sub30']\n",
    "all_pairs_df['diff_delta_views_d2'] = all_pairs_df['mean_delta_views_add60'] - all_pairs_df['mean_delta_views_sub30']\n",
    "all_pairs_df['diff_delta_views_d3'] = all_pairs_df['mean_delta_views_add90'] - all_pairs_df['mean_delta_views_sub30']\n",
    "\n",
    "all_pairs_df['diff_delta_subs_d1'] = all_pairs_df['mean_delta_subs_add30'] - all_pairs_df['mean_delta_subs_sub30']\n",
    "all_pairs_df['diff_delta_subs_d2'] = all_pairs_df['mean_delta_subs_add60'] - all_pairs_df['mean_delta_subs_sub30']\n",
    "all_pairs_df['diff_delta_subs_d3'] = all_pairs_df['mean_delta_subs_add90'] - all_pairs_df['mean_delta_subs_sub30']\n",
    "\n",
    "all_pairs_df['diff_duration_d1'] = all_pairs_df['mean_duration_add30'] - all_pairs_df['mean_duration_sub30']\n",
    "all_pairs_df['diff_duration_d2'] = all_pairs_df['mean_duration_add60'] - all_pairs_df['mean_duration_sub30']\n",
    "all_pairs_df['diff_duration_d3'] = all_pairs_df['mean_duration_add90'] - all_pairs_df['mean_duration_sub30']\n",
    "\n",
    "all_pairs_df['diff_likes_d1'] = all_pairs_df['mean_likes_add30'] - all_pairs_df['mean_likes_sub30']\n",
    "all_pairs_df['diff_likes_d2'] = all_pairs_df['mean_likes_add60'] - all_pairs_df['mean_likes_sub30']\n",
    "all_pairs_df['diff_likes_d3'] = all_pairs_df['mean_likes_add90'] - all_pairs_df['mean_likes_sub30']\n",
    "all_pairs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pairs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 70)\n",
    "columns_order = [\n",
    "            'treated_patreon_id', 'patreon_id', 'yt_channel_id', 'treat','Propensity_score',\n",
    "            'd1', 'd2', 'd3', 'd4',\n",
    "            'ratio_d1_d2',\n",
    "            'bkpt_date_sub30', 'bkpt_date', 'bkpt_date_add30', 'bkpt_date_add60', 'bkpt_date_add90',\n",
    "            'avg_patrons_sub30', 'avg_patrons_bkpnt', 'avg_patrons_add30', 'avg_patrons_add60', 'avg_patrons_add90',\n",
    "            'mean_delta_patrons_sub30', 'mean_delta_patrons_add30', 'mean_delta_patrons_add60', 'mean_delta_patrons_add90',\n",
    "            'mean_delta_earnings_sub30', 'mean_delta_earnings_add30', 'mean_delta_earnings_add60','mean_delta_earnings_add90',\n",
    "            'mean_delta_videos_sub30', 'mean_delta_videos_add30', 'mean_delta_videos_add60', 'mean_delta_videos_add90',\n",
    "            'mean_delta_views_sub30','mean_delta_views_add30', 'mean_delta_views_add60', 'mean_delta_views_add90',\n",
    "            'mean_delta_subs_sub30', 'mean_delta_subs_add30', 'mean_delta_subs_add60', 'mean_delta_subs_add90',\n",
    "            'mean_duration_sub30', 'mean_duration_add30', 'mean_duration_add60', 'mean_duration_add90',\n",
    "            'mean_likes_sub30', 'mean_likes_add30', 'mean_likes_add60', 'mean_likes_add90',\n",
    "            'diff_delta_patrons_d1', 'diff_delta_patrons_d2', 'diff_delta_patrons_d3',\n",
    "            'diff_delta_earnings_d1', 'diff_delta_earnings_d2','diff_delta_earnings_d3',\n",
    "            'diff_delta_videos_d1','diff_delta_videos_d2', 'diff_delta_videos_d3',\n",
    "            'diff_delta_views_d1','diff_delta_views_d2', 'diff_delta_views_d3',\n",
    "            'diff_delta_subs_d1','diff_delta_subs_d2', 'diff_delta_subs_d3',\n",
    "            'diff_duration_d1','diff_duration_d2', 'diff_duration_d3',\n",
    "            'diff_likes_d1','diff_likes_d2', 'diff_likes_d3'\n",
    "]\n",
    "\n",
    "# reorder columns\n",
    "all_pairs_df = all_pairs_df[columns_order]\n",
    "all_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at weird thing where treated accts, but neg diff delta patrons\n",
    "# all_pairs_df[(all_pairs_df['treat'] == 1) & (all_pairs_df['diff_delta_patrons'] < 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter pairs according to increase of the treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TREATED_INCR_RATIO_THRESH = 3\n",
    "print(f\"Treated increase threshold: ratio > {TREATED_INCR_RATIO_THRESH}\")\n",
    "\n",
    "\n",
    "# filter pairs according to increase of the treated\n",
    "filtered_list = all_pairs_df[(all_pairs_df['treat'] == 1) & (all_pairs_df['ratio_d1_d2'] > TREATED_INCR_RATIO_THRESH)].treated_patreon_id\n",
    "print(f\"Number of treated accounts:              {len(filtered_list):>3}\")\n",
    "\n",
    "all_pairs_df_filt = all_pairs_df[all_pairs_df['treated_patreon_id'].isin(filtered_list)]\n",
    "print(f\"Number of corresponding matched pairs:   {len(all_pairs_df_filt):>3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot diffs of delta means 1, 2 and 3 months after breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15,8), sharex=False, sharey='row')\n",
    "   \n",
    "sns.stripplot(x=\"treat\", y='diff_delta_patrons_d1', data=all_pairs_df_filt, ax=axs[0,0])\n",
    "axs[0,0].set(title=f'Distribution of diff_delta_patrons_d1')\n",
    "axs[0,0].set_ylabel('diff_delta_patrons_d1')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_patrons_d2', data=all_pairs_df_filt, ax=axs[0,1])\n",
    "axs[0,1].set(title=f'Distribution of diff_delta_patrons_d1')\n",
    "axs[0,1].set_ylabel('diff_delta_patrons_d2')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_patrons_d3', data=all_pairs_df_filt, ax=axs[0,2])\n",
    "axs[0,2].set(title=f'Distribution of diff_delta_patrons_d1')\n",
    "axs[0,2].set_ylabel('diff_delta_patrons_d3')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_earnings_d1', data=all_pairs_df_filt, ax=axs[1,0])\n",
    "axs[1,0].set(title=f'Distribution of diff_delta_earnings_d1')\n",
    "axs[1,0].set_ylabel('diff_delta_earnings_d1')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_earnings_d2', data=all_pairs_df_filt, ax=axs[1,1])\n",
    "axs[1,1].set(title=f'Distribution of diff_delta_earnings_d2')\n",
    "axs[1,1].set_ylabel('diff_delta_earnings_d2')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_earnings_d3', data=all_pairs_df_filt, ax=axs[1,2])\n",
    "axs[1,2].set(title=f'Distribution of diff_delta_earnings_d3')\n",
    "axs[1,2].set_ylabel('diff_delta_earnings_d3')\n",
    "\n",
    "plt.suptitle(\"Sanity check with patreons variables\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15,10), sharex=False, sharey='row')\n",
    "\n",
    "# videos\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_videos_d1', data=all_pairs_df_filt, ax=axs[0,0])\n",
    "axs[0,0].set(title=f'Distribution of diff_delta_videos_d1')\n",
    "axs[0,0].set_ylabel('diff_delta_videos_d1')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_videos_d2', data=all_pairs_df_filt, ax=axs[0,1])\n",
    "axs[0,1].set(title=f'Distribution of diff_delta_videos_d2')\n",
    "axs[0,1].set_ylabel('diff_delta_videos_d2')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_videos_d3', data=all_pairs_df_filt, ax=axs[0,2])\n",
    "axs[0,2].set(title=f'Distribution of diff_delta_videos_d3')\n",
    "axs[0,2].set_ylabel('diff_delta_videos_d3')\n",
    "\n",
    "\n",
    "# views\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_views_d1', data=all_pairs_df_filt, ax=axs[1,0])\n",
    "axs[1,0].set(title=f'Distribution of diff_delta_views_d1')\n",
    "axs[1,0].set_ylabel('diff_delta_views_d1')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_views_d2', data=all_pairs_df_filt, ax=axs[1,1])\n",
    "axs[1,1].set(title=f'Distribution of diff_delta_views_d2')\n",
    "axs[1,1].set_ylabel('diff_delta_views_d2')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_views_d3', data=all_pairs_df_filt, ax=axs[1,2])\n",
    "axs[1,2].set(title=f'Distribution of diff_delta_views_d3')\n",
    "axs[1,2].set_ylabel('diff_delta_views_d3')\n",
    "\n",
    "\n",
    "# subs\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_subs_d1', data=all_pairs_df_filt, ax=axs[2,0])\n",
    "axs[2,0].set(title=f'Distribution of diff_delta_subs_d1')\n",
    "axs[2,0].set_ylabel('diff_delta_subs_d1')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_subs_d3', data=all_pairs_df_filt, ax=axs[2,1])\n",
    "axs[2,1].set(title=f'Distribution of diff_delta_subs_d3')\n",
    "axs[2,1].set_ylabel('diff_delta_subs_d3')\n",
    "\n",
    "sns.stripplot(x=\"treat\", y='diff_delta_subs_d3', data=all_pairs_df_filt, ax=axs[2,2])\n",
    "axs[2,2].set(title=f'Distribution of diff_delta_subs_d3')\n",
    "axs[2,2].set_ylabel('diff_delta_subs_d3')\n",
    "\n",
    "\n",
    "plt.suptitle(\"Tests\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = all_pairs_df_filt.loc[all_pairs_df_filt['treat'] == 1] \n",
    "control = all_pairs_df_filt.loc[all_pairs_df_filt['treat'] == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['diff_delta_videos_d1', 'diff_delta_views_d1', 'diff_delta_subs_d1']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.distplot(treated[col], hist=True, label='treated', ax=ax)\n",
    "    sns.distplot(control[col], hist=True, label='control', ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(col)\n",
    "    # ax.set(yscale=\"log\")\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_plot(ax, x, y, title, x_axis_label=\"default x\", y_axis_label=\"default y\", color=\"#1f77b4\", alpha=1):\n",
    "#     ax.plot(x, y, color, alpha)\n",
    "#     ax.set(title=title)\n",
    "#     ax.set_xlabel(x_axis_label)    \n",
    "#     ax.set_ylabel(y_axis_label)    \n",
    "\n",
    "# custom_plot(axs[0,0], ts_pt_df['date'], ts_pt_df['patrons'], alpha=0.2)\n",
    "# custom_plot(axs[0,0], ts_pt_df['date'], ts_pt_df['patrons_ravg'], \"Number of patrons\", y_axis_label=\"# Patrons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter accounts that match selected Patreon ids\n",
    "df_yt_metadata_pt_filtered = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(all_pairs_df['patreon_id'])].copy()\n",
    "print(f'Filter accounts that match selected Patreon ids: {len(df_yt_metadata_pt_filtered):,} ({len(df_yt_metadata_pt_filtered)/len(df_yt_metadata_pt):.1%} of videos containing a PT accounts) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt_filtered['crawl_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['crawl_date'])\n",
    "df_yt_metadata_pt_filtered['upload_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['upload_date'])\n",
    "df_yt_metadata_pt_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pairs_df = all_pairs_df.sort_values('ratio', ascending=False)\n",
    "all_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare Patreon and YouTube timeseries + YouTube metadata\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# WEEK_OFFSET = pd.DateOffset(weeks=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# variables for Granger Tests\n",
    "MAXLAG = 2\n",
    "granger_dict = {} # dictionary with  keys (cause --> effect) and values with list of corresponding patreon account(s)\n",
    "not_granger = []\n",
    "YT_variables = ['yt_delta_videos', 'yt_delta_views', 'yt_delta_subs']\n",
    "PT_variables = ['pt_delta_patrons']\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in tqdm(all_pairs_df.iterrows(), total=all_pairs_df.shape[0]):\n",
    "    patreon = row['patreon_id']\n",
    "    \n",
    "    fig, axs = plt.subplots(7, 4, figsize=(26, 10), sharey=False, sharex=False)\n",
    "        \n",
    "\n",
    "    \n",
    "    ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = df_top_pt_accts[df_top_pt_accts['patreon'] == patreon].copy()  \n",
    "    tmp_df_pt = tmp_df_pt.sort_values(by=['date'])\n",
    "    tmp_df_pt = tmp_df_pt.drop_duplicates()\n",
    "    # print(\"\\ntmp_df_pt:\")\n",
    "    # display(tmp_df_pt)\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon].copy()\n",
    "    tmp_df_yt = tmp_df_yt.sort_values(by=['datetime'])\n",
    "    \n",
    "    # youtube metadata\n",
    "    tmp_df_yt_meta = df_yt_metadata_pt_filtered[df_yt_metadata_pt_filtered['patreon_id'] == patreon].copy()   \n",
    "    tmp_df_yt_meta = tmp_df_yt_meta.sort_values('upload_date')\n",
    "    # tmp_df_yt_meta['upload_date'] = pd.to_datetime(tmp_df_yt_meta['upload_date'])\n",
    "    \n",
    "    # replace dates that were collected after 23:00 to their next day, and remove hour\n",
    "    tmp_df_yt['datetime_original'] = tmp_df_yt['datetime']\n",
    "    tmp_df_yt['datetime'] = tmp_df_yt['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "    \n",
    "    # remove hours and convert to datetime type\n",
    "    tmp_df_yt['datetime'] = pd.to_datetime(tmp_df_yt['datetime'].dt.date)\n",
    "    \n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    ch_ids = tmp_df_yt['channel'].unique()\n",
    "    print(f\"\\n\\n\\n\\033[1m {idx+1}: {patreon[12:]} (treat = {row['treat']})\\033[0m\")\n",
    "    print(f\"https://www.{patreon}\")\n",
    "    print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    for ch_id in ch_ids:\n",
    "        print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "   \n",
    "    print(f'\\nYouTube Metadata: ')\n",
    "    \n",
    "    if not (tmp_df_yt_meta.empty):\n",
    "        print('• YT videos were uploaded between {} and {}'.format(tmp_df_yt_meta['upload_date'].min().strftime('%B %d, %Y'),\n",
    "                                                                 tmp_df_yt_meta['upload_date'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "        print('• YT metadata was crawled between {} and {}'.format(tmp_df_yt_meta['crawl_date'].min().strftime('%B %d, %Y'),\n",
    "                                                                 tmp_df_yt_meta['crawl_date'].max().strftime('%B %d, %Y')))\n",
    "    else:\n",
    "        print('• No metadata available for this acount')\n",
    "\n",
    "    \n",
    "    ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "    # set min and max dates for plots   \n",
    "    date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "    date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "    if date_max < date_min:\n",
    "        print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "        continue\n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "    tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    tmp_df_yt_meta = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min) & (tmp_df_yt_meta['upload_date'] <= date_max)]\n",
    "    \n",
    "    # align both dataframes since youtube starts once a week\n",
    "    tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "              \n",
    "    ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "    # breakpoint_date = find_breakpoint_v2(tmp_df_pt, 'patrons_ma')\n",
    "    print(\"Breakpoint date: \", breakpoint_date.date())\n",
    "\n",
    "    # check that dates prior and after breakpoint exist\n",
    "    if not (((breakpoint_date - 1*MONTH_OFFSET)) in ts_pt_df.index and ((breakpoint_date + 1*MONTH_OFFSET) in ts_pt_df.index)):\n",
    "        print(f\"ERROR: Breakpoint too close to edge of patreon time series or missing data\\n\")\n",
    "        plt.figure().clear(); plt.close(); plt.cla(); plt.clf(); plt.show()\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    \n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "\n",
    "    \n",
    "    r = row['ratio']\n",
    "\n",
    "    print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "    print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "    print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "    print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    \n",
    "    print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "    print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    \n",
    "    print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "    print(f\"• Ratio between 2 increases:                            d2/d1  = {r:.2f}\")\n",
    "    print(f\"• Percentage increase:                              d2/d1*100  = {r:>+.0%}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ################################### CALCULATE DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "    \n",
    "    ##### PATREON #####\n",
    "    tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpt_date_sub30) & (ts_pt_weekly_avg_df_float64.index <= bkpt_date)]\n",
    "    tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpt_date) & (ts_pt_weekly_avg_df_float64.index <= bkpt_date_add30)]\n",
    "\n",
    "    # delta patrons\n",
    "    mean_delta_patrons_befor = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "    mean_delta_patrons_after = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "        \n",
    "    # delta earnings\n",
    "    mean_delta_earnings_befor = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "    mean_delta_earnings_after = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "\n",
    "    \n",
    "    ##### YOUTUBE TIME SERIES #####\n",
    "    tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpt_date_sub30) & (tmp_df_yt['datetime'] <= bkpt_date      )]\n",
    "    tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpt_date      ) & (tmp_df_yt['datetime'] <= bkpt_date_add30)]\n",
    "    \n",
    "    # delta videos\n",
    "    mean_delta_videos_befor = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "    mean_delta_videos_after = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "\n",
    "    # delta views\n",
    "    mean_delta_views_befor = tmp_df_YT_sub30['delta_views'].mean()\n",
    "    mean_delta_views_after = tmp_df_YT_add30['delta_views'].mean()  \n",
    "\n",
    "    # delta subscriptions\n",
    "    mean_delta_subs_befor = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "    mean_delta_subs_after = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "\n",
    "    \n",
    "    ##### YOUTUBE METADATA #####\n",
    "    tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpt_date_sub30) & (tmp_df_yt_meta['upload_date'] <= bkpt_date      )]\n",
    "    tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpt_date      ) & (tmp_df_yt_meta['upload_date'] <= bkpt_date_add30)]\n",
    "        \n",
    "    # durations\n",
    "    mean_duration_befor = tmp_df_YT_META_sub30['duration'].mean()\n",
    "    mean_duration_after = tmp_df_YT_META_add30['duration'].mean()      \n",
    "        \n",
    "    # likes\n",
    "    mean_likes_befor = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "    mean_likes_after = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "        \n",
    "    \n",
    "    # plot dots in the middle of region for the region means   \n",
    "    axs[0,2].plot(tmp_df_PT_sub30.index.mean(), mean_delta_patrons_befor, marker='o', color='green', markersize=15)\n",
    "    axs[0,2].plot(tmp_df_PT_add30.index.mean(), mean_delta_patrons_after, marker='o', color='orange', markersize=15)\n",
    "    axs[1,2].plot(tmp_df_PT_sub30.index.mean(), mean_delta_earnings_befor, marker='o', color='green', markersize=15)\n",
    "    axs[1,2].plot(tmp_df_PT_add30.index.mean(), mean_delta_earnings_after, marker='o', color='orange', markersize=15)\n",
    "    axs[2,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_videos_befor, marker='o', color='green', markersize=15)\n",
    "    axs[2,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_videos_after, marker='o', color='orange', markersize=15)\n",
    "    axs[3,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_views_befor, marker='o', color='green', markersize=15)\n",
    "    axs[3,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_views_after, marker='o', color='orange', markersize=15)  \n",
    "    axs[4,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_subs_befor, marker='o', color='green', markersize=15)\n",
    "    axs[4,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_subs_after, marker='o', color='orange', markersize=15)\n",
    "    \n",
    "    # sometimes there is no value at all for this period of time in YT meta --> error when plotting\n",
    "    if not (tmp_df_YT_META_sub30.empty or tmp_df_YT_META_add30.empty):\n",
    "        axs[5,2].plot(tmp_df_YT_META_sub30['upload_date'].mean(), mean_duration_befor, marker='o', color='green', markersize=15)\n",
    "        axs[5,2].plot(tmp_df_YT_META_add30['upload_date'].mean(), mean_duration_after, marker='o', color='orange', markersize=15)  \n",
    "        axs[6,2].plot(tmp_df_YT_META_sub30['upload_date'].mean(), mean_likes_befor, marker='o', color='green', markersize=15)\n",
    "        axs[6,2].plot(tmp_df_YT_META_add30['upload_date'].mean(), mean_likes_after, marker='o', color='orange', markersize=15)  \n",
    "\n",
    "    \n",
    "    # plot horizontal lines for means\n",
    "    mean_befor_list = [mean_delta_patrons_befor, mean_delta_earnings_befor, mean_delta_videos_befor, mean_delta_views_befor, mean_delta_subs_befor, mean_duration_befor, mean_likes_befor]\n",
    "    mean_afer_list = [mean_delta_patrons_after, mean_delta_earnings_after, mean_delta_videos_after, mean_delta_views_after, mean_delta_subs_after, mean_duration_after, mean_likes_after]\n",
    "       \n",
    "    for idx, mean in enumerate(mean_befor_list):\n",
    "            if not math.isnan(mean):\n",
    "                axs[idx,2].hlines(y=mean, xmin=bkpt_date_sub30, xmax=bkpt_date      , linewidth=2, linestyle='--', color='green')\n",
    "\n",
    "    for idx, mean in enumerate(mean_afer_list):\n",
    "            if not math.isnan(mean):\n",
    "                axs[idx,2].hlines(y=mean, xmin=bkpt_date,       xmax=bkpt_date_add30, linewidth=2, linestyle='--', color='orange')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    ################################### ZOOM OUT PLOTS ###################################\n",
    "    \n",
    "    # number of patrons (delta)\n",
    "    axs[0,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,0].set(title=\"Delta patrons per week\")\n",
    "    axs[0,0].set_ylabel(\"Δ Patrons\")    \n",
    "    color_neg_pos(axs[0,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'])\n",
    "\n",
    "    # number of patrons (cumulative)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons'], alpha=0.2)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons_ma'])\n",
    "    axs[0,1].set(title=\"Number of patrons\")\n",
    "    axs[0,1].set_ylabel(\"# Patrons\")\n",
    "\n",
    "    # patreon earnings (delta)\n",
    "    axs[1,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,0].set(title=\"Patreon delta earnings per week\")\n",
    "    axs[1,0].set_ylabel(\"Δ Earnings\") \n",
    "    color_neg_pos(axs[1,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'])\n",
    "\n",
    "    # patreon earnings (cumulative)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning'], alpha=0.2)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning_ma'], color='royalblue')\n",
    "    axs[1,1].set(title=\"Patreon earnings per month\")\n",
    "    axs[1,1].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # youtube videos (delta)\n",
    "    axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,0].set(title=\"YouTube delta videos per week\")\n",
    "    axs[2,0].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,0], tmp_df_yt['datetime'], tmp_df_yt['delta_videos'])\n",
    "\n",
    "    # youtube videos (cumulative)\n",
    "    axs[2,1].plot(tmp_df_yt['datetime'], tmp_df_yt['videos'], 'r')\n",
    "    axs[2,1].set(title=\"YouTube cumulative videos\")\n",
    "    axs[2,1].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # youtube views (delta)\n",
    "    axs[3,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,0].set(title=\"YouTube delta views per week\")\n",
    "    axs[3,0].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,0], tmp_df_yt['datetime'], tmp_df_yt['delta_views'])\n",
    "\n",
    "    # youtube views (cumulative)\n",
    "    axs[3,1].plot(tmp_df_yt['datetime'], tmp_df_yt['views'], 'g')\n",
    "    axs[3,1].set(title=\"YouTube cumulative views\")\n",
    "    axs[3,1].set_ylabel(\"# Views\")\n",
    "\n",
    "    # youtube subs (delta)\n",
    "    axs[4,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,0].set(title=\"YouTube delta subscriptions per week\")\n",
    "    axs[4,0].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,0], tmp_df_yt['datetime'], tmp_df_yt['delta_subs'])\n",
    "\n",
    "    # youtube subs (cumulative)\n",
    "    axs[4,1].plot(tmp_df_yt['datetime'], tmp_df_yt['subs'], 'm')\n",
    "    axs[4,1].set(title=\"YouTube cumulative subscriptions\")\n",
    "    axs[4,1].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,0].set(title=\"YouTube videos durations\")\n",
    "    axs[5,0].set_ylabel(\"Duration\")\n",
    "    \n",
    "    \n",
    "    # youtube likes at crawl date\n",
    "    axs[6,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,0].set(title=\"YouTube likes (plotted against upload date)\")\n",
    "    axs[6,0].set_ylabel(\"Likes\")\n",
    "    \n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = breakpoint_date - (2 * MONTH_OFFSET)\n",
    "    date_max_zoom = breakpoint_date + (2 * MONTH_OFFSET)\n",
    "            \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_meta_zoomed = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min_zoom) & (tmp_df_yt_meta['upload_date'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "    \n",
    "   ################################### ZOOM IN PLOTS  ###################################\n",
    "\n",
    "    # zoomed in patron numbers (delta)\n",
    "    axs[0,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', alpha=0.3)\n",
    "    axs[0,2].set(title=\"Delta patrons per week\")\n",
    "    axs[0,2].set_ylabel(\"Δ Patrons\")\n",
    "    color_neg_pos(axs[0,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'])\n",
    "    \n",
    "    # zoomed in patron numbers (cumulative)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons'], alpha=0.2)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons_ma'])\n",
    "    axs[0,3].set(title=\"Number of patrons (zoomed in)\")\n",
    "    axs[0,3].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "    # zoomed in patron earnings (delta)\n",
    "    axs[1,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', alpha=0.3)\n",
    "    axs[1,2].set(title=\"Delta Patreon earnings per week (zoomed in)\")\n",
    "    axs[1,2].set_ylabel(\"Earnings\")  \n",
    "    color_neg_pos(axs[1,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_earning'])\n",
    "\n",
    "    # zoomed in patron earnings (cumulative)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning'], alpha=0.2)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning_ma'], color='royalblue')\n",
    "    axs[1,3].set(title=\"Patreon earnings per month (zoomed in)\")\n",
    "    axs[1,3].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # zoomed in youtube videos (delta)\n",
    "    axs[2,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', alpha=0.3)\n",
    "    axs[2,2].set(title=\"YouTube delta videos per week (zoomed in)\")\n",
    "    axs[2,2].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_videos'])\n",
    "\n",
    "    # zoomed in youtube videos (cumulative)\n",
    "    axs[2,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['videos'], 'r')\n",
    "    axs[2,3].set(title=\"YouTube cumulative videos (zoomed in)\")\n",
    "    axs[2,3].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # zoomed in youtube views (delta)\n",
    "    axs[3,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', alpha=0.3)\n",
    "    axs[3,2].set(title=\"YouTube delta views per week (zoomed in)\")\n",
    "    axs[3,2].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_views'])\n",
    "\n",
    "    # zoomed in youtube views (cumulative)\n",
    "    axs[3,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['views'], 'g')\n",
    "    axs[3,3].set(title=\"YouTube cumulative views (zoomed in)\")\n",
    "    axs[3,3].set_ylabel(\"# Views\")\n",
    "    \n",
    "    # zoomed in youtube subs (delta)\n",
    "    axs[4,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', alpha=0.3)\n",
    "    axs[4,2].set(title=\"YouTube delta subscriptions per week (zoomed in)\")\n",
    "    axs[4,2].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_subs'])\n",
    "\n",
    "    # zoomed in youtube subs (cumulative)\n",
    "    axs[4,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['subs'], 'm')\n",
    "    axs[4,3].set(title=\"YouTube cumulative subscriptions (zoomed in)\")\n",
    "    axs[4,3].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', alpha=0.3)\n",
    "    axs[5,2].set(title=\"YouTube videos durations (zoomed in)\")\n",
    "    axs[5,2].set_ylabel(\"Duration\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'])\n",
    "    \n",
    "        \n",
    "   # youtube likes per uploads\n",
    "    axs[6,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', alpha=0.3)\n",
    "    axs[6,2].set(title=\"YouTube likes (plotted against upload date) (zoomed in)\")\n",
    "    axs[6,2].set_ylabel(\"Likes\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['crawl_date'], tmp_df_yt_meta_zoomed['like_count'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################### FORMAT AXES ###################################\n",
    "\n",
    "    # format the axes\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if j < 2:\n",
    "                axs[i,j].set_xlim([date_min, date_max])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            if j >= 2:\n",
    "                axs[i,j].set_xlim([date_min_zoom, date_max_zoom])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "                # axs[i,j].xaxis.set_minor_locator(mdates.WeekdayLocator())\n",
    "            axs[i,j].xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.set_major_formatter(KM_formatter)\n",
    "            \n",
    "            \n",
    "    ################################### PLOT BREAKPOINT LINES AND POINTS ###################################\n",
    "\n",
    "    # plot vertical lines for breakpoint, breakpoint-1month, breakpoint+1month\n",
    "    print_legend = True\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if print_legend:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', label='break', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', label='- ' + str(MONTH_OFFSET.months)+' months', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', label='+' + str(MONTH_OFFSET.months)+' months', linewidth=2)          \n",
    "                # print_legend = False\n",
    "            else:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "    # axs[0,0].legend()\n",
    "    axs[0,1].legend()\n",
    "\n",
    "    # plot point for mean nb of patrons for breakpoint, breakpoint-1month, breakpoint+1month    \n",
    "    axs[0,3].plot(breakpoint_date - MONTH_OFFSET, ts_pt_df.at[(breakpoint_date - MONTH_OFFSET), 'patrons_ma'], marker='o', color='green')\n",
    "    axs[0,3].plot(breakpoint_date,               ts_pt_df.at[breakpoint_date              , 'patrons_ma'], marker='o', color='red')    \n",
    "    axs[0,3].plot(breakpoint_date + MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "\n",
    "\n",
    "    ################################### GRANGER CAUSALITY TESTS ###################################\n",
    "\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    fig.tight_layout(w_pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# print('\\n\\nGranger tests summary statistics:')\n",
    "    \n",
    "# print(f'• Number of patreon accounts analysed (patrons increase ratio > {incr_thresh_ratio}): {len(df_granger)}')\n",
    "# print(f'• Number of patreon with no Granger-causal link: {len(not_granger)} ({len(not_granger)/len(df_granger):.0%})')\n",
    "\n",
    "# print(f'• Number of patreon accounts per Granger-causal link:')\n",
    "# # Converting granger dict into list of tuples (in order to sort it), the 2nd value of the tuple being the count of accounts\n",
    "# granger_list = [(k, len(v)) for k, v in granger_dict.items()]\n",
    "# # sort by count desc\n",
    "# granger_list_desc = sorted(granger_list, key=lambda tup: -tup[1])\n",
    "# for (k,v) in granger_list_desc:\n",
    "#     print(f'    • {k[0]} \\t--> {k[1]}:\\t {v} ({v/len(df_granger):.0%})')\n",
    "\n",
    "\n",
    "# df_granger[columns] = df_granger[columns].astype('Int64')\n",
    "# df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'patreon.com/adamthewoo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save matches to disk in pickle format\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"match_pairs_dict.pickle\"\n",
    "\n",
    "# with open(output_file_path, 'wb') as f:\n",
    "#     pickle.dump(match_pairs_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l {LOCAL_DATA_FOLDER}match_pairs_dict.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(LOCAL_DATA_FOLDER+\"match_pairs_dict.pickle\", 'rb') as f:\n",
    "#     match_pairs_dict = pickle.load(f)\n",
    "# print(f\"Total number of matched pairs loaded: {len(match_pairs_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_balanced = balanced_df.loc[balanced_df['treat'] == 1] #People that attained the program\n",
    "control_balanced = balanced_df.loc[balanced_df['treat'] == 0] #People that didn't attain the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of control accounts  < ratio < {CONTROL_MAX_RATIO}):\\t {len(control_balanced)}')\n",
    "print(f'Number of treated accounts ({TREATED_MIN_RATIO} < ratio):\\t\\t {len(treated_balanced)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_balanced.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_balanced.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of patrons at breakpoint\n",
    "balanced_df.boxplot(by='treat', column='avg_patrons_bkpnt', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "balanced_df.boxplot(by='treat', column='mean_delta_patrons_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "balanced_df.boxplot(by='treat', column='mean_delta_views_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(balanced_df[columns_to_explore_relationships])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution BEFORE matching\n",
    "ax = sns.distplot(treated['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint, BEFORE matching', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution AFTER matching\n",
    "ax = sns.distplot(treated_balanced['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control_balanced['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint, AFTER matching', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Granger causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute Granger Tests and store statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granger_columns = [\n",
    "'pt_delta_patrons->yt_delta_videos',\n",
    "'pt_delta_patrons->yt_delta_views',\n",
    "'pt_delta_patrons->yt_delta_subs',\n",
    "'yt_delta_videos->pt_delta_patrons',\n",
    "'yt_delta_views->pt_delta_patrons',\n",
    "'yt_delta_subs->pt_delta_patrons'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare YouTube and Patreon timeseries for top patreon accounts with rolling average - MANUAL VERSION 2\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# WEEK_OFFSET = pd.DateOffset(weeks=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# variables for Granger Tests\n",
    "MAXLAG = 2\n",
    "granger_dict = {} # dictionary with  keys (cause --> effect) and values with list of corresponding patreon account(s)\n",
    "not_granger = []\n",
    "YT_variables = ['yt_delta_videos', 'yt_delta_views', 'yt_delta_subs']\n",
    "# PT_variables = ['pt_delta_patrons', 'pt_delta_earning']\n",
    "PT_variables = ['pt_delta_patrons']\n",
    "\n",
    "df_granger = df_treated.copy()\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in tqdm(df_granger.iterrows()):   \n",
    "\n",
    "    \n",
    "    ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    patreon = row['patreon_id']\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = df_top_pt_accts[df_top_pt_accts['patreon'] == patreon].copy()  \n",
    "    tmp_df_pt = tmp_df_pt.sort_values(by=['date'])\n",
    "    tmp_df_pt = tmp_df_pt.drop_duplicates()\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon].copy()\n",
    "    tmp_df_yt = tmp_df_yt.sort_values(by=['datetime'])\n",
    "    \n",
    "    # replace dates that were collected after 23:00 to their next day, and remove hour\n",
    "    tmp_df_yt['datetime_original'] = tmp_df_yt['datetime']\n",
    "    tmp_df_yt['datetime'] = tmp_df_yt['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "    \n",
    "    # remove hours and convert to datetime type\n",
    "    tmp_df_yt['datetime'] = pd.to_datetime(tmp_df_yt['datetime'].dt.date)\n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    # ch_ids = tmp_df_yt['channel'].unique()\n",
    "    # print(f\"\\n\\n\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "    # print(f\"https://www.{patreon}\")\n",
    "    # print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    # for ch_id in ch_ids:\n",
    "    #     print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "\n",
    "    \n",
    "    ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "    # set min and max dates for plots   \n",
    "    date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "    date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "    if date_max < date_min:\n",
    "        print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "        continue\n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "    tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    \n",
    "    # align both dataframes since youtube starts once a week\n",
    "    tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "               \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    \n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "\n",
    "    \n",
    "    r = row['ratio']\n",
    "\n",
    "#     print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "#     print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "#     print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "#     print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    \n",
    "#     print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "#     print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "#     print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    \n",
    "#     print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "#     print(f\"• Ratio between 2 increases:                            d2/d1  = {r:.2f}\")\n",
    "#     print(f\"• Percentage increase:                            |d2/d1|*100  = {abs(r):>+.0%}\")\n",
    "    \n",
    "\n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = breakpoint_date - (2 * MONTH_OFFSET)\n",
    "    date_max_zoom = breakpoint_date + (2 * MONTH_OFFSET)\n",
    "            \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "\n",
    "\n",
    "    ################################### GRANGER CAUSALITY TESTS ###################################\n",
    "\n",
    "    # create a new dataframe with merged columns (the dates might have a day difference)\n",
    "    selected_pt_columns  = ['delta_earning', 'delta_patrons']\n",
    "    df_pt = ts_pt_weekly_avg_df_zoomed\n",
    "    df_pt = df_pt[selected_pt_columns].reset_index().add_prefix('pt_')\n",
    "\n",
    "    # selected_yt_columns = ['datetime', 'delta_views', 'delta_subs', 'delta_videos']\n",
    "    selected_yt_columns = ['datetime', 'datetime_original', 'delta_views', 'delta_subs', 'delta_videos']\n",
    "    df_yt = tmp_df_yt_zoomed\n",
    "    df_yt = df_yt[selected_yt_columns].reset_index().add_prefix('yt_')\n",
    "\n",
    "    # concatenated 2 dfs and select and reorder columns\n",
    "    df_concat = pd.concat([df_pt, df_yt], axis=1)\n",
    "    concat_columns = ['pt_date', 'yt_datetime', 'pt_delta_earning', 'pt_delta_patrons', 'yt_delta_views', 'yt_delta_subs', 'yt_delta_videos']\n",
    "    df_concat = df_concat[concat_columns]\n",
    "    # df_concat['dates_match'] = df_concat['pt_date'] == df_concat['yt_datetime']\n",
    "    \n",
    "    # display(df_concat.round())\n",
    "    # display(df_concat.style.set_caption(f\"df_concat\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(f\"\\nGranger Causality Tests:\")\n",
    "    \n",
    "    granger_causal_link = False\n",
    "    for pt_var in PT_variables:\n",
    "        for yt_var in YT_variables:\n",
    "            \n",
    "            # if nan values in this df, skip\n",
    "            if df_concat[[yt_var, pt_var]].isna().values.any():\n",
    "                continue\n",
    "                \n",
    "            pvalue_fwd = {}\n",
    "            pvalue_rev = {}\n",
    "            \n",
    "            try:\n",
    "                # print(f'\\n\\n• {pt_var} --> {yt_var}')\n",
    "                granger_test_fwd = grangercausalitytests(df_concat[[yt_var, pt_var]], maxlag=MAXLAG, verbose=False)  \n",
    "                # print(f'\\n\\n• {yt_var} --> {pt_var}')\n",
    "                granger_test_rev = grangercausalitytests(df_concat[[pt_var, yt_var]], maxlag=MAXLAG, verbose=False) \n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "\n",
    "            for lag in range(1, MAXLAG+1):           \n",
    "                pvalue_fwd[lag] = granger_test_fwd[lag][0]['ssr_ftest'][1]\n",
    "                pvalue_rev[lag] = granger_test_rev[lag][0]['ssr_ftest'][1]\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            min_pvalue_fwd = min(pvalue_fwd.values())\n",
    "            if min_pvalue_fwd < 0.05:\n",
    "                granger_causal_link = True\n",
    "                min_lag_fwd = [k for k, v in pvalue_fwd.items() if v == min_pvalue_fwd][0]\n",
    "                # print(f'• {pt_var} --> {yt_var} (pvalue={min_pvalue_fwd:.3f}, lag={min_lag_fwd})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 1\n",
    "\n",
    "                if (pt_var, yt_var) in granger_dict:                   \n",
    "                    granger_dict[(pt_var, yt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(pt_var, yt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 0\n",
    "                \n",
    "            min_pvalue_rev = min(pvalue_rev.values())\n",
    "            if min_pvalue_rev < 0.05:\n",
    "                granger_causal_link = True\n",
    "                min_lag_rev = [k for k, v in pvalue_rev.items() if v == min_pvalue_rev][0]\n",
    "                # print(f'• {yt_var} --> {pt_var} (pvalue={min_pvalue_rev:.3f}, lag={min_lag_rev})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 1\n",
    "                \n",
    "                if (yt_var, pt_var) in granger_dict:\n",
    "                    granger_dict[(yt_var, pt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(yt_var, pt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 0\n",
    "                \n",
    "\n",
    "    if (granger_causal_link == False):\n",
    "        # print(\"• No Granger causality found for this account\")\n",
    "        not_granger.append(patreon)\n",
    "    \n",
    "    # print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "print(f'\\n\\nGranger tests summary statistics: (with maxlag = {MAXLAG})')\n",
    "    \n",
    "print(f'• Number of patreon accounts analysed (patrons increase ratio > {INCR_RATIO_THRESH}): {len(df_granger)}')\n",
    "print(f'• Number of patreon with no Granger-causal link: {len(not_granger)} ({len(not_granger)/len(df_granger):.0%})')\n",
    "\n",
    "print(f'• Number of patreon accounts per Granger-causal link:')\n",
    "\n",
    "# Converting granger dict into list of tuples (in order to sort it), the 2nd value of the tuple being the count of accounts\n",
    "granger_list = [(k, len(v)) for k, v in granger_dict.items()]\n",
    "# sort by count desc\n",
    "granger_list_desc = sorted(granger_list, key=lambda tup: -tup[1])\n",
    "for (k,v) in granger_list_desc:\n",
    "    print(f'    • {k[0]} \\t--> {k[1]}:\\t {v} ({v/len(df_granger):.0%})')\n",
    "\n",
    "df_granger[granger_columns] = df_granger[granger_columns].astype('Int64')\n",
    "df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"df_granger\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "output_file_path = LOCAL_DATA_FOLDER+\"df_granger.tsv.gz\"\n",
    "df_granger.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Granger causality plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_granger.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load granger dataframe\n",
    "df_granger = pd.read_csv(LOCAL_DATA_FOLDER+\"df_granger.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_granger['bkpt_date'] = pd.to_datetime(df_granger['bkpt_date'])\n",
    "df_granger['bkpt_date_sub30'] = pd.to_datetime(df_granger['bkpt_date_sub30'])\n",
    "df_granger['bkpt_date_add30'] = pd.to_datetime(df_granger['bkpt_date_add30'])\n",
    "df_granger[granger_columns] = df_granger[granger_columns].astype('Int64')\n",
    "df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split columns in PT->YT, and reverse YT->PT\n",
    "cols1 = [\n",
    "'pt_delta_patrons->yt_delta_videos',\n",
    "'pt_delta_patrons->yt_delta_views',\n",
    "'pt_delta_patrons->yt_delta_subs'\n",
    "]\n",
    "cols2 = [\n",
    "'yt_delta_videos->pt_delta_patrons',\n",
    "'yt_delta_views->pt_delta_patrons',\n",
    "'yt_delta_subs->pt_delta_patrons'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For different minimum ratios of increase, plot sum of Granger-causal links between Patreon and YouTube time-series (in blue) and vice-versa (in orange)\n",
    "\n",
    "nb_plots = 10\n",
    "sbplt_cols = 5\n",
    "sbplt_rows = int(nb_plots / sbplt_cols)\n",
    "\n",
    "fig, axs = plt.subplots(sbplt_rows, sbplt_cols, figsize=(16,12), sharey=True, sharex=True)\n",
    "for idx in range(0, nb_plots):\n",
    "\n",
    "    row = math.floor(idx/sbplt_cols)\n",
    "    col = idx % sbplt_cols\n",
    "    sbplt = axs[row, col]\n",
    "    \n",
    "    ratio_df = df_granger[df_granger['ratio'] > idx+1]\n",
    "    \n",
    "    # print(f'\\n\\nratio > {idx+1}:')\n",
    "    # print(f'total number of accounts: {len(ratio_df)}:')\n",
    "    # no_causal_links_df = ratio_df[ratio_df[cols1 + cols2].sum(axis=1) == 0]\n",
    "    # print(f'nb accts with no Granger-causal links: {len(no_causal_links_df)} ({len(no_causal_links_df)/len(ratio_df):.0%})')\n",
    "    # print(f'\\nPatreon --> YouTube:')\n",
    "    # print(ratio_df[cols1].sum())\n",
    "    # print(f'\\nYouTube --> Patreon:')\n",
    "    # print(ratio_df[cols2].sum())\n",
    "    \n",
    "    granger_series = ratio_df[cols1 + cols2].sum()/len(ratio_df)\n",
    "    sbplt.bar(granger_series[cols1].index, granger_series[cols1].values, label='PT --> YT')\n",
    "    sbplt.bar(granger_series[cols2].index, granger_series[cols2].values, label='YT --> PT')\n",
    "    sbplt.set_title(f\"ratio > {idx+1}\\n # accnts = {len(ratio_df)}\")\n",
    "    # sbplt.set_xlabel(\"Granger-causal links\")\n",
    "    # sbplt.set_ylabel(\"% of PT accts\")\n",
    "    sbplt.tick_params(labelrotation=90)\n",
    "    \n",
    "\n",
    "axs[0, 0].legend()\n",
    "axs[1, 4].legend()\n",
    "\n",
    "\n",
    "fig.suptitle('Granger-causal links between Patreon and YouTube time-series, for different minimum ratios of increase at breakpoint \\n (one account can have multiple causal-links)', fontweight=\"bold\")\n",
    "fig.text(0.5,0, 'Granger-causal links')\n",
    "fig.text(0,0.5, 'Percentage of Patreon accts ', rotation = 90)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=3, w_pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Although there are granger links in both directions, we notice that there are more granger links going from Patreon --> YouTube, than YouTube --> Patreon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 Plot PT time-series, YT time-series, and YT metadata + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_plot(ax, x, y, title, x_axis_label=\"default x\", y_axis_label=\"default y\", color=\"#1f77b4\", alpha=1):\n",
    "#     ax.plot(x, y, color, alpha)\n",
    "#     ax.set(title=title)\n",
    "#     ax.set_xlabel(x_axis_label)    \n",
    "#     ax.set_ylabel(y_axis_label)    \n",
    "\n",
    "# custom_plot(axs[0,0], ts_pt_df['date'], ts_pt_df['patrons'], alpha=0.2)\n",
    "# custom_plot(axs[0,0], ts_pt_df['date'], ts_pt_df['patrons_ravg'], \"Number of patrons\", y_axis_label=\"# Patrons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_neg_pos(ax, x, y):\n",
    "    if y.isnull().all():\n",
    "        return\n",
    "    if (y.min() < 0): \n",
    "        # fill negative values in red and draw a horizontal line at 0\n",
    "        ax.fill_between(x, y.min(), 0, color='red', alpha=0.05)\n",
    "        ax.axhline(y=0, linestyle='solid', color= 'black', linewidth=0.5)\n",
    "    # fill positive values in green\n",
    "    # ax.fill_between(x, 0, y.max(), color='green', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number in thousand (k) or in million (M) on y axis\n",
    "def KM(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    if x > 999_999:\n",
    "        return '%2.1fM' % (x * 1e-6)\n",
    "    elif x > 999:\n",
    "        return '%2.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%3.0f ' % (x)\n",
    "KM_formatter = FuncFormatter(KM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter accounts that match selected Patreon ids\n",
    "df_yt_metadata_pt_filtered = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(df_treated['patreon_id'])].copy()\n",
    "print(f'Filter accounts that match selected Patreon ids: {len(df_yt_metadata_pt_filtered):,} ({len(df_yt_metadata_pt_filtered)/len(df_yt_metadata_pt):.1%} of videos containing a PT accounts) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt_filtered['crawl_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['crawl_date'])\n",
    "df_yt_metadata_pt_filtered['upload_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['upload_date'])\n",
    "df_yt_metadata_pt_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare Patreon and YouTube timeseries + YouTube metadata\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# WEEK_OFFSET = pd.DateOffset(weeks=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# variables for Granger Tests\n",
    "MAXLAG = 2\n",
    "granger_dict = {} # dictionary with  keys (cause --> effect) and values with list of corresponding patreon account(s)\n",
    "not_granger = []\n",
    "YT_variables = ['yt_delta_videos', 'yt_delta_views', 'yt_delta_subs']\n",
    "# PT_variables = ['pt_delta_patrons', 'pt_delta_earning']\n",
    "PT_variables = ['pt_delta_patrons']\n",
    "\n",
    "# df_granger = df_pt_bkpnt_filt.copy()\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in df_treated.iterrows():\n",
    "    fig, axs = plt.subplots(7, 4, figsize=(26, 10), sharey=False, sharex=False)\n",
    "    \n",
    "    \n",
    "    patreon = row['patreon_id']\n",
    "    \n",
    "    ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = df_top_pt_accts[df_top_pt_accts['patreon'] == patreon].copy()  \n",
    "    tmp_df_pt = tmp_df_pt.sort_values(by=['date'])\n",
    "    tmp_df_pt = tmp_df_pt.drop_duplicates()\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon].copy()\n",
    "    tmp_df_yt = tmp_df_yt.sort_values(by=['datetime'])\n",
    "    \n",
    "    # youtube metadata\n",
    "    tmp_df_yt_meta = df_yt_metadata_pt_filtered[df_yt_metadata_pt_filtered['patreon_id'] == patreon].copy()   \n",
    "    tmp_df_yt_meta = tmp_df_yt_meta.sort_values('upload_date')\n",
    "    # tmp_df_yt_meta['upload_date'] = pd.to_datetime(tmp_df_yt_meta['upload_date'])\n",
    "    \n",
    "    # replace dates that were collected after 23:00 to their next day, and remove hour\n",
    "    tmp_df_yt['datetime_original'] = tmp_df_yt['datetime']\n",
    "    tmp_df_yt['datetime'] = tmp_df_yt['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "    \n",
    "    # remove hours and convert to datetime type\n",
    "    tmp_df_yt['datetime'] = pd.to_datetime(tmp_df_yt['datetime'].dt.date)\n",
    "    \n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    ch_ids = tmp_df_yt['channel'].unique()\n",
    "    print(f\"\\n\\n\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "    print(f\"https://www.{patreon}\")\n",
    "    print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    for ch_id in ch_ids:\n",
    "        print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "   \n",
    "    print(f'\\nYouTube Metadata: ')\n",
    "    print('• YT videos were uploaded between {} and {}'.format(tmp_df_yt_meta['upload_date'].min().strftime('%B %d, %Y'),\n",
    "                                                             tmp_df_yt_meta['upload_date'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "    print('• YT metadata was crawled between {} and {}'.format(tmp_df_yt_meta['crawl_date'].min().strftime('%B %d, %Y'),\n",
    "                                                             tmp_df_yt_meta['crawl_date'].max().strftime('%B %d, %Y')))\n",
    "    \n",
    "    ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "    # set min and max dates for plots   \n",
    "    date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "    date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "    if date_max < date_min:\n",
    "        print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "        continue\n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "    tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    tmp_df_yt_meta = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min) & (tmp_df_yt_meta['upload_date'] <= date_max)]\n",
    "    \n",
    "    # align both dataframes since youtube starts once a week\n",
    "    tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "              \n",
    "    ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "    # breakpoint_date = find_breakpoint_v2(tmp_df_pt, 'patrons_ma')\n",
    "    # print(\"Breakpoint date: \", breakpoint_date.date())\n",
    "\n",
    "    # check that dates prior and after breakpoint exist\n",
    "    if not (((breakpoint_date - 1*MONTH_OFFSET)) in ts_pt_df.index and ((breakpoint_date + 1*MONTH_OFFSET) in ts_pt_df.index)):\n",
    "        print(f\"ERROR: Breakpoint too close to edge of patreon time series or missing data\\n\")\n",
    "        plt.figure().clear(); plt.close(); plt.cla(); plt.clf(); plt.show()\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    \n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "\n",
    "    \n",
    "    r = row['ratio']\n",
    "\n",
    "    print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "    print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "    print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "    print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    \n",
    "    print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "    print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    \n",
    "    print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "    print(f\"• Ratio between 2 increases:                            d2/d1  = {r:.2f}\")\n",
    "    print(f\"• Percentage increase:                            |d2/d1|*100  = {abs(r):>+.0%}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ################################### CALCULATE DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "    \n",
    "    ##### PATREON #####\n",
    "    tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpt_date_sub30) & (ts_pt_weekly_avg_df_float64.index <= bkpt_date)]\n",
    "    tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpt_date) & (ts_pt_weekly_avg_df_float64.index <= bkpt_date_add30)]\n",
    "\n",
    "    # delta patrons\n",
    "    mean_delta_patrons_befor = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "    mean_delta_patrons_after = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "        \n",
    "    # delta earnings\n",
    "    mean_delta_earnings_befor = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "    mean_delta_earnings_after = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "\n",
    "    \n",
    "    ##### YOUTUBE TIME SERIES #####\n",
    "    tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpt_date_sub30) & (tmp_df_yt['datetime'] <= bkpt_date      )]\n",
    "    tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpt_date      ) & (tmp_df_yt['datetime'] <= bkpt_date_add30)]\n",
    "    \n",
    "    # delta videos\n",
    "    mean_delta_videos_befor = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "    mean_delta_videos_after = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "\n",
    "    # delta views\n",
    "    mean_delta_views_befor = tmp_df_YT_sub30['delta_views'].mean()\n",
    "    mean_delta_views_after = tmp_df_YT_add30['delta_views'].mean()  \n",
    "\n",
    "    # delta subscriptions\n",
    "    mean_delta_subs_befor = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "    mean_delta_subs_after = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "\n",
    "    \n",
    "    ##### YOUTUBE METADATA #####\n",
    "    tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpt_date_sub30) & (tmp_df_yt_meta['upload_date'] <= bkpt_date      )]\n",
    "    tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpt_date      ) & (tmp_df_yt_meta['upload_date'] <= bkpt_date_add30)]\n",
    "        \n",
    "    # durations\n",
    "    mean_duration_befor = tmp_df_YT_META_sub30['duration'].mean()\n",
    "    mean_duration_after = tmp_df_YT_META_add30['duration'].mean()      \n",
    "        \n",
    "    # likes\n",
    "    mean_likes_befor = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "    mean_likes_after = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "        \n",
    "    \n",
    "    # plot dots in the middle of region for the region means   \n",
    "    axs[0,2].plot(tmp_df_PT_sub30.index.mean(), mean_delta_patrons_befor, marker='o', color='green', markersize=15)\n",
    "    axs[0,2].plot(tmp_df_PT_add30.index.mean(), mean_delta_patrons_after, marker='o', color='orange', markersize=15)\n",
    "    axs[1,2].plot(tmp_df_PT_sub30.index.mean(), mean_delta_earnings_befor, marker='o', color='green', markersize=15)\n",
    "    axs[1,2].plot(tmp_df_PT_add30.index.mean(), mean_delta_earnings_after, marker='o', color='orange', markersize=15)\n",
    "    axs[2,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_videos_befor, marker='o', color='green', markersize=15)\n",
    "    axs[2,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_videos_after, marker='o', color='orange', markersize=15)\n",
    "    axs[3,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_views_befor, marker='o', color='green', markersize=15)\n",
    "    axs[3,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_views_after, marker='o', color='orange', markersize=15)  \n",
    "    axs[4,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_subs_befor, marker='o', color='green', markersize=15)\n",
    "    axs[4,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_subs_after, marker='o', color='orange', markersize=15)\n",
    "    \n",
    "    # sometimes there is no value at all for this period of time in YT meta --> error when plotting\n",
    "    if not (tmp_df_YT_META_sub30.empty or tmp_df_YT_META_add30.empty):\n",
    "        axs[5,2].plot(tmp_df_YT_META_sub30['upload_date'].mean(), mean_duration_befor, marker='o', color='green', markersize=15)\n",
    "        axs[5,2].plot(tmp_df_YT_META_add30['upload_date'].mean(), mean_duration_after, marker='o', color='orange', markersize=15)  \n",
    "        axs[6,2].plot(tmp_df_YT_META_sub30['upload_date'].mean(), mean_likes_befor, marker='o', color='green', markersize=15)\n",
    "        axs[6,2].plot(tmp_df_YT_META_add30['upload_date'].mean(), mean_likes_after, marker='o', color='orange', markersize=15)  \n",
    "\n",
    "    \n",
    "    # plot horizontal lines for means\n",
    "    mean_befor_list = [mean_delta_patrons_befor, mean_delta_earnings_befor, mean_delta_videos_befor, mean_delta_views_befor, mean_delta_subs_befor, mean_duration_befor, mean_likes_befor]\n",
    "    mean_afer_list = [mean_delta_patrons_after, mean_delta_earnings_after, mean_delta_videos_after, mean_delta_views_after, mean_delta_subs_after, mean_duration_after, mean_likes_after]\n",
    "       \n",
    "    for idx, mean in enumerate(mean_befor_list):\n",
    "            if not math.isnan(mean):\n",
    "                axs[idx,2].hlines(y=mean, xmin=bkpt_date_sub30, xmax=bkpt_date      , linewidth=2, linestyle='--', color='green')\n",
    "\n",
    "    for idx, mean in enumerate(mean_afer_list):\n",
    "            if not math.isnan(mean):\n",
    "                axs[idx,2].hlines(y=mean, xmin=bkpt_date,       xmax=bkpt_date_add30, linewidth=2, linestyle='--', color='orange')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    ################################### ZOOM OUT PLOTS ###################################\n",
    "    \n",
    "    # number of patrons (delta)\n",
    "    axs[0,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,0].set(title=\"Delta patrons per week\")\n",
    "    axs[0,0].set_ylabel(\"Δ Patrons\")    \n",
    "    color_neg_pos(axs[0,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'])\n",
    "\n",
    "    # number of patrons (cumulative)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons'], alpha=0.2)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons_ma'])\n",
    "    axs[0,1].set(title=\"Number of patrons\")\n",
    "    axs[0,1].set_ylabel(\"# Patrons\")\n",
    "\n",
    "    # patreon earnings (delta)\n",
    "    axs[1,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,0].set(title=\"Patreon delta earnings per week\")\n",
    "    axs[1,0].set_ylabel(\"Δ Earnings\") \n",
    "    color_neg_pos(axs[1,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'])\n",
    "\n",
    "    # patreon earnings (cumulative)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning'], alpha=0.2)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning_ma'], color='royalblue')\n",
    "    axs[1,1].set(title=\"Patreon earnings per month\")\n",
    "    axs[1,1].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # youtube videos (delta)\n",
    "    axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,0].set(title=\"YouTube delta videos per week\")\n",
    "    axs[2,0].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,0], tmp_df_yt['datetime'], tmp_df_yt['delta_videos'])\n",
    "\n",
    "    # youtube videos (cumulative)\n",
    "    axs[2,1].plot(tmp_df_yt['datetime'], tmp_df_yt['videos'], 'r')\n",
    "    axs[2,1].set(title=\"YouTube cumulative videos\")\n",
    "    axs[2,1].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # youtube views (delta)\n",
    "    axs[3,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,0].set(title=\"YouTube delta views per week\")\n",
    "    axs[3,0].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,0], tmp_df_yt['datetime'], tmp_df_yt['delta_views'])\n",
    "\n",
    "    # youtube views (cumulative)\n",
    "    axs[3,1].plot(tmp_df_yt['datetime'], tmp_df_yt['views'], 'g')\n",
    "    axs[3,1].set(title=\"YouTube cumulative views\")\n",
    "    axs[3,1].set_ylabel(\"# Views\")\n",
    "\n",
    "    # youtube subs (delta)\n",
    "    axs[4,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,0].set(title=\"YouTube delta subscriptions per week\")\n",
    "    axs[4,0].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,0], tmp_df_yt['datetime'], tmp_df_yt['delta_subs'])\n",
    "\n",
    "    # youtube subs (cumulative)\n",
    "    axs[4,1].plot(tmp_df_yt['datetime'], tmp_df_yt['subs'], 'm')\n",
    "    axs[4,1].set(title=\"YouTube cumulative subscriptions\")\n",
    "    axs[4,1].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,0].set(title=\"YouTube videos durations\")\n",
    "    axs[5,0].set_ylabel(\"Duration\")\n",
    "    \n",
    "    \n",
    "    # youtube likes at crawl date\n",
    "    axs[6,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,0].set(title=\"YouTube likes (plotted against upload date)\")\n",
    "    axs[6,0].set_ylabel(\"Likes\")\n",
    "    \n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = breakpoint_date - (2 * MONTH_OFFSET)\n",
    "    date_max_zoom = breakpoint_date + (2 * MONTH_OFFSET)\n",
    "            \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_meta_zoomed = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min_zoom) & (tmp_df_yt_meta['upload_date'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "    \n",
    "   ################################### ZOOM IN PLOTS  ###################################\n",
    "\n",
    "    # zoomed in patron numbers (delta)\n",
    "    axs[0,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', alpha=0.3)\n",
    "    axs[0,2].set(title=\"Delta patrons per week\")\n",
    "    axs[0,2].set_ylabel(\"Δ Patrons\")\n",
    "    color_neg_pos(axs[0,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'])\n",
    "    \n",
    "    # zoomed in patron numbers (cumulative)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons'], alpha=0.2)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons_ma'])\n",
    "    axs[0,3].set(title=\"Number of patrons (zoomed in)\")\n",
    "    axs[0,3].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "    # zoomed in patron earnings (delta)\n",
    "    axs[1,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', alpha=0.3)\n",
    "    axs[1,2].set(title=\"Delta Patreon earnings per week (zoomed in)\")\n",
    "    axs[1,2].set_ylabel(\"Earnings\")  \n",
    "    color_neg_pos(axs[1,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_earning'])\n",
    "\n",
    "    # zoomed in patron earnings (cumulative)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning'], alpha=0.2)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning_ma'], color='royalblue')\n",
    "    axs[1,3].set(title=\"Patreon earnings per month (zoomed in)\")\n",
    "    axs[1,3].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # zoomed in youtube videos (delta)\n",
    "    axs[2,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', alpha=0.3)\n",
    "    axs[2,2].set(title=\"YouTube delta videos per week (zoomed in)\")\n",
    "    axs[2,2].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_videos'])\n",
    "\n",
    "    # zoomed in youtube videos (cumulative)\n",
    "    axs[2,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['videos'], 'r')\n",
    "    axs[2,3].set(title=\"YouTube cumulative videos (zoomed in)\")\n",
    "    axs[2,3].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # zoomed in youtube views (delta)\n",
    "    axs[3,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', alpha=0.3)\n",
    "    axs[3,2].set(title=\"YouTube delta views per week (zoomed in)\")\n",
    "    axs[3,2].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_views'])\n",
    "\n",
    "    # zoomed in youtube views (cumulative)\n",
    "    axs[3,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['views'], 'g')\n",
    "    axs[3,3].set(title=\"YouTube cumulative views (zoomed in)\")\n",
    "    axs[3,3].set_ylabel(\"# Views\")\n",
    "    \n",
    "    # zoomed in youtube subs (delta)\n",
    "    axs[4,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', alpha=0.3)\n",
    "    axs[4,2].set(title=\"YouTube delta subscriptions per week (zoomed in)\")\n",
    "    axs[4,2].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_subs'])\n",
    "\n",
    "    # zoomed in youtube subs (cumulative)\n",
    "    axs[4,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['subs'], 'm')\n",
    "    axs[4,3].set(title=\"YouTube cumulative subscriptions (zoomed in)\")\n",
    "    axs[4,3].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', alpha=0.3)\n",
    "    axs[5,2].set(title=\"YouTube videos durations (zoomed in)\")\n",
    "    axs[5,2].set_ylabel(\"Duration\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'])\n",
    "    \n",
    "        \n",
    "   # youtube likes per uploads\n",
    "    axs[6,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', alpha=0.3)\n",
    "    axs[6,2].set(title=\"YouTube likes (plotted against upload date) (zoomed in)\")\n",
    "    axs[6,2].set_ylabel(\"Likes\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['crawl_date'], tmp_df_yt_meta_zoomed['like_count'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################### FORMAT AXES ###################################\n",
    "\n",
    "    # format the axes\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if j < 2:\n",
    "                axs[i,j].set_xlim([date_min, date_max])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            if j >= 2:\n",
    "                axs[i,j].set_xlim([date_min_zoom, date_max_zoom])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "                # axs[i,j].xaxis.set_minor_locator(mdates.WeekdayLocator())\n",
    "            axs[i,j].xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.set_major_formatter(KM_formatter)\n",
    "            \n",
    "            \n",
    "    ################################### PLOT BREAKPOINT LINES AND POINTS ###################################\n",
    "\n",
    "    # plot vertical lines for breakpoint, breakpoint-1month, breakpoint+1month\n",
    "    print_legend = True\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if print_legend:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', label='break', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', label='- ' + str(MONTH_OFFSET.months)+' months', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', label='+' + str(MONTH_OFFSET.months)+' months', linewidth=2)          \n",
    "                # print_legend = False\n",
    "            else:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "    # axs[0,0].legend()\n",
    "    axs[0,1].legend()\n",
    "\n",
    "    # plot point for mean nb of patrons for breakpoint, breakpoint-1month, breakpoint+1month    \n",
    "    axs[0,3].plot(breakpoint_date - MONTH_OFFSET, ts_pt_df.at[(breakpoint_date - MONTH_OFFSET), 'patrons_ma'], marker='o', color='green')\n",
    "    axs[0,3].plot(breakpoint_date,               ts_pt_df.at[breakpoint_date              , 'patrons_ma'], marker='o', color='red')    \n",
    "    axs[0,3].plot(breakpoint_date + MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "\n",
    "\n",
    "    ################################### GRANGER CAUSALITY TESTS ###################################\n",
    "\n",
    "    # create a new dataframe with merged columns (the dates might have a day difference)\n",
    "    selected_pt_columns  = ['delta_earning', 'delta_patrons']\n",
    "    df_pt = ts_pt_weekly_avg_df_zoomed\n",
    "    df_pt = df_pt[selected_pt_columns].reset_index().add_prefix('pt_')\n",
    "\n",
    "    # selected_yt_columns = ['datetime', 'delta_views', 'delta_subs', 'delta_videos']\n",
    "    selected_yt_columns = ['datetime', 'datetime_original', 'delta_views', 'delta_subs', 'delta_videos']\n",
    "    df_yt = tmp_df_yt_zoomed\n",
    "    df_yt = df_yt[selected_yt_columns].reset_index().add_prefix('yt_')\n",
    "\n",
    "    # concatenated 2 dfs and select and reorder columns\n",
    "    df_concat = pd.concat([df_pt, df_yt], axis=1)\n",
    "    concat_columns = ['pt_date', 'yt_datetime', 'pt_delta_earning', 'pt_delta_patrons', 'yt_delta_views', 'yt_delta_subs', 'yt_delta_videos']\n",
    "    df_concat = df_concat[concat_columns]\n",
    "    # df_concat['dates_match'] = df_concat['pt_date'] == df_concat['yt_datetime']\n",
    "    \n",
    "    # display(df_concat.round())\n",
    "    # display(df_concat.style.set_caption(f\"df_concat\"))\n",
    "    \n",
    "    \n",
    "    print(f\"\\nGranger Causality Tests:\")\n",
    "    \n",
    "    granger_causal_link = False\n",
    "    for pt_var in PT_variables:\n",
    "        for yt_var in YT_variables:\n",
    "            \n",
    "            # if nan values in this df, skip\n",
    "            if df_concat[[yt_var, pt_var]].isna().values.any():\n",
    "                continue\n",
    "                \n",
    "            pvalue_fwd = {}\n",
    "            pvalue_rev = {}\n",
    "            \n",
    "            try:\n",
    "                # print(f'\\n\\n• {pt_var} --> {yt_var}')\n",
    "                granger_test_fwd = grangercausalitytests(df_concat[[yt_var, pt_var]], maxlag=MAXLAG, verbose=False)  \n",
    "                # print(f'\\n\\n• {yt_var} --> {pt_var}')\n",
    "                granger_test_rev = grangercausalitytests(df_concat[[pt_var, yt_var]], maxlag=MAXLAG, verbose=False) \n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "\n",
    "            for lag in range(1, MAXLAG+1):           \n",
    "                pvalue_fwd[lag] = granger_test_fwd[lag][0]['ssr_ftest'][1]\n",
    "                pvalue_rev[lag] = granger_test_rev[lag][0]['ssr_ftest'][1]\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            min_pvalue_fwd = min(pvalue_fwd.values())\n",
    "            if min_pvalue_fwd < 0.05:\n",
    "                granger_causal_link = True\n",
    "                min_lag_fwd = [k for k, v in pvalue_fwd.items() if v == min_pvalue_fwd][0]\n",
    "                print(f'• {pt_var} --> {yt_var} (pvalue={min_pvalue_fwd:.3f}, lag={min_lag_fwd})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 1\n",
    "\n",
    "                if (pt_var, yt_var) in granger_dict:                   \n",
    "                    granger_dict[(pt_var, yt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(pt_var, yt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 0\n",
    "                \n",
    "                \n",
    "                \n",
    "            min_pvalue_rev = min(pvalue_rev.values())\n",
    "            if min_pvalue_rev < 0.05:\n",
    "                granger_causal_link = True\n",
    "                min_lag_rev = [k for k, v in pvalue_rev.items() if v == min_pvalue_rev][0]\n",
    "                print(f'• {yt_var} --> {pt_var} (pvalue={min_pvalue_rev:.3f}, lag={min_lag_rev})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 1\n",
    "                \n",
    "                if (yt_var, pt_var) in granger_dict:\n",
    "                    granger_dict[(yt_var, pt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(yt_var, pt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 0\n",
    "                \n",
    "\n",
    "    if (granger_causal_link == False):\n",
    "        print(\"• No Granger causality found for this account\")\n",
    "        not_granger.append(patreon)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    fig.tight_layout(w_pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# print('\\n\\nGranger tests summary statistics:')\n",
    "    \n",
    "# print(f'• Number of patreon accounts analysed (patrons increase ratio > {incr_thresh_ratio}): {len(df_granger)}')\n",
    "# print(f'• Number of patreon with no Granger-causal link: {len(not_granger)} ({len(not_granger)/len(df_granger):.0%})')\n",
    "\n",
    "# print(f'• Number of patreon accounts per Granger-causal link:')\n",
    "# # Converting granger dict into list of tuples (in order to sort it), the 2nd value of the tuple being the count of accounts\n",
    "# granger_list = [(k, len(v)) for k, v in granger_dict.items()]\n",
    "# # sort by count desc\n",
    "# granger_list_desc = sorted(granger_list, key=lambda tup: -tup[1])\n",
    "# for (k,v) in granger_list_desc:\n",
    "#     print(f'    • {k[0]} \\t--> {k[1]}:\\t {v} ({v/len(df_granger):.0%})')\n",
    "\n",
    "\n",
    "# df_granger[columns] = df_granger[columns].astype('Int64')\n",
    "# df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Propensity Score Matching\n",
    "_(inspired by ADA exercise session by Tiziano Piccardi and Kristina Gligoric)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt_bkpnt = pd.read_csv(LOCAL_DATA_FOLDER+\"df_pt_bkpnt.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_pt_bkpnt['bkpt_date'] = pd.to_datetime(df_pt_bkpnt['bkpt_date'])\n",
    "df_pt_bkpnt['bkpt_date_sub30'] = pd.to_datetime(df_pt_bkpnt['bkpt_date_sub30'])\n",
    "df_pt_bkpnt['bkpt_date_add30'] = pd.to_datetime(df_pt_bkpnt['bkpt_date_add30'])\n",
    "df_pt_bkpnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split \"treated\" VS \"non-treated\" PT accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the following caracteristics for the Treated vs Non-Treated accounts:\n",
    "\n",
    "    \n",
    "- **Treated** accounts (increasing more after the breakpoint) will have\n",
    "    - increase ratio > 2\n",
    "    - d1 > 0\n",
    "    - d2 > 0\n",
    "\n",
    "\n",
    "- **Non-Treated / Control** accounts(increasing linearly before and after breakpoint) will have\n",
    "    - increase ratio = 1\n",
    "    - d1 > 0\n",
    "    - d2 > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns to see d1 d2 and ratio at the beginning\n",
    "columns_new_order = ['patreon_id', 'yt_channel_id', 'treat', 'd1', 'd2', 'ratio', 'bkpt_date', 'bkpt_date_sub30',\n",
    "       'bkpt_date_add30', 'avg_patrons_bkpnt', 'avg_patrons_sub30',\n",
    "       'avg_patrons_add30', 'mean_delta_patrons_befor',\n",
    "       'mean_delta_patrons_after', 'mean_delta_earnings_befor',\n",
    "       'mean_delta_earnings_after', 'mean_delta_videos_befor',\n",
    "       'mean_delta_videos_after', 'mean_delta_views_befor',\n",
    "       'mean_delta_views_after', 'mean_delta_subs_befor',\n",
    "       'mean_delta_subs_after', 'mean_duration_befor', 'mean_likes_after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCR_MIN_RATIO = 1\n",
    "TREATED_MIN_RATIO = 2\n",
    "CONTROL_MAX_RATIO = 2\n",
    "\n",
    "predicate1 =        df_pt_bkpnt['d1'] > 0\n",
    "predicate2 =        df_pt_bkpnt['d2'] > 0\n",
    "predicate3 =        df_pt_bkpnt['ratio'] > INCR_MIN_RATIO\n",
    "\n",
    "df_treat = df_pt_bkpnt[predicate1 & predicate2 & predicate3]\n",
    "df_treat = df_treat.reset_index(drop=True)\n",
    "\n",
    "df_treat['treat'] = df_treat['ratio'].map(lambda x: 1 if x > TREATED_MIN_RATIO else 0)\n",
    "\n",
    "df_treat = df_treat[columns_new_order]\n",
    "\n",
    "\n",
    "treated = df_treat[df_treat['treat'] == 1]\n",
    "control = df_treat[df_treat['treat'] == 0]\n",
    "                   \n",
    "print(f'Total number of accounts   ({INCR_MIN_RATIO} < ratio):\\t\\t {len(df_treat)}')\n",
    "print(f'Number of control accounts ({INCR_MIN_RATIO} < ratio < {CONTROL_MAX_RATIO}):\\t {len(control)}')\n",
    "print(f'Number of treated accounts ({TREATED_MIN_RATIO} < ratio):\\t\\t {len(treated)}')\n",
    "df_treat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(treated['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The treated group has:\n",
    "\n",
    "- lower mean delta videos value\n",
    "- higher first (25%) percentile\n",
    "- Some outliers of really high delta videos - with maximum income\n",
    "\n",
    "The control group has:\n",
    "- higher mean earnings value\n",
    "- higher percentile (50%,75%)\n",
    "- higher number of accounts with avg delta videos in the interval 3 - 9\n",
    "\n",
    "We conclude that, in general, the control group outperforms the treated one in most of the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A closer look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_explore_relationships = ['treat', 'd1', 'd2', 'ratio',\n",
    "        'bkpt_date', 'avg_patrons_bkpnt', \n",
    "        'mean_delta_patrons_befor',\n",
    "        'mean_delta_earnings_befor',\n",
    "        'mean_delta_videos_befor',\n",
    "        'mean_delta_views_befor',\n",
    "        'mean_delta_views_after',\n",
    "        'mean_delta_subs_befor',\n",
    "        'mean_duration_befor',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairwise relationships\n",
    "sns.pairplot(df_treat[columns_to_explore_relationships])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of patrons at breakpoint\n",
    "df_treat.boxplot(by='treat', column='avg_patrons_bkpnt', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "df_treat.boxplot(by='treat', column='mean_delta_patrons_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "df_treat.boxplot(by='treat', column='mean_delta_views_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propensity score model\n",
    "- Use logistic regression to estimate propensity scores for all points in the dataset. \n",
    "-  Use statsmodels to fit the logistic regression model and apply it to each data point to obtain propensity scores.\n",
    "\n",
    "The propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Treatment:**\n",
    "    - Increase of Patrons at breakpoint - converted to a binary variable as follows:\n",
    "        - `treat` = 1 (treatment group), if number of patrons increase ratio at breakpoint > threshold\n",
    "        - `treat` = 0 (control group), if number of patrons increase linearly (increase ratio btw 1 and 2)\n",
    "        \n",
    "- **Outcome**\n",
    "    - `mean_delta_videos_after`: YouTube delta views (post-treatment)\n",
    "    \n",
    "- **Observed covariates:**\n",
    "    - `mean_delta_videos_befor`: YouTube delta videos (pre-treatment) \n",
    "    - `mean_delta_views_befor`:  YouTube delta views (pre-treatment) \n",
    "    - `mean_delta_subs_befor`:   YouTube delta subs (pre-treatment) \n",
    "    - `mean_duration_befor`:     YouTube video duration (pre-treatment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = ['avg_patrons_bkpnt',\n",
    " 'avg_patrons_sub30',\n",
    " 'avg_patrons_add30',\n",
    " 'mean_delta_patrons_befor',\n",
    " 'mean_delta_patrons_after',\n",
    " 'mean_delta_earnings_befor',\n",
    " 'mean_delta_earnings_after',\n",
    " 'mean_delta_videos_befor',\n",
    " 'mean_delta_videos_after',\n",
    " 'mean_delta_views_befor',\n",
    " 'mean_delta_views_after',\n",
    " 'mean_delta_subs_befor',\n",
    " 'mean_delta_subs_after',\n",
    " 'mean_duration_befor',\n",
    " 'mean_likes_after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the continuous features\n",
    "for feature in continuous_features:\n",
    "    df_treat[feature] = (df_treat[feature] - df_treat[feature].mean())/df_treat[feature].std()\n",
    "\n",
    "df_treat_notna = df_treat.dropna(subset=['mean_delta_videos_befor', 'mean_delta_views_befor', 'mean_delta_subs_befor', 'mean_duration_befor']).copy()\n",
    "df_treat_notna = df_treat_notna.reset_index(drop=True)\n",
    "df_treat_notna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.logit(formula='treat ~  avg_patrons_bkpnt + mean_delta_videos_befor + mean_delta_views_befor + mean_delta_subs_befor + mean_duration_befor', data=df_treat_notna)\n",
    "\n",
    "res = mod.fit()\n",
    "\n",
    "# Extract the estimated propensity scores\n",
    "df_treat_notna['Propensity_score'] = res.predict()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the treatment and control groups\n",
    "treatment_df = df_treat_notna[df_treat_notna['treat'] == 1]\n",
    "control_df   = df_treat_notna[df_treat_notna['treat'] == 0]\n",
    "\n",
    "# Create an empty undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Loop through all the pairs of instances\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        # Calculate the similarity \n",
    "        similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                    treatment_row['Propensity_score'])\n",
    "\n",
    "        # Add an edge between the two instances weighted by the similarity between them\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "# Generate and return the maximum weight matching on the generated graph\n",
    "matching = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = df_treat_notna.iloc[matched]\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_balanced = balanced_df.loc[balanced_df['treat'] == 1] #People that attained the program\n",
    "control_balanced = balanced_df.loc[balanced_df['treat'] == 0] #People that didn't attain the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of control accounts ({INCR_MIN_RATIO} < ratio < {CONTROL_MAX_RATIO}):\\t {len(control_balanced)}')\n",
    "print(f'Number of treated accounts ({TREATED_MIN_RATIO} < ratio):\\t\\t {len(treated_balanced)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_balanced.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_balanced.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of patrons at breakpoint\n",
    "balanced_df.boxplot(by='treat', column='avg_patrons_bkpnt', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "balanced_df.boxplot(by='treat', column='mean_delta_patrons_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "balanced_df.boxplot(by='treat', column='mean_delta_views_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(balanced_df[columns_to_explore_relationships])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution BEFORE matching\n",
    "ax = sns.distplot(treated['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint, BEFORE matching', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution AFTER matching\n",
    "ax = sns.distplot(treated_balanced['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control_balanced['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint, AFTER matching', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
