{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Characterizing Patronage on YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libaries imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import zstandard\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "from tqdm.notebook import tqdm\n",
    "import timeit\n",
    "import ast\n",
    "import math\n",
    "import datetime\n",
    "import ruptures as rpt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import pickle\n",
    "import statsmodels.formula.api as smf\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder paths\n",
    "DATA_FOLDER = \"/dlabdata1/youtube_large/\"\n",
    "LOCAL_DATA_FOLDER = \"local_data/\"\n",
    "\n",
    "\n",
    "\n",
    "PATH_YT_METADATA_SRC = DATA_FOLDER+\"yt_metadata_en.jsonl.gz\"\n",
    "PATH_YT_METADATA_DST = LOCAL_DATA_FOLDER+\"yt_metadata_en_pt.tsv.gz\"\n",
    "PATH_YT_METADATA_UNIQUE_PT_DST = LOCAL_DATA_FOLDER+\"yt_metadata_en_unique_pt_per_chan.tsv.gz\"\n",
    "PATH_YT_METADATA_UNIQUE_YT_PT_DST = LOCAL_DATA_FOLDER+\"yt_metadata_en_unique_pt_yt.tsv.gz\"\n",
    "\n",
    "\n",
    "PATH_LINKED_CHANNELS_PATRONS = LOCAL_DATA_FOLDER+\"df_linked_channels_patreons.tsv.gz\"\n",
    "\n",
    "\n",
    "PATH_YT_TIMESERIES_SRC = DATA_FOLDER+\"df_timeseries_en.tsv.gz\"\n",
    "PATH_YT_TIMESERIES_DST = LOCAL_DATA_FOLDER+\"df_yt_timeseries_restricted.tsv.gz\"\n",
    "\n",
    "\n",
    "\n",
    "PATH_GT_TIMESERIES_DST = LOCAL_DATA_FOLDER+\"df_gt_timeseries_filtered.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list all files in DATA_FOLDER\n",
    "# !ls -lh {DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in LOCAL_DATA_FOLDER\n",
    "# !ls -lh {LOCAL_DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Link YouTube channels with Patreon accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files used in this section\n",
    "\n",
    "**YouNiverse dataset:**\n",
    "\n",
    "- (`df_channels_en.tsv.gz`: channel metadata.)\n",
    "- `df_timeseries_en.tsv.gz`: channel-level time-series.\n",
    "- `yt_metadata_en.jsonl.gz`: raw video metadata.\n",
    "\n",
    "**Graphteon dataset:**\n",
    "- `final_processed_file.jsonl.gz` all graphteon time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1. Filter YouTube metadata containing patreon id\n",
    "_Extract Patreon urls from YouTube metadata description (if they exist) and keep only those rows_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Patreon url from description and keep only those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_escape(str):\n",
    "    \"\"\"\n",
    "    replace new line special character by a space\n",
    "    \"\"\"\n",
    "    return str.replace(\"\\\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract patreon accounts from youtube channel descriptions and\n",
    "# # filter the metadata to retain only the rows which description contains a patreon url\n",
    "\n",
    "# # MAX_ITER = 10_000\n",
    "\n",
    "# nb_rows_read = 0\n",
    "# JSONDecodeErrors_cnt = 0 \n",
    "# lines_json = []    \n",
    "\n",
    "# # match patterns starting with patreon.com/ and matching at least 1 character after\n",
    "# # until it reaches anything thats not a word character\n",
    "# pattern = re.compile(r'patreon.com/[^\\W]+')\n",
    "\n",
    "# compressed_file_size = os.stat(PATH_YT_METADATA_SRC).st_size\n",
    "# print(\"Compressed file size is :                 {:>3,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "# uncompressed_file_size = 97_600_000_000\n",
    "# print(\"Estimated Uncompressed file size is :     {:>3,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "# start = timeit.default_timer()\n",
    "\n",
    "# # Load tqdm with size counter instead of file counter\n",
    "# with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "#     with gzip.open(PATH_YT_METADATA_SRC, \"r\") as f:\n",
    "#         for i, line_byte in enumerate(f): \n",
    "\n",
    "#             read_bytes = len(line_byte)\n",
    "#             if read_bytes:\n",
    "#                 pbar.set_postfix(file=PATH_YT_METADATA_SRC[len(DATA_FOLDER)+1:], refresh=False)\n",
    "#                 pbar.update(read_bytes)\n",
    "\n",
    "#             nb_rows_read += 1\n",
    "            \n",
    "#             # set a maximum iteration for tests\n",
    "#             # if nb_rows_read >= MAX_ITER:\n",
    "#             #     break\n",
    "\n",
    "#             # convert bytes into string\n",
    "#             line_str = line_byte.decode(\"utf-8\")\n",
    "\n",
    "#             # convert string into json after escaping new line characters\n",
    "#             line_str_esc = json_escape(line_str)\n",
    "#             try:\n",
    "#                 line_json = json.loads(line_str_esc)\n",
    "#             except Exception as e:\n",
    "#                 JSONDecodeErrors_cnt += 1\n",
    "#                 pass\n",
    "\n",
    "#             # print(line_json)\n",
    "#             # print(line_json['categories'])\n",
    "            \n",
    "#             # add line if description contains a patreon.com id\n",
    "#             if re.search(pattern, line_json['description']):\n",
    "#                 patreon_id = re.findall(pattern, line_json['description'])[0]\n",
    "#                 line_json['patreon_id'] = patreon_id\n",
    "#                 lines_json.append(line_json)\n",
    "\n",
    "# stop = timeit.default_timer()\n",
    "# time_diff = stop - start\n",
    "\n",
    "# print()\n",
    "# print(\"==> total time to read and filter youtube metadata:                {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "# print(\"==> number of rows read:                                           {:>10,}\".format(nb_rows_read))\n",
    "# print(\"==> number of videos containing a patreon link in the description: {:>10,} ({:.3%})\".format(len(lines_json), len(lines_json)/nb_rows_read ))\n",
    "# print(\"==> number of skipped rows (JSONDecodeErrors):                     {:>10,} ({:.3%})\".format(JSONDecodeErrors_cnt, JSONDecodeErrors_cnt/nb_rows_read))\n",
    "\n",
    "# # create new dataframe with the filtered lines\n",
    "# df_yt_metadata_pt = pd.DataFrame(data=lines_json, index=None)\n",
    "\n",
    "# # calculate memory usage of the new dataframe\n",
    "# mem_cons = df_yt_metadata_pt.memory_usage(index=True).sum()\n",
    "# print(\"==> memory usage of new (filtered) dataframe:                      {:12,.2f} GB ({:,} bytes)\".format(mem_cons / 2**30, mem_cons))\n",
    "\n",
    "\n",
    "\n",
    "# # remove rows where patreon_ids = patreon.com/posts or patreon.com/user (in the future fix in regex)\n",
    "# df_yt_metadata_pt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'] != 'patreon.com/posts']\n",
    "# df_yt_metadata_pt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'] != 'patreon.com/user']\n",
    "# df_yt_metadata_pt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'] != 'patreon.com/join']\n",
    "\n",
    "\n",
    "# # lowercase all patreon ids to avoid duplicates\n",
    "# df_yt_metadata_pt['patreon_id'] = df_yt_metadata_pt['patreon_id'].str.lower()\n",
    "\n",
    "# # save youtube metadata df containing patreon accounts\n",
    "# # df_yt_metadata_pt.to_csv(PATH_YT_METADATA_DST, index=False, sep='\\t', compression='gzip')\n",
    "# # !ls -lh {PATH_YT_METADATA_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Summary of results\n",
    "\n",
    "YT_metadata_filter_results_040422.jpg _(filter script in script/scripts.ipynb)_\n",
    "<div>\n",
    "    <img src=\"img/YT_metadata_filter_results_040422.jpg\" alt=\"YT_metadata_filter_results_040422.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restrict to 1 patreon id per youtube channel\n",
    "Some YouTube channels use multiple patreon accounts. We'll only keep the most used patreon account for each YouTube channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read youtube metadata file containing patreon accounts (takes about 2 mins)\n",
    "df_yt_metadata_pt = pd.read_csv(PATH_YT_METADATA_DST, sep=\"\\t\", lineterminator='\\n', compression='gzip') # takes about 2 mins\n",
    "df_yt_metadata_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original YT dataset\n",
    "\n",
    "# use if running script above\n",
    "# DF_YT_METADATA_ROWS = nb_rows_read \n",
    "\n",
    "# use if load df_yt_metadata_pt\n",
    "DF_YT_METADATA_ROWS = 72_924_794 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique patreon ids in df_yt_metadata_pt\n",
    "yt_patreon_list = df_yt_metadata_pt.patreon_id.unique()\n",
    "yt_pt_channel_list = df_yt_metadata_pt['channel_id'].unique()\n",
    "print(\"[Filtered YouTube metadata] total number of unique patreon ids:                       {:>9,}\".format(len(yt_patreon_list)))\n",
    "print(\"[Filtered YouTube metadata] number of unique channels that contain a patreon account: {:>9,}\".format(len(yt_pt_channel_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats \n",
    "print(\"[YouTube metadata] Total number of videos:                                                {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "print(\"[Filtered YouTube metadata] number of videos that contain a patreon link in description:  {:>10,} ({:.1%} of total dataset)\".format(len(df_yt_metadata_pt), len(df_yt_metadata_pt)/DF_YT_METADATA_ROWS))\n",
    "\n",
    "# get list of all unique patreon ids in df_yt_metadata_pt\n",
    "yt_patreon_list = df_yt_metadata_pt['patreon_id'].unique()\n",
    "yt_pt_channel_list = df_yt_metadata_pt['channel_id'].unique()\n",
    "\n",
    "print(\"[Filtered YouTube metadata] total number of unique patreon ids:                           {:>9,}\".format(len(yt_patreon_list)))\n",
    "print(\"[Filtered YouTube metadata] number of unique channels that contain a patreon account:     {:>9,}\".format(len(yt_pt_channel_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** \\\n",
    "We can see that we have _**more patreon ids than channels**_ . Let's investigate further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by channel_id AND patreon_id and count the number of unique videos (display_ids)\n",
    "df_yt_metadata_pt_grp_chan = df_yt_metadata_pt.groupby(['channel_id','patreon_id']).agg(display_id_cnt=(\"display_id\", pd.Series.nunique))\n",
    "df_yt_metadata_pt_grp_chan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_yt_metadata_pt_grp_chan = df_yt_metadata_pt_grp_chan.reset_index()\n",
    "# df_yt_metadata_pt_grp_chan.head(4)\n",
    "\n",
    "# count the number of patreon_ids per channel\n",
    "pt_id_cnt_pr_chan = df_yt_metadata_pt_grp_chan.groupby('channel_id').count()['patreon_id'].sort_values(ascending=False)\n",
    "pt_id_cnt_pr_chan = pt_id_cnt_pr_chan.to_frame(name='patreon_id_cnt')\n",
    "pt_id_cnt_pr_chan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Distribution of patreon ids per channel\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(6,4))\n",
    "\n",
    "# plot with log scale for x axis and log scale for y axis\n",
    "sns.histplot(data=pt_id_cnt_pr_chan, ax=axs, bins=50, kde=False, legend=False, color=f'C{0}')\n",
    "axs.set(title=f'Distribution of patreon ids per channel (log scale)')\n",
    "axs.set_xlabel(\"Number of patreon ids\")\n",
    "axs.set_ylabel(\"Count of channels (log scale)\")\n",
    "axs.set(yscale=\"log\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# descriptive statistics table\n",
    "pt_id_cnt_pr_chan.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "As we observed earlier, some channels use more than 1 patreon id, and use different patreon urls for different videos. For example:\n",
    "- [Patreon_Gaming](https://www.youtube.com/channel/UCAsLyFlWkbdhvri02tO6veA) uses 73 different patreon ids.\n",
    "- [Artistic Maniacs](https://www.youtube.com/channel/UC3pcSD6_RRisNLaHGznemJA) uses 69 different patreon ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for Artistic Maniacs\n",
    "df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan['channel_id'] == 'UC3pcSD6_RRisNLaHGznemJA'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Optional: Keep only most used patreon_id per channel (patreon_id with most videos for each channel)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort metadata df by diplay_id_cnt within each channel_id group\n",
    "df_yt_metadata_pt_grp_chan = df_yt_metadata_pt_grp_chan.sort_values(['channel_id','display_id_cnt'], ascending=[True, False])\n",
    "df_yt_metadata_pt_grp_chan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of duplicate of rows with same channel id but different patreon ids\n",
    "dup_chan_id = df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan.duplicated(subset=['channel_id'], keep='first')]\n",
    "print(\"Number of duplicate rows (same channel id with multiple patreon_ids): {:,}\".format(len(dup_chan_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan.duplicated('channel_id')].sort_values('channel_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows, keep the patreon ids with the most videos\n",
    "df_yt_metadata_unique_pt = df_yt_metadata_pt_grp_chan.drop_duplicates(subset='channel_id', keep='first')\n",
    "print('Removed {:,} rows'.format(len(df_yt_metadata_pt_grp_chan) - len(df_yt_metadata_unique_pt)))\n",
    "df_yt_metadata_unique_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_unique_pt_per_chan = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(df_yt_metadata_unique_pt['patreon_id'])]\n",
    "print(f\"removed {len(df_yt_metadata_unique_pt_per_chan) - len(df_yt_metadata_pt)} from dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"YouTube Metadata unique patreon account per channel\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "df_yt_metadata_unique_pt_per_chan.to_csv(PATH_YT_METADATA_UNIQUE_PT_DST, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_YT_METADATA_UNIQUE_PT_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restrict to 1 youtube channel per patreon id \n",
    "Some patreon accounts are on multiple YT channels. We'll remove those accounts \\\n",
    "Remove accounts with multiple youtube ids per patreon accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### NEW SECTION - Restrict to 1 youtube channel per patreon id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Observation:** \\\n",
    "When grouping YouTube metadata by `channel_id` and `patreon_id`, we also notice that we have more rows than the total number of unique patreon ids. \\\n",
    "This is because some `patreon_id` are used on multiple channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"total rows:                        {:,}\".format(len(df_yt_metadata_pt_grp_chan)))\n",
    "# print(\"total number of unique patreon ids {:,}\".format(df_yt_metadata_pt.patreon_id.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show patreon_id that are used on multiple channels.\n",
    "# df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan.duplicated(subset=['patreon_id'], keep=False)].sort_values(by='patreon_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[Filtered YouTube metadata] number of channels per patreon id:\")\n",
    "\n",
    "# chan_cnt_per_patreon_id = df_yt_metadata_pt.groupby('patreon_id')\\\n",
    "#                                             .agg(channel_id_count=('channel_id', 'count'))\\\n",
    "#                                             .sort_values(by=['channel_id_count'], ascending=False)\n",
    "# chan_cnt_per_patreon_id\n",
    "# # chan_cnt_per_patreon_id.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_unique_pt_per_chan.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove patreon accounts that have more than 1 youtube channel\n",
    "\n",
    "# Count YT channel_ids per patreon_id\n",
    "df_yt_metadata_pt_chan_id_cnt = df_yt_metadata_unique_pt_per_chan.groupby(['patreon_id','channel_id']).agg(channel_id_cnt=(\"channel_id\", pd.Series.nunique)).groupby('patreon_id').count().sort_values('channel_id_cnt', ascending=False)\n",
    "df_yt_metadata_pt_chan_id_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep patreon ids that have exactly 1 channel id only\n",
    "df_yt_metadata_pt_chan_id_cnt_unique_chan = df_yt_metadata_pt_chan_id_cnt[df_yt_metadata_pt_chan_id_cnt['channel_id_cnt']==1]\n",
    "\n",
    "print(f\"removed {len(df_yt_metadata_pt_chan_id_cnt) - len(df_yt_metadata_pt_chan_id_cnt_unique_chan)} accounts\")\n",
    "\n",
    "patreons_with_unique_chan = df_yt_metadata_pt_chan_id_cnt_unique_chan.index\n",
    "\n",
    "print(\"Number of patreon accounts with only 1 YT channel:\")\n",
    "patreons_with_unique_chan.size\n",
    "patreons_with_unique_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_yt_metadata_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_unique_pt_yt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(patreons_with_unique_chan)]\n",
    "print(f\"removed {len(df_yt_metadata_pt) - len(df_yt_metadata_unique_pt_yt)} from dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_unique_pt_yt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"YouTube Metadata unique patreon account per channel\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "df_yt_metadata_unique_pt_yt.to_csv(PATH_YT_METADATA_UNIQUE_YT_PT_DST, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_YT_METADATA_UNIQUE_YT_PT_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### END OF NEW SECTION - Restrict to 1 youtube channel per patreon id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Link\" dataframe (channel/patreon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider them linked only if \n",
    "- [TODO] there is a Patreon link >10% of their videos and if the second most common Patreon link occurs less than 2-3 videos.\n",
    "- [TODO] Remove channels whose patreon ids are not unique\n",
    "- Link YouTube channel to Patreon id which appears in most of its videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_unique_all = df_yt_metadata_unique_pt[df_yt_metadata_unique_pt['patreon_id'].isin(patreons_with_unique_chan)]\n",
    "print(f\"Removed {len(df_yt_metadata_unique_pt) - len(df_yt_metadata_unique_all)} accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store into new \"matched\" dataframe\n",
    "df_linked_channels_patreons = df_yt_metadata_unique_all[['channel_id', 'patreon_id']]\n",
    "df_linked_channels_patreons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_LINKED_CHANNELS_PATRONS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"linked\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "df_linked_channels_patreons.to_csv(PATH_LINKED_CHANNELS_PATRONS, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_LINKED_CHANNELS_PATRONS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save another version of YT metadata file\n",
    "\n",
    "PATH_YT_METADATA_UNIQUE_YT_PT_DST = LOCAL_DATA_FOLDER+\"yt_metadata_en_unique_pt_yt.tsv.gz\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [_Ignore for now_] Number of videos per patreon id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by patreon_id and count the number of unique display_ids\n",
    "vids_cnt_per_patreon_id = df_yt_metadata_pt.groupby('patreon_id').agg({\"display_id\": pd.Series.nunique}).sort_values(by='display_id', ascending=False)\n",
    "vids_cnt_per_patreon_id.rename(columns={'display_id':'display_id_cnt'}, inplace=True)\n",
    "\n",
    "print(\"[Filtered YouTube metadata] number of videos per patreon id:\")\n",
    "vids_cnt_per_patreon_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with linear scale for both axes\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(6,4))\n",
    "\n",
    "\n",
    "# plot with log scale for x axis and log scale for y axis\n",
    "sns.histplot(data=vids_cnt_per_patreon_id, ax=axs, bins=50, kde=False, color=f'C{0}')\n",
    "axs.set(title=f'Distribution of videos per patreon id (log scale)')\n",
    "axs.set_xlabel(\"Number of videos\")\n",
    "axs.set_ylabel(\"# patreon ids (log scale)\")\n",
    "axs.set(yscale=\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# descriptive statistics table\n",
    "vids_cnt_per_patreon_id.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "From the above graph and table, we can see that the _videos_ distributions among patreon ids follows a **power law**, meaning that most patreon accounts have a only a few videos, but a few of them have a lot of videos.\n",
    "\n",
    "More specifically:\n",
    "- 25% of the Patreon accounts have 1 video\n",
    "- 50% of the Patreon accounts have less than 4 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Filter YouTube timeseries - Restrict YouTube channels (4 filters)\n",
    "Restrict YouTube channels according to the following criteria (filters are applied sequentially):\n",
    "- Filter 1: Keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account \n",
    "- Filter 2: At least 2 year between first and last video\n",
    "- Filter 3: At least 20 videos with patreon ids\n",
    "- Filter 4: At least 250k subscribers at data crawling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_YT_TIMESERIES_SRC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load channel-level time-series. (takes about 50 secs)\n",
    "df_yt_timeseries = pd.read_csv(PATH_YT_TIMESERIES_SRC, sep=\"\\t\", compression='gzip', parse_dates=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global values for filters\n",
    "MIN_DAYS_DELTA = \"730 day\"    # filter 2\n",
    "NB_PATREON_VIDS = 20          # filter 3\n",
    "NB_SUBS = 250_000             # filter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of channels of original YT timeseries dataset (need to first load df_yt_timeseries in 1.1.2)\n",
    "yt_ts_uniq_chan_cnt = df_yt_timeseries['channel'].nunique()\n",
    "print(\"[YouTube Timeseries] Nb of rows of original dataset:                  {:>10,}\".format(len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of channels of original dataset:              {:>10,}\".format(yt_ts_uniq_chan_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **• Filter 1:** Keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter 1: retain only the YT channels that exist in the filtered YT metadata dataset (need to first load df_yt_metadata_pt and yt_pt_channel_list in 2.2.1)\n",
    "df_yt_timeseries_filt1 = df_yt_timeseries[df_yt_timeseries['channel'].isin(yt_pt_channel_list)]\n",
    "chan_list_filt1 = df_yt_timeseries_filt1['channel'].unique()\n",
    "chan_list_filt1_cnt = len(chan_list_filt1)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1:           {:>10,} ({:5.1%} of original dataset)\".format(len(df_yt_timeseries_filt1), len(df_yt_timeseries_filt1)/len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1:          {:>10,} ({:5.1%} of original dataset)\".format(chan_list_filt1_cnt, chan_list_filt1_cnt/yt_ts_uniq_chan_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **• Filter 2:** At least 2 year between first and last video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# among filter1 channels, calculate time difference between the first and the last video for each channel\n",
    "datetime_data = df_yt_timeseries_filt1.groupby('channel').agg(datetime_min=('datetime', 'min'),\n",
    "                                                              datetime_max=('datetime', 'max'))\n",
    "datetime_data['delta_datetime'] = datetime_data['datetime_max'] - datetime_data['datetime_min']\n",
    "\n",
    "# filter channels that we have data for at least MIN_TIME_DELTA days\n",
    "datetime_data_filt2 = datetime_data[datetime_data['delta_datetime'] > pd.Timedelta(MIN_DAYS_DELTA)]\n",
    "\n",
    "# Apply filter on YT Timeseries dataset: retain only those channels that have data for at least MIN_TIME_DELTA days\n",
    "df_yt_timeseries_filt2 = df_yt_timeseries_filt1[df_yt_timeseries_filt1['channel'].isin(datetime_data_filt2.index)]\n",
    "\n",
    "chan_list_filt2 = df_yt_timeseries_filt2['channel'].unique()\n",
    "chan_list_filt2_cnt = len(chan_list_filt2)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2:         {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 dataset)\".format(len(df_yt_timeseries_filt2), len(df_yt_timeseries_filt2)/len(df_yt_timeseries), len(df_yt_timeseries_filt2)/len(df_yt_timeseries_filt1)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2:        {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 channels)\".format(chan_list_filt2_cnt, chan_list_filt2_cnt/yt_ts_uniq_chan_cnt, chan_list_filt2_cnt/chan_list_filt1_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **• Filter 3:** At least 20 videos with patreon ids per channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by channel_id AND patreon_id and count the number of unique videos (=display_ids). (need to load df_yt_metadata_pt_grp_chan from point 2.2.1)\n",
    "# Then filter rows that have at least 20 videos (display_ids) \n",
    "df_yt_metadata_pt_grp_chan_filt3 = df_yt_metadata_pt_grp_chan[df_yt_metadata_pt_grp_chan['display_id_cnt'] > NB_PATREON_VIDS]\n",
    "df_yt_metadata_pt_grp_chan_filt3\n",
    "\n",
    "# get list of unique channels satisfying filter 3\n",
    "chan_list_filt_3 = df_yt_metadata_pt_grp_chan_filt3['channel_id'].unique()\n",
    "\n",
    "# Apply filter on YT Timeseries dataset: retain only those channels from filt 2 that are in the chan_list_filt_3\n",
    "df_yt_timeseries_filt3 = df_yt_timeseries_filt2[df_yt_timeseries_filt2['channel'].isin(chan_list_filt_3)]\n",
    "\n",
    "chan_list_filt3 = df_yt_timeseries_filt3['channel'].unique()\n",
    "chan_list_filt3_cnt = len(chan_list_filt3)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3:       {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 dataset)\".format(len(df_yt_timeseries_filt3), len(df_yt_timeseries_filt3)/len(df_yt_timeseries), len(df_yt_timeseries_filt3)/len(df_yt_timeseries_filt2)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3:      {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 channels)\".format(chan_list_filt3_cnt, chan_list_filt3_cnt/yt_ts_uniq_chan_cnt, chan_list_filt3_cnt/chan_list_filt2_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **• Filter 4:** At least 250k subscribers at data crawling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregates per channel\n",
    "subs_aggr_per_channel = df_yt_timeseries_filt3.groupby('channel')\\\n",
    "                                               .agg(min_subs=('subs', 'min'),\n",
    "                                                    max_subs=('subs', 'max'))\\\n",
    "                                                .sort_values(by=['max_subs'], ascending=False)\\\n",
    "                                                .reset_index()\n",
    "# subs_aggr_per_channel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to first load data_per_channel (aggregates per channel in 1.1.2 'Datetime points accross channels' section)\n",
    "subs_per_channel_filt4 = subs_aggr_per_channel[subs_aggr_per_channel['max_subs'] > NB_SUBS]\n",
    "\n",
    "# get list of unique channels satisfying filter 4\n",
    "chan_list_filt_4 = subs_per_channel_filt4['channel'].unique()\n",
    "\n",
    "# # Apply filter on YT Timeseries dataset: retain only those channels from filt_3 that are in the chan_list_filt_4\n",
    "df_yt_timeseries_filt4 = df_yt_timeseries_filt3[df_yt_timeseries_filt3['channel'].isin(chan_list_filt_4)]\n",
    "\n",
    "chan_list_filt4 = df_yt_timeseries_filt4['channel'].unique()\n",
    "chan_list_filt4_cnt = len(chan_list_filt4)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3+4:     {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 dataset)\".format(len(df_yt_timeseries_filt4), len(df_yt_timeseries_filt4)/len(df_yt_timeseries), len(df_yt_timeseries_filt4)/len(df_yt_timeseries_filt3)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3+4:    {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 channels)\".format(chan_list_filt4_cnt, chan_list_filt4_cnt/yt_ts_uniq_chan_cnt, chan_list_filt4_cnt/chan_list_filt3_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **• Filter 4b**: At least 50k subscribers in the first 6 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "**• Filters summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[YouTube Timeseries] Stats before and after filters:\")\n",
    "print()\n",
    "\n",
    "print(\"Filter 1 = \\\"keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account\\\"\")\n",
    "print(\"Filter 2 = \\\"at least {:.1f} years ({} days) between first and last video\\\"\".format(pd.Timedelta(MIN_DAYS_DELTA).days/365, pd.Timedelta(MIN_DAYS_DELTA).days))\n",
    "print(\"Filter 3 = \\\"at least {:,} videos with patreon ids per channel\\\"\".format(NB_PATREON_VIDS))\n",
    "print(\"Filter 4 = \\\"at least {:,} subscribers at data crawling time\\\"\".format(NB_SUBS))\n",
    "print()\n",
    "print(\"[YouTube Timeseries] Nb of rows of original dataset:                  {:>10,}\".format(len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1:           {:>10,} ({:5.1%} of original dataset)\".format(len(df_yt_timeseries_filt1), len(df_yt_timeseries_filt1)/len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2:         {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 dataset)\".format(len(df_yt_timeseries_filt2), len(df_yt_timeseries_filt2)/len(df_yt_timeseries), len(df_yt_timeseries_filt2)/len(df_yt_timeseries_filt1)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3:       {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 dataset)\".format(len(df_yt_timeseries_filt3), len(df_yt_timeseries_filt3)/len(df_yt_timeseries), len(df_yt_timeseries_filt3)/len(df_yt_timeseries_filt2)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3+4:     {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 dataset)\".format(len(df_yt_timeseries_filt4), len(df_yt_timeseries_filt4)/len(df_yt_timeseries), len(df_yt_timeseries_filt4)/len(df_yt_timeseries_filt3)))\n",
    "print()\n",
    "print(\"[YouTube Timeseries] Nb of channels of original dataset:              {:>10,}\".format(yt_ts_uniq_chan_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1:          {:>10,} ({:5.1%} of original dataset)\".format(chan_list_filt1_cnt, chan_list_filt1_cnt/yt_ts_uniq_chan_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2:        {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 channels)\".format(chan_list_filt2_cnt, chan_list_filt2_cnt/yt_ts_uniq_chan_cnt, chan_list_filt2_cnt/chan_list_filt1_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3:      {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 channels)\".format(chan_list_filt3_cnt, chan_list_filt3_cnt/yt_ts_uniq_chan_cnt, chan_list_filt3_cnt/chan_list_filt2_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3+4:    {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 channels)\".format(chan_list_filt4_cnt, chan_list_filt4_cnt/yt_ts_uniq_chan_cnt, chan_list_filt4_cnt/chan_list_filt3_cnt))\n",
    "print()\n",
    "print('[YouTube Timeseries] Time range of original dataset                   {} and {}'.format(df_yt_timeseries['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                              df_yt_timeseries['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "print('[YouTube Timeseries] Time range after applying filter 1+2+3+4        {} and {}'.format(df_yt_timeseries_filt4['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                              df_yt_timeseries_filt4['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "display(df_yt_timeseries_filt4.head())\n",
    "print(\"Restricted list of channels after 4 filters (count = {:,}):\".format(chan_list_filt4_cnt))\n",
    "print(chan_list_filt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh {LOCAL_DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries_restricted = df_yt_timeseries_filt4.copy()\n",
    "\n",
    "# save youtube restricted timeseries df\n",
    "df_yt_timeseries_restricted.to_csv(PATH_YT_TIMESERIES_DST, index=False, sep='\\t', compression='gzip')\n",
    "!ls -lh {PATH_YT_TIMESERIES_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[ignore] Match patreon_ids and channel_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter YT metadata dataset by list of filtered channels from YT timeseries above\n",
    "# df_yt_metadata_pt_restr = df_yt_metadata_pt[df_yt_metadata_pt['channel_id'].isin(chan_list_filt4)]\n",
    "\n",
    "# # get unique channels for youtube metadata (original and restricted)\n",
    "# yt_metadata_uniq_chan = df_yt_metadata_pt['channel_id'].unique()\n",
    "# yt_metadata_uniq_chan_restr = df_yt_metadata_pt_restr['channel_id'].unique()\n",
    "\n",
    "# # get unique patreon ids for youtube metadata (original and restricted)\n",
    "# yt_metadata_uniq_pat = df_yt_metadata_pt['patreon_id'].unique()\n",
    "# yt_metadata_uniq_pat_restr = df_yt_metadata_pt_restr['patreon_id'].unique()\n",
    "\n",
    "# print(\"[YouTube Metadata]:\")\n",
    "# print()\n",
    "# print(\"Restriction = \\\"keep only YouTube channels that are in YouTube Timeseries filtered (filters 1-4) dataset\\\"\")\n",
    "# print()\n",
    "# # print(\"[YouTube Metadata] Nb of videos in original dataset:                                   {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "# # print(\"[YouTube Metadata] Nb of videos in pre-filtered (containing patreon id) dataset:       {:>10,}\".format(len(df_yt_metadata_pt)))\n",
    "# # print(\"[YouTube Metadata] Nb of videos after filtering by restricted channels:                {:>10,} ({:5.1%} of pre-filtered dataset dataset)\".format(len(df_yt_metadata_pt_restr), len(df_yt_metadata_pt_restr)/len(df_yt_metadata_pt)))\n",
    "# # print()\n",
    "# print(\"[YouTube Metadata] Nb of channels in pre-filtered (containing patreon id) dataset:     {:>10,}\".format(len(yt_metadata_uniq_chan)))\n",
    "# print(\"[YouTube Metadata] Nb of channels after filtering by restricted channels:              {:>10,} ({:5.1%} of pre-filtered dataset dataset)\".format(len(yt_metadata_uniq_chan_restr), len(yt_metadata_uniq_chan_restr)/len(yt_metadata_uniq_chan)))\n",
    "# print()\n",
    "# print(\"[YouTube Metadata] Nb of patreon ids in pre-filtered (containing patreon id) dataset:  {:>10,}\".format(len(yt_metadata_uniq_pat)))\n",
    "# print(\"[YouTube Metadata] Nb of patreon ids after filtering by restricted channels:           {:>10,} ({:5.1%} of pre-filtered dataset dataset)\".format(len(yt_metadata_uniq_pat_restr), len(yt_metadata_uniq_pat_restr)/len(yt_metadata_uniq_pat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.3 Filter Graphtreon to keep only the ones matching patreon id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GT_timeseries_filter_results_032622.jpg _(filter script in scripts/scripts.ipynb)_\n",
    "<div>\n",
    "    <img src=\"img/GT_timeseries_filter_results_032622.jpg\" alt=\"GT_timeseries_filter_results_032622.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original GT dataset\n",
    "GT_final_processed_file_ROWS = 232_269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_GT_TIMESERIES_DST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_filtered = pd.read_csv(PATH_GT_TIMESERIES_DST, sep=\"\\t\", compression='gzip')\n",
    "# df_gt_timeseries_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistics of loaded pre-filtered Graphtreon Timeseries file:\")\n",
    "print(\"[Graphtreon Timeseries] Total number of patreon ids:                                                   {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids that exist in both GT Timeseries and YT metadata:            {:>9,} ({:.1%} of GT timeseries dataset)\".format(len(df_gt_timeseries_filtered), len(df_gt_timeseries_filtered)/GT_final_processed_file_ROWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.3.1. Join GT timeseries with matched channel_id\n",
    "Add corresponding YT channel id to dataframe \\\n",
    "(match the channels in the restricted list of channels of the matched dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linked_channels_patreons.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_filtered.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join GT timeseries and matched channels\n",
    "df_gt_timeseries_merged = df_gt_timeseries_filtered.merge(df_linked_channels_patreons, left_on='patreon', right_on='patreon_id')\n",
    "df_gt_timeseries_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.3.2. Filter/Restrict GT timeseries further\n",
    "We now want to reduce the Graphtreon dataset by keeping only rows in filtered list of channels (chan_list_filt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter Graphtreon dataset by keeping only rows in filtered list of channels (chan_list_filt4)\n",
    "df_gt_timeseries_restricted = df_gt_timeseries_merged[df_gt_timeseries_merged['channel_id'].isin(chan_list_filt4)]\n",
    "\n",
    "print(\"[Graphtreon Timeseries] Total number of patreon ids:                                                   {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids that exist in both GT Timeseries and YT metadata:            {:>9,} ({:.1%} of GT timeseries dataset)\".format(len(df_gt_timeseries_filtered), len(df_gt_timeseries_filtered)/GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids that exist in both GT Timeseries and YT metadata restricted  {:>9,} ({:.1%} of GT timeseries dataset)\".format(len(df_gt_timeseries_restricted), len(df_gt_timeseries_restricted)/GT_final_processed_file_ROWS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Extract the date and daily earnings per patreon account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique patreon ids in df_gt_timeseries_restricted\n",
    "yt_gt_patreon_list_restricted = df_gt_timeseries_restricted.patreon.unique()\n",
    "print(\"list of restricted patreon ids\", yt_gt_patreon_list_restricted)\n",
    "print(\"number of restricted patreon ids\", len(yt_gt_patreon_list_restricted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_restricted.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_restricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of NaN value\n",
    "# df_gt_timeseries_sample[df_gt_timeseries_sample['creatorName'] == 'Comedy Trap House']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From the Graphtreon dataset, for each channel, extract the date and earnings from “dailyGraph_earningsSeriesData” (takes about 3 mins)\n",
    "# input_file_path = DATA_FOLDER+\"/final_processed_file.jsonl.gz\"\n",
    "\n",
    "# MAX_ITER = 100\n",
    "\n",
    "# nb_rows_read = 0\n",
    "# valid_predicate_count = 0\n",
    "# JSONDecodeErrors_cnt = 0 \n",
    "# dailyEarningsError_cnt = 0 \n",
    "# lines_json = []    \n",
    "\n",
    "# compressed_file_size = os.stat(input_file_path).st_size\n",
    "# print(\"Compressed file size is :                 {:>8,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "# uncompressed_file_size = 13_310_000_000\n",
    "# print(\"Estimated Uncompressed file size is :     {:>8,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "# start = timeit.default_timer()\n",
    "\n",
    "# # Load tqdm with size counter instead of file counter\n",
    "# with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "#     with gzip.open(input_file_path, \"r\") as f:\n",
    "#         for i, line in enumerate(f): \n",
    "\n",
    "#             read_bytes = len(line)\n",
    "#             if read_bytes:\n",
    "#                 pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "#                 pbar.update(read_bytes)\n",
    "\n",
    "#             nb_rows_read += 1\n",
    "            \n",
    "#             # set a maximum iteration for tests\n",
    "#             if nb_rows_read >= MAX_ITER:\n",
    "#                 break\n",
    "    \n",
    "#             try:\n",
    "#                 line_json = json.loads(line)\n",
    "#             except Exception as e:\n",
    "#                 JSONDecodeErrors_cnt += 1\n",
    "#                 continue\n",
    "                \n",
    "#             # add line if patreon id is exists in df_yt_metadata_pt\n",
    "#             if line_json['patreon'] in yt_gt_patreon_list_restricted:\n",
    "#                 valid_predicate_count += 1\n",
    "                \n",
    "#                 # Use ast.literal_eval to convert string of lists, to list of list\n",
    "#                 dailyGraph_earningsSeriesData = line_json.get('dailyGraph_earningsSeriesData')\n",
    "                \n",
    "#                 if dailyGraph_earningsSeriesData:\n",
    "#                     daily_earnings = ast.literal_eval(dailyGraph_earningsSeriesData)\n",
    "#                 else:\n",
    "#                     daily_earnings = [[np.nan, np.nan]]\n",
    "                                            \n",
    "#                 for daily_earning in daily_earnings:\n",
    "#                     # case where there are multiple tuples per row\n",
    "#                     if isinstance(daily_earning, list):\n",
    "#                         date = daily_earning[0]\n",
    "#                         earning = daily_earning[1]\n",
    "#                         lines_json.append({\n",
    "#                             'creatorName':   line_json.get('creatorName'), \n",
    "#                             'creatorRange':  line_json.get('creatorRange'), \n",
    "#                             'startDate':     line_json.get('startDate'),\n",
    "#                             'categoryTitle': line_json.get('categoryTitle'),\n",
    "#                             'patreon':       line_json.get('patreon'),\n",
    "#                             'date':          date,\n",
    "#                             'earning':       earning\n",
    "#                         })\n",
    "#                     else:\n",
    "#                         dailyEarningsError_cnt += 1\n",
    "#                         print(\">>>> dailyEarningsError - skipped line value: \")\n",
    "#                         print(line_json.get('creatorName'), line_json.get('creatorRange'), line_json.get('startDate'), line_json.get('categoryTitle'), line_json.get('patreon'), daily_earnings)\n",
    "\n",
    "# stop = timeit.default_timer()\n",
    "# time_diff = stop - start\n",
    "\n",
    "# print()\n",
    "# print(\"==> total time to read and filter graphtreon time series:                      {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "# print(\"==> number of rows read:                                                       {:>10,}\".format(nb_rows_read))\n",
    "# print(\"==> number of patreon ids that exist in both GTts and restricted YT metadata:  {:>10,} ({:.2%})\".format(valid_predicate_count, valid_predicate_count/nb_rows_read ))\n",
    "# print(\"==> number of skipped rows (JSONDecodeErrors):                                 {:>10,}\".format(JSONDecodeErrors_cnt))\n",
    "# print(\"==> number of skipped rows (dailyEarningsError):                               {:>10,}\".format(dailyEarningsError_cnt))\n",
    "\n",
    "# # create new dataframe with the filtered lines\n",
    "# df_dailyGraph_earningsSeries = pd.DataFrame(data=lines_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GT_timeseries_date_earnings_extract_040422.jpg _(filter script above)_\n",
    "<div>\n",
    "    <img src=\"img/GT_timeseries_date_earnings_extract_040422.jpg\" alt=\"GT_timeseries_date_earnings_extract_040422.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values\n",
    "# df_dailyGraph_earningsSeries[df_dailyGraph_earningsSeries.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv (5.3Mb)\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"dailyGraph_earningsSeries.tsv.gz\"\n",
    "# df_dailyGraph_earningsSeries.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Extract the date and daily patrons per patreon account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From the Graphtreon dataset, for each channel, extract the date and patrons from “dailyGraph_patronSeriesData” (takes about 3 mins)\n",
    "# input_file_path = DATA_FOLDER+\"/final_processed_file.jsonl.gz\"\n",
    "\n",
    "# MAX_ITER = 1000\n",
    "\n",
    "# nb_rows_read = 0\n",
    "# valid_predicate_count = 0\n",
    "# JSONDecodeErrors_cnt = 0 \n",
    "# dailyPatronsError_cnt = 0 \n",
    "# lines_json = []    \n",
    "\n",
    "# compressed_file_size = os.stat(input_file_path).st_size\n",
    "# print(\"Compressed file size is :                 {:>8,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "# uncompressed_file_size = 13_310_000_000\n",
    "# print(\"Estimated Uncompressed file size is :     {:>8,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "# start = timeit.default_timer()\n",
    "\n",
    "# # Load tqdm with size counter instead of file counter\n",
    "# with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "#     with gzip.open(input_file_path, \"r\") as f:\n",
    "#         for i, line in enumerate(f): \n",
    "\n",
    "#             read_bytes = len(line)\n",
    "#             if read_bytes:\n",
    "#                 pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "#                 pbar.update(read_bytes)\n",
    "\n",
    "#             nb_rows_read += 1\n",
    "            \n",
    "#             # set a maximum iteration for tests\n",
    "#             if nb_rows_read >= MAX_ITER:\n",
    "#                 break\n",
    "    \n",
    "#             try:\n",
    "#                 line_json = json.loads(line)\n",
    "#             except Exception as e:\n",
    "#                 JSONDecodeErrors_cnt += 1\n",
    "#                 continue\n",
    "                \n",
    "#             # add line if patreon id is exists in df_yt_metadata_pt\n",
    "#             if line_json['patreon'] in yt_gt_patreon_list_restricted:\n",
    "#                 valid_predicate_count += 1\n",
    "                \n",
    "#                 # Use ast.literal_eval to convert string of lists, to list of list\n",
    "#                 dailyGraph_patronSeriesData = line_json.get('dailyGraph_patronSeriesData')\n",
    "                \n",
    "#                 if dailyGraph_patronSeriesData:\n",
    "#                     daily_patrons = ast.literal_eval(dailyGraph_patronSeriesData)\n",
    "#                 else:\n",
    "#                     daily_patrons = [[np.nan, np.nan]]\n",
    "                                            \n",
    "#                 for daily_patron in daily_patrons:\n",
    "#                     # case where there are multiple tuples per row\n",
    "#                     if isinstance(daily_patron, list):\n",
    "#                         date = daily_patron[0]\n",
    "#                         patrons = daily_patron[1]\n",
    "#                         lines_json.append({\n",
    "#                             'creatorName':   line_json.get('creatorName'), \n",
    "#                             'creatorRange':  line_json.get('creatorRange'), \n",
    "#                             'startDate':     line_json.get('startDate'),\n",
    "#                             'categoryTitle': line_json.get('categoryTitle'),\n",
    "#                             'patreon':       line_json.get('patreon'),\n",
    "#                             'date':          date,\n",
    "#                             'patrons':       patrons\n",
    "#                         })\n",
    "#                     else:\n",
    "#                         dailyPatronsError_cnt += 1\n",
    "#                         print(\">>>> dailyPatronsError - skipped line value: \")\n",
    "#                         print(line_json.get('creatorName'), line_json.get('creatorRange'), line_json.get('startDate'), line_json.get('categoryTitle'), line_json.get('patreon'), daily_patrons)\n",
    "\n",
    "# stop = timeit.default_timer()\n",
    "# time_diff = stop - start\n",
    "\n",
    "# print()\n",
    "# print(\"==> total time to read and filter graphtreon time series:                      {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "# print(\"==> number of rows read:                                                       {:>10,}\".format(nb_rows_read))\n",
    "# print(\"==> number of patreon ids that exist in both GTts and restricted YT metadata:  {:>10,} ({:.2%})\".format(valid_predicate_count, valid_predicate_count/nb_rows_read ))\n",
    "# print(\"==> number of skipped rows (JSONDecodeErrors):                                 {:>10,}\".format(JSONDecodeErrors_cnt))\n",
    "# print(\"==> number of skipped rows (dailyPatronsError):                               {:>10,}\".format(dailyPatronsError_cnt))\n",
    "\n",
    "# # create new dataframe with the filtered lines\n",
    "# df_dailyGraph_patronsSeries = pd.DataFrame(data=lines_json)\n",
    "# df_dailyGraph_patronsSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GT_timeseries_date_patrons_extract_042922.jpg _(filter script above)_\n",
    "<div>\n",
    "    <img src=\"img/GT_timeseries_date_patrons_extract_042922.jpg\" alt=\"GT_timeseries_date_patrons_extract_042922.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values\n",
    "# df_dailyGraph_patronsSeries[df_dailyGraph_patronsSeries.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv (7.1Mb)\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"dailyGraph_patronsSeries.tsv.gz\"\n",
    "# df_dailyGraph_patronsSeries.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Merge extracted times series of daily earnings and daily patrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dailyGraph_earningsSeries.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dailyGraph_earningsSeries file from disk and convert dates\n",
    "df_dailyGraph_earningsSeries = pd.read_csv(LOCAL_DATA_FOLDER+\"dailyGraph_earningsSeries.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "# df_dailyGraph_earningsSeries.date = pd.to_datetime(df_dailyGraph_earningsSeries.date, unit='ms')\n",
    "df_dailyGraph_earningsSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dailyGraph_patronsSeries.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dailyGraph_patronsSeries from disk and convert dates\n",
    "df_dailyGraph_patronsSeries = pd.read_csv(LOCAL_DATA_FOLDER+\"dailyGraph_patronsSeries.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "# df_dailyGraph_patronsSeries.date = pd.to_datetime(df_dailyGraph_patronsSeries.date, unit='ms')\n",
    "df_dailyGraph_patronsSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dailyGraph_earningsSeries with df_dailyGraph_patronsSeries\n",
    "df_dailyGraph_patrons_and_earnings_Series = df_dailyGraph_earningsSeries.merge(df_dailyGraph_patronsSeries, how='outer')\n",
    "\n",
    "# convert patrons column to Int64 so it can hold NaN values after outer join\n",
    "df_dailyGraph_patrons_and_earnings_Series['patrons'] = df_dailyGraph_patrons_and_earnings_Series['patrons'].astype('Int64')\n",
    "df_dailyGraph_patrons_and_earnings_Series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv (6.2Mb)\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"dailyGraph_patrons_and_earnings_Series.tsv.gz\"\n",
    "# df_dailyGraph_patrons_and_earnings_Series.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}dailyGraph_patrons_and_earnings_Series.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read merged dailyGraph_patrons_and_earnings_Series from disk\n",
    "df_dailyGraph_patrons_and_earnings_Series = pd.read_csv(LOCAL_DATA_FOLDER+\"dailyGraph_patrons_and_earnings_Series.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_dailyGraph_patrons_and_earnings_Series['date'] = pd.to_datetime(df_dailyGraph_patrons_and_earnings_Series['date'], unit='ms')\n",
    "df_dailyGraph_patrons_and_earnings_Series['patrons'] = df_dailyGraph_patrons_and_earnings_Series['patrons'].astype('Int64')\n",
    "\n",
    "print(df_dailyGraph_patrons_and_earnings_Series.dtypes)\n",
    "df_dailyGraph_patrons_and_earnings_Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.4.1 Plot Patreon Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = mdates.YearLocator()\n",
    "months = mdates.MonthLocator()\n",
    "years_fmt = mdates.DateFormatter('%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-declare global variable for size of original GT dataset\n",
    "GT_final_processed_file_ROWS = 232_269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restrict patreons accounts (>100 patrons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_CNT = 863\n",
    "MIN_MAX_PATRONS = 100\n",
    "# group by patreon account, sort by max number of patrons\n",
    "dailyGraph_grp_patreon = df_dailyGraph_patrons_and_earnings_Series.groupby('patreon')\\\n",
    "                                                     .agg(date_cnt=('date', 'count'),\n",
    "                                                          earliest_date=('date', 'min'),\n",
    "                                                          lastest_date=('date', 'max'),\n",
    "                                                          daily_earning_mean=('earning', 'mean'),\n",
    "                                                          daily_earning_max=('earning', 'max'),\n",
    "                                                          daily_patrons_mean=('patrons', 'mean'),\n",
    "                                                          daily_patrons_max=('patrons', 'max'))\\\n",
    "                                                     .sort_values(by=['daily_patrons_max'], ascending=False)\\\n",
    "                                                     .round(2)\n",
    "\n",
    "# remove patreon accounts with no earnings data at all\n",
    "dailyGraph_grp_patreon = dailyGraph_grp_patreon[dailyGraph_grp_patreon['daily_earning_mean'].notna()]\n",
    "dailyGraph_grp_patreon = dailyGraph_grp_patreon.reset_index()\n",
    "\n",
    "# remove hours from dates\n",
    "# dailyGraph_grp_patreon.earliest_date = dailyGraph_grp_patreon.earliest_date.dt.date\n",
    "# dailyGraph_grp_patreon.lastest_date = dailyGraph_grp_patreon.lastest_date.dt.date\n",
    "\n",
    "# remove patrons accounts that have less than 50 patrons\n",
    "top_patreons = dailyGraph_grp_patreon[dailyGraph_grp_patreon['daily_patrons_max'] > MIN_MAX_PATRONS]['patreon']\n",
    "\n",
    "# extract the top most profitable patreon accounts\n",
    "# top_patreons = dailyGraph_grp_patreon[:TOP_CNT]['patreon']\n",
    "\n",
    "\n",
    "\n",
    "print(\"[Graphtreon Timeseries] Total number of patreon ids (original file):                      {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids in dailyGraph patreon + earnings time series:   {:>9,} ({:.1%} of original dataset)\".format(len(dailyGraph_grp_patreon), len(dailyGraph_grp_patreon)/GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Filtered \\\"top\\\" patrons (> {} patrons):                           {:>9,} ({:.1%} of original dataset)\".format(MIN_MAX_PATRONS, len(top_patreons), len(top_patreons)/GT_final_processed_file_ROWS))\n",
    "\n",
    "print()\n",
    "\n",
    "dailyGraph_grp_patreon[:10].style.set_caption(f\"Top {len(top_patreons)} Patreon accounts (sorted by max patrons)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_pt_accts = df_dailyGraph_patrons_and_earnings_Series[df_dailyGraph_patrons_and_earnings_Series['patreon'].isin(top_patreons)]\n",
    "df_top_pt_accts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "\n",
    "TOP_CNT_local = TOP_CNT\n",
    "TOP_CNT_local = 4\n",
    "# plot Patreon daily earningsSeriesData for top patreon accounts\n",
    "fig, axs = plt.subplots(math.ceil(TOP_CNT_local/2), 2, figsize=(12, TOP_CNT_local*1.2), sharey=False, sharex=False)\n",
    "for idx, patreon in tqdm(enumerate(top_patreons[:TOP_CNT_local])):\n",
    "    row = math.floor(idx/2)\n",
    "    col = idx % 2\n",
    "    ax1 = axs[row, col]\n",
    "    \n",
    "    # ax1.scatter(x[:4], y[:4], s=10, c='b', marker=\"s\", label='first')\n",
    "    # ax1.scatter(x[40:],y[40:], s=10, c='r', marker=\"o\", label='second')\n",
    "\n",
    "    tmp_df = df_top_pt_accts[df_top_pt_accts['patreon'] == patreon]\n",
    "\n",
    "    # sbplt = axs[idx, 0]\n",
    "    \n",
    "\n",
    "    color = 'tab:blue'\n",
    "    patrons, = ax1.plot(tmp_df['date'], tmp_df['patrons'], color=color, label='patrons')\n",
    "    ax1.set_ylabel('# Patrons', color=color) \n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set(title=patreon)\n",
    "    \n",
    "    \n",
    "    color = 'tab:orange'\n",
    "    ax2 = ax1.twinx()  # Create a twin Axes sharing the xaxis.\n",
    "    earnings, = ax2.plot(tmp_df['date'], tmp_df['earning'], color=color, label='earnings')\n",
    "    ax2.set_ylabel(\"Earnings per month\", color=color) \n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax1.xaxis.set_major_locator(years)\n",
    "    ax1.xaxis.set_major_formatter(years_fmt)\n",
    "    ax1.xaxis.set_minor_locator(months)\n",
    "    # ax1.legend(handles=[earnings, patrons], loc='upper left');\n",
    "    \n",
    "fig.suptitle(f'Timeseries of the top {TOP_CNT_local} Patreon accounts (most subscribers) \\n', fontweight=\"bold\")\n",
    "fig.text(0.5,0, 'Month')\n",
    "# fig.text(0,0.5, 'Earnings per month ($)', rotation = 90)\n",
    "fig.tight_layout(pad=3, w_pad=5, h_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "We can see a drop of income at the beginning of each month which is due to people unsubscribing (we will average it out later with a rolling average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter YT timeseries channels matching top patreon accounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matching dataframe\n",
    "df_linked_channels_patreons = pd.read_csv(PATH_LINKED_CHANNELS_PATRONS, sep=\"\\t\", compression=\"gzip\")\n",
    "df_linked_channels_patreons.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add patreon_id column to YT timeseries\n",
    "df_yt_timeseries_filt4_merged = df_yt_timeseries_filt4.merge(df_linked_channels_patreons, left_on='channel', right_on='channel_id')\n",
    "df_yt_timeseries_filt4_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter YT channels matching top patreon accounts\n",
    "df_yt_timeseries_top_pt = df_yt_timeseries_filt4_merged[df_yt_timeseries_filt4_merged['patreon_id'].isin(top_patreons)].copy()\n",
    "\n",
    "# replace dates that were collected after 23:00 to their next day, and remove hour\n",
    "df_yt_timeseries_top_pt['datetime_original'] = df_yt_timeseries_top_pt['datetime']\n",
    "df_yt_timeseries_top_pt['datetime'] = df_yt_timeseries_top_pt['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "\n",
    "# remove hours and convert to datetime type\n",
    "df_yt_timeseries_top_pt['datetime'] = pd.to_datetime(df_yt_timeseries_top_pt['datetime'].dt.date)\n",
    "    \n",
    "\n",
    "print('[YouTube Timeseries] Time range after applying filter 1+2+3+4              {} and {}'.format(df_yt_timeseries_filt4_merged['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                                                                    df_yt_timeseries_filt4_merged['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "print('[YouTube Timeseries] Time range after matching top patreon accounts        {} and {}'.format(df_yt_timeseries_top_pt['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                                                                    df_yt_timeseries_top_pt['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "print()\n",
    "top_yt_patreons = df_yt_timeseries_top_pt.patreon_id.unique()\n",
    "print(f\"Number of YouTube channels before matching with top patreon accounts: {len(df_yt_timeseries_filt4_merged.patreon_id.unique()):>5}\" )\n",
    "print(f\"Number of top patreon accounts:                                       {len(top_patreons):>5}\" )\n",
    "print(f\"Number of YouTube channels after  matching with top patreon accounts: {len(top_yt_patreons):>5}\" )\n",
    "df_yt_timeseries_top_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries_top_pt.groupby(['patreon_id', 'channel_id'])\\\n",
    "                                                     .agg(datetime_cnt=('datetime', 'count'),\n",
    "                                                          date_min=('datetime', 'min'),\n",
    "                                                          date_max=('datetime', 'max'),\n",
    "                                                          views_max=('views', 'max'),\n",
    "                                                          subs_date=('subs', 'max'),\n",
    "                                                          videos_max=('videos', 'mean'))\\\n",
    "                                                     .sort_values(by=['videos_max'], ascending=False)\n",
    "                                                     #      \\\n",
    "                                                     # .reset_index()\\\n",
    "                                                     # .round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Compare YouTube and top Patreon timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remove patreon accounts having more than 1 YT channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove patreon accounts that have more than 1 youtube channel\n",
    "df_yt_timeseries_top_pt_chan_id_cnt = df_yt_timeseries_top_pt.groupby(['patreon_id','channel_id']).agg(channel_id_cnt=(\"channel_id\", pd.Series.nunique))\n",
    "df_yt_timeseries_top_pt_chan_id_cnt = df_yt_timeseries_top_pt_chan_id_cnt.groupby('patreon_id').count()\n",
    "df_yt_timeseries_top_pt_unique_chan = df_yt_timeseries_top_pt_chan_id_cnt[df_yt_timeseries_top_pt_chan_id_cnt['channel_id_cnt']==1]\n",
    "\n",
    "top_patreons_unique_chan = df_yt_timeseries_top_pt_unique_chan.index\n",
    "\n",
    "print(\"Number of patreon accounts with only 1 YT channel:\")\n",
    "top_patreons_unique_chan.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter YT metadata channels that match top Patreon accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YT metadata containing patreon ids in description (already loaded in 2.1)\n",
    "!ls -lh {LOCAL_DATA_FOLDER}yt_metadata_en_pt_040422.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter accounts that match selected Patreon ids\n",
    "df_yt_metadata_pt_filtered = df_yt_metadata_pt\n",
    "df_yt_metadata_pt_filtered = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(top_patreons_unique_chan)].copy()\n",
    "print(f'Filter accounts that match top Patreon ids: {len(df_yt_metadata_pt_filtered):,} ({len(df_yt_metadata_pt_filtered)/len(df_yt_metadata_pt):.1%} of videos containing a PT accounts) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt_filtered['crawl_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['crawl_date'])\n",
    "df_yt_metadata_pt_filtered['upload_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['upload_date'])\n",
    "df_yt_metadata_pt_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find breakpoint and get statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Change point detection algo v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # find max increase algo V2\n",
    "# def find_breakpoint_v2(df, column):\n",
    "#     max_diff = 0\n",
    "#     max_index = 0\n",
    "#     i = 0\n",
    "#     df_len = len(df)\n",
    "\n",
    "#     # scan dataset for largest increase\n",
    "#     for date_index, row in ts_pt_df.iterrows():\n",
    "#         if (i >= 30 and i < df_len-30):\n",
    "#             sub30 = df.iloc[i-30][column]\n",
    "#             point = df.iloc[i][column]\n",
    "#             add30 = df.iloc[i+30][column]\n",
    "\n",
    "#             d1 = point - sub30\n",
    "#             d2 = add30 - point\n",
    "#             cur_diff = d2 - d1\n",
    "            \n",
    "#             if cur_diff > max_diff:\n",
    "#                 max_diff = cur_diff\n",
    "#                 max_index = i\n",
    "#         i = i + 1\n",
    "    \n",
    "#     return df.iloc[max_index]['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"img/increase_decrease_options_051322.jpg\" alt=\"increase_decrease_options_051322.jpg\" width=\"800\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change point detection algo v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max increase algo V3\n",
    "def find_breakpoint_v3(df, column, ratio_threshold):\n",
    "    \"\"\"\n",
    "    Scan column of the dataframe until it finds a breakpoint (= increase larger than threshold)\n",
    "    \n",
    "    :param df: dataframe\n",
    "    :param column: column to scan\n",
    "    :param ratio_threshold: minimum increase ratio threshold \n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    df_len = len(df)\n",
    "    moving_avg_half = 15\n",
    "        \n",
    "    # with pd.option_context('display.max_rows', 70):\n",
    "    #     display(df.head(65))\n",
    "\n",
    "    # scan dataset for increase larger than threshold\n",
    "    for date_index, row in ts_pt_df.iterrows():\n",
    "        if (i >= (60 + moving_avg_half) and i < df_len):\n",
    "            sub60 = df.iloc[i-60][column]\n",
    "            sub30 = df.iloc[i-30][column]\n",
    "            now = df.iloc[i][column]\n",
    "            \n",
    "        \n",
    "            d1 = sub30 - sub60\n",
    "            d2 = now - sub30\n",
    "            \n",
    "            # avoid  weird ratios obtained by diving by a difference between -1 and 1 \n",
    "            if (0 <= d1 < 1):\n",
    "                d1 = 1\n",
    "            elif (-1 < d1 < 0):\n",
    "                d1 = -1\n",
    "        \n",
    "            r = d2 / d1\n",
    "\n",
    "            # at least 10 patrons in the prior period\n",
    "            if (d1 > 10) & (d2 > d1) & (r >= ratio_threshold):\n",
    "                bkpnt_dict = {\n",
    "                    \"bkpt_date\"         : df.iloc[i-30]['date'],\n",
    "                    \"bkpt_date_sub30\"   : df.iloc[i-60]['date'],\n",
    "                    \"bkpt_date_add30\"   : df.iloc[i]['date'],\n",
    "                    \"avg_patrons_bkpnt\" : sub30,\n",
    "                    \"avg_patrons_sub30\" : sub60,\n",
    "                    \"avg_patrons_add30\" : now,\n",
    "                    \"d1\"                : d1,\n",
    "                    \"d2\"                : d2,\n",
    "                    \"r\"                 : r\n",
    "                }\n",
    "                \n",
    "                return bkpnt_dict\n",
    "        i = i + 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to 1 patreon account, sort by date and drop duplicates\n",
    "def restrict_acct_and_sort_df(df, patreon_col, patreon_id, date_col):\n",
    "    restr_df = df[df[patreon_col] == patreon_id].copy()  \n",
    "    restr_df = restr_df.sort_values(by=[date_col])\n",
    "    restr_df = restr_df.drop_duplicates()\n",
    "    return restr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select \"treated\" accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Find breakpoint and store Patreon breakpoint in \"df_treated\"\n",
    "\n",
    "# # variables declaration\n",
    "# MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# ROLLING_AVG_WINDOW = 30\n",
    "# INCR_RATIO_THRESH = 3\n",
    "# treated_tuples = []\n",
    "\n",
    "\n",
    "# print(f'Iterate over {len(top_patreons_unique_chan)} patreon accounts...')\n",
    "\n",
    "# # LOOP OVER TOP PATREON ACCOUNTS\n",
    "# for idx, patreon in tqdm(enumerate(top_patreons_unique_chan)):\n",
    "#     treat = None\n",
    "    \n",
    "#     ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "#     # patreon earnings and users\n",
    "#     tmp_df_pt = restrict_acct_and_sort_df(df_top_pt_accts, 'patreon', patreon, 'date')\n",
    "\n",
    "#     # youtube videos\n",
    "#     tmp_df_yt = restrict_acct_and_sort_df(df_yt_timeseries_top_pt, 'patreon_id', patreon, 'datetime')\n",
    "\n",
    "#     # youtube metadata\n",
    "#     tmp_df_yt_meta = restrict_acct_and_sort_df(df_yt_metadata_pt_filtered, 'patreon_id', patreon, 'upload_date')\n",
    "    \n",
    "#     ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "#     # set min and max dates for plots   \n",
    "#     date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "#     date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "#     # if no overlap period between YT and Patreon datasets, skip account\n",
    "#     if date_max < date_min:\n",
    "#         # print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "#         continue\n",
    "        \n",
    "#     # restrict datasets between min and max dates\n",
    "#     tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "#     tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    \n",
    "#     # align both dataframes since youtube starts once a week\n",
    "#     tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "#     tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "#     tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "#     ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "#     # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "#     ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "#     ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "#     ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "#     ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "#     tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "#     # reorder columns to have deltas columns next to their respective columns\n",
    "#     patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "#     ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "#     # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "#     ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "\n",
    "\n",
    "    \n",
    "#     ########################## PRINT TITLES ##########################\n",
    "#     # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "#     # ch_ids = tmp_df_yt['channel'].unique()\n",
    "#     # print(f\"\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "    \n",
    "#     ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "#     # breakpoint_date = find_breakpoint_v2(tmp_df_pt, 'patrons_ma')\n",
    "#     bkpnt_dict = find_breakpoint_v3(tmp_df_pt, 'patrons_ma', INCR_RATIO_THRESH)\n",
    "#     # print(\"bkpnt_dict: \", bkpnt_dict)\n",
    "    \n",
    "#     if bkpnt_dict == None:\n",
    "#         # print(\"No breakpoint for this account...\")\n",
    "#         continue\n",
    "#     else: \n",
    "#         treat = 1\n",
    "\n",
    "    \n",
    "#     ################################### CALCULATE DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "    \n",
    "#     ##### PATREON #####\n",
    "#     tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_sub30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date'])]\n",
    "#     tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add30'])]\n",
    "\n",
    "#     # delta patrons\n",
    "#     mean_delta_patrons_befor = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "#     mean_delta_patrons_after = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "        \n",
    "#     # delta earnings\n",
    "#     mean_delta_earnings_befor = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "#     mean_delta_earnings_after = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "\n",
    "    \n",
    "#     ##### YOUTUBE TIME SERIES #####\n",
    "#     tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date']      )]\n",
    "#     tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "    \n",
    "#     # delta videos\n",
    "#     mean_delta_videos_befor = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "#     mean_delta_videos_after = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "\n",
    "#     # delta views\n",
    "#     mean_delta_views_befor = tmp_df_YT_sub30['delta_views'].mean()\n",
    "#     mean_delta_views_after = tmp_df_YT_add30['delta_views'].mean()  \n",
    "\n",
    "#     # delta subscriptions\n",
    "#     mean_delta_subs_befor = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "#     mean_delta_subs_after = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "\n",
    "    \n",
    "#     ##### YOUTUBE METADATA #####\n",
    "#     tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date']      )]\n",
    "#     tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "        \n",
    "#     # durations\n",
    "#     mean_duration_befor = tmp_df_YT_META_sub30['duration'].mean()\n",
    "#     mean_duration_after = tmp_df_YT_META_add30['duration'].mean()      \n",
    "        \n",
    "#     # likes\n",
    "#     mean_likes_befor = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "#     mean_likes_after = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "    \n",
    "#     yt_channel_id = tmp_df_yt['channel'].unique()[0]\n",
    "\n",
    "        \n",
    "        \n",
    "#     treated_tuples.append(\n",
    "#         (          \n",
    "#             patreon, \n",
    "#             yt_channel_id,   \n",
    "#             treat,\n",
    "#             bkpnt_dict[\"d1\"], \n",
    "#             bkpnt_dict[\"d2\"], \n",
    "#             bkpnt_dict[\"r\"],\n",
    "#             bkpnt_dict[\"bkpt_date\"], \n",
    "#             bkpnt_dict[\"bkpt_date_sub30\"], \n",
    "#             bkpnt_dict[\"bkpt_date_add30\"],\n",
    "#             bkpnt_dict[\"avg_patrons_bkpnt\"], \n",
    "#             bkpnt_dict[\"avg_patrons_sub30\"], \n",
    "#             bkpnt_dict[\"avg_patrons_add30\"], \n",
    "#             mean_delta_patrons_befor, \n",
    "#             mean_delta_patrons_after, \n",
    "#             mean_delta_earnings_befor, \n",
    "#             mean_delta_earnings_after, \n",
    "#             mean_delta_videos_befor, \n",
    "#             mean_delta_videos_after,\n",
    "#             mean_delta_views_befor,\n",
    "#             mean_delta_views_after,\n",
    "#             mean_delta_subs_befor,\n",
    "#             mean_delta_subs_after,\n",
    "#             mean_duration_befor,\n",
    "#             mean_likes_after\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "            \n",
    "# df_treated = pd.DataFrame(treated_tuples, columns = [\n",
    "#     'patreon_id',\n",
    "#     'yt_channel_id',\n",
    "#     'treat',\n",
    "#     'd1', \n",
    "#     'd2', \n",
    "#     'ratio',\n",
    "#     'bkpt_date',     \n",
    "#     'bkpt_date_sub30', \n",
    "#     'bkpt_date_add30', \n",
    "#     'avg_patrons_bkpnt', \n",
    "#     'avg_patrons_sub30', \n",
    "#     'avg_patrons_add30', \n",
    "#     'mean_delta_patrons_befor', \n",
    "#     'mean_delta_patrons_after', \n",
    "#     'mean_delta_earnings_befor', \n",
    "#     'mean_delta_earnings_after', \n",
    "#     'mean_delta_videos_befor', \n",
    "#     'mean_delta_videos_after',\n",
    "#     'mean_delta_views_befor',\n",
    "#     'mean_delta_views_after',\n",
    "#     'mean_delta_subs_befor',\n",
    "#     'mean_delta_subs_after',\n",
    "#     'mean_duration_befor',\n",
    "#     'mean_likes_after'\n",
    "# ])\n",
    "\n",
    "# print(f'Patreon accounts added to the treated group (increase ratio >= {INCR_RATIO_THRESH}):  {len(df_treated)}')\n",
    "# df_treated['bkpt_date'] = pd.to_datetime(df_treated['bkpt_date'])\n",
    "# df_treated['bkpt_date_sub30'] = pd.to_datetime(df_treated['bkpt_date_sub30'])\n",
    "# df_treated['bkpt_date_add30'] = pd.to_datetime(df_treated['bkpt_date_add30'])\n",
    "# df_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"df_treated\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"df_treated.tsv.gz\"\n",
    "# df_treated.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_treated.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated = pd.read_csv(LOCAL_DATA_FOLDER+\"df_treated.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_treated['bkpt_date'] = pd.to_datetime(df_treated['bkpt_date'])\n",
    "df_treated['bkpt_date_sub30'] = pd.to_datetime(df_treated['bkpt_date_sub30'])\n",
    "df_treated['bkpt_date_add30'] = pd.to_datetime(df_treated['bkpt_date_add30'])\n",
    "df_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of ratios\n",
    "df_treated['ratio'].hist(bins=50)\n",
    "plt.title(\"Distribution of increase ratios of the treated group\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "# plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select potential \"control\" accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each subject in “Treated” group\n",
    "    - For each subject in Population (except for treated subject)\n",
    "        - Run breakpoint detection algorithm up to the Treated breakpoint date\n",
    "            - If we find a breakpoint --> reject \n",
    "            - If we dont find a breakpoint --> add to _potential_ control subjects (aka “Potential Control Subjects”) for this Treated subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_potential_matches = {}  \n",
    "\n",
    "# print(f\"Look for potential matches for {len(df_treated)} treated subjects\")\n",
    "\n",
    "# for idx_treated, treated_subject_row in tqdm(df_treated.iterrows()):\n",
    "#     treated_subject = treated_subject_row['patreon_id']\n",
    "#     # print(f\"\\ntreated subject: {idx_treated}, {treated_subject}\")\n",
    "    \n",
    "#     # make sure date cant go beyond treated group\n",
    "#     date_max = treated_subject_row['bkpt_date_add30']    \n",
    "#     # print(\"date_max: \", date_max)\n",
    "    \n",
    "#     for idx_potential_control, potential_control_subject in enumerate(top_patreons_unique_chan):\n",
    "#     # print(f\"\\n\\tpotential control subject: {idx_potential_control}, {potential_control_subject}\")        \n",
    "        \n",
    "#         # make sure potential control account is not the same as treated account         \n",
    "#         if (potential_control_subject == treated_subject):\n",
    "#             # print(\"potential_control_subject == treated_subject\")\n",
    "#             continue\n",
    "            \n",
    "#         # eliminate potential control accounts that have a breakpoint date earlier than treated\n",
    "#         if (potential_control_subject not in df_treated['patreon_id'].tolist()):\n",
    "#             # print(\"\\t\\tNo breakpoint for this account...      => KEEP AS A POTENTIAL CONTROL GROUP :)\")\n",
    "#             if treated_subject in dict_potential_matches:\n",
    "#                 dict_potential_matches[treated_subject].append(potential_control_subject)\n",
    "#             else:\n",
    "#                 dict_potential_matches[treated_subject] = [potential_control_subject]\n",
    "#         else: \n",
    "#             # if in the treated group but has a breakpoint after max date, consider it\n",
    "#             if (df_treated[df_treated['patreon_id'] == potential_control_subject].bkpt_date.iloc[0] > date_max):\n",
    "#                 # print(\"\\t\\tBreakpoint after the max date...      => KEEP AS A POTENTIAL CONTROL GROUP :)\")\n",
    "#                 if treated_subject in dict_potential_matches:\n",
    "#                     dict_potential_matches[treated_subject].append(potential_control_subject)\n",
    "#                 else:\n",
    "#                     dict_potential_matches[treated_subject] = [potential_control_subject]\n",
    "#             else:\n",
    "#                 # reject it\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save potential dictionary matches to disk in pickle format\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"dict_potential_matches.pickle\"\n",
    "\n",
    "# with open(output_file_path, 'wb') as f:\n",
    "#     pickle.dump(dict_potential_matches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l {LOCAL_DATA_FOLDER}dict_potential_matches.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOCAL_DATA_FOLDER+\"dict_potential_matches.pickle\", 'rb') as f:\n",
    "    dict_potential_matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of treated subjects: {len(dict_potential_matches)}\")\n",
    "\n",
    "print(\"\\nNumber of potential control group for each treated subject (print first 5): \") \n",
    "for idx, (k, v) in enumerate(dict_potential_matches.items()):\n",
    "    if idx <=5:\n",
    "        print(f'{k}: {len(v)}')\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find best matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each account, create a treated / pontential control dataframe and collect statistics of the potential control group for the same time frame as the treated group breakpoint\n",
    "\n",
    "# for treated_subject, potential_control_list in dict_potential_matches.items():\n",
    "#     print(treated_subject)\n",
    "#     for potential_control in potential_control_list:\n",
    "#         print(\"\\t\", potential_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation pseudo intervention dates on potential control group\n",
    "def align_breakpoints(treated_subject, potential_control_list, patreon_df, youtube_timeseries_df, youtube_meta_df):\n",
    "    \"\"\"\n",
    "    for a treated subject in dict, \n",
    "    get breakpoint dates values of potential control accounts\n",
    "    \"\"\"\n",
    "    # print(f\"\\n matching regions of potential control subjects for treated subject: {treated_subject}....)\")\n",
    "          \n",
    "    # variables declaration\n",
    "    MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "    ROLLING_AVG_WINDOW = 30\n",
    "    potential_control_tuples = []\n",
    "\n",
    "\n",
    "        \n",
    "    treated_subject_row = df_treated[df_treated['patreon_id'] == treated_subject]\n",
    "    bkpt_date_sub30   = treated_subject_row['bkpt_date_sub30'].iloc[0]\n",
    "    bkpt_date         = treated_subject_row['bkpt_date'].iloc[0]\n",
    "    bkpt_date_add30   = treated_subject_row['bkpt_date_add30'].iloc[0]\n",
    "    \n",
    "    # print(\"treated_subject_row:\", treated_subject_row)\n",
    "    # print(\"bkpt_date_sub30:\", bkpt_date_sub30)    \n",
    "    \n",
    "    for potential_control in tqdm(potential_control_list):\n",
    "        # print(\"\\t\", potential_control)\n",
    "        treat = 0\n",
    "        \n",
    "\n",
    "        ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "        # patreon earnings and users\n",
    "        tmp_df_pt = restrict_acct_and_sort_df(patreon_df, 'patreon', potential_control, 'date')\n",
    "\n",
    "        # sanity checks to make sure the treated date range exist in potential_control_df\n",
    "        if (tmp_df_pt[tmp_df_pt['date'] == bkpt_date_sub30].empty) or (tmp_df_pt[tmp_df_pt['date'] == bkpt_date_add30].empty):\n",
    "            # print(\"skip Patreon account as the breakpoint dates dont exist\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # youtube videos\n",
    "        tmp_df_yt = restrict_acct_and_sort_df(youtube_timeseries_df, 'patreon_id', potential_control, 'datetime')\n",
    "            \n",
    "\n",
    "        # youtube metadata\n",
    "        tmp_df_yt_meta = restrict_acct_and_sort_df(youtube_meta_df, 'patreon_id', potential_control, 'upload_date')\n",
    "\n",
    "        ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "\n",
    "        # set min and max dates for plots   \n",
    "        date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "        date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "\n",
    "        # if no overlap period between YT and Patreon datasets, skip account\n",
    "        if date_max < date_min:\n",
    "            # print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "            continue\n",
    "\n",
    "        # restrict datasets between min and max dates\n",
    "        tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "        tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "\n",
    "        # align both dataframes since youtube starts once a week\n",
    "        tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "\n",
    "        tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "        tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "        ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "\n",
    "        # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "        ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "        ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "        ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "        ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "        tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "\n",
    "        # reorder columns to have deltas columns next to their respective columns\n",
    "        patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "        ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "\n",
    "        # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "        ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "\n",
    "\n",
    "\n",
    "        ########################## PRINT TITLES ##########################\n",
    "        # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "        # ch_ids = tmp_df_yt['channel'].unique()\n",
    "        # print(f\"\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "\n",
    "        ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "#         # breakpoint_date = find_breakpoint_v2(tmp_df_pt, 'patrons_ma')\n",
    "#         bkpnt_dict = find_breakpoint_v3(tmp_df_pt, 'patrons_ma', INCR_RATIO_THRESH)\n",
    "#         # print(\"bkpnt_dict: \", bkpnt_dict)\n",
    "\n",
    "#         if bkpnt_dict == None:\n",
    "#             # print(\"No breakpoint for this account...\")\n",
    "#             continue\n",
    "#         else: \n",
    "#             treat = 1\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        try: \n",
    "            sub60 = tmp_df_pt[tmp_df_pt['date'] == bkpt_date_sub30]['patrons_ma'].iloc[0]\n",
    "            sub30 = tmp_df_pt[tmp_df_pt['date'] == bkpt_date]['patrons_ma'].iloc[0]\n",
    "            now = tmp_df_pt[tmp_df_pt['date'] == bkpt_date_add30]['patrons_ma'].iloc[0]\n",
    "        except Exception as e:\n",
    "            # print(\"Exception in align_breakpoint function: \", e)\n",
    "            continue\n",
    "            \n",
    "        d1 = sub30 - sub60\n",
    "        d2 = now - sub30\n",
    "\n",
    "        # avoid  weird ratios obtained by diving by a difference between -1 and 1 \n",
    "        if (0 <= d1 < 1):\n",
    "            d1 = 1\n",
    "        elif (-1 < d1 < 0):\n",
    "            d1 = -1\n",
    "\n",
    "        r = d2 / d1\n",
    "\n",
    "\n",
    "        bkpnt_dict = {\n",
    "            \"bkpt_date\"         : bkpt_date,\n",
    "            \"bkpt_date_sub30\"   : bkpt_date_sub30,\n",
    "            \"bkpt_date_add30\"   : bkpt_date_add30,\n",
    "            \"avg_patrons_bkpnt\" : sub30,\n",
    "            \"avg_patrons_sub30\" : sub60,\n",
    "            \"avg_patrons_add30\" : now,\n",
    "            \"d1\"                : d1,\n",
    "            \"d2\"                : d2,\n",
    "            \"r\"                 : r\n",
    "        }\n",
    "                \n",
    "                \n",
    "                \n",
    "        ################################### CALCULATE DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "\n",
    "        ##### PATREON #####\n",
    "        tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date_sub30']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date'])]\n",
    "        tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpnt_dict['bkpt_date']) & (ts_pt_weekly_avg_df_float64.index <= bkpnt_dict['bkpt_date_add30'])]\n",
    "\n",
    "        # delta patrons\n",
    "        mean_delta_patrons_befor = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "        mean_delta_patrons_after = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "\n",
    "        # delta earnings\n",
    "        mean_delta_earnings_befor = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "        mean_delta_earnings_after = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "\n",
    "\n",
    "        ##### YOUTUBE TIME SERIES #####\n",
    "        tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date']      )]\n",
    "        tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt['datetime'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "\n",
    "        # delta videos\n",
    "        mean_delta_videos_befor = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "        mean_delta_videos_after = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "\n",
    "        # delta views\n",
    "        mean_delta_views_befor = tmp_df_YT_sub30['delta_views'].mean()\n",
    "        mean_delta_views_after = tmp_df_YT_add30['delta_views'].mean()  \n",
    "\n",
    "        # delta subscriptions\n",
    "        mean_delta_subs_befor = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "        mean_delta_subs_after = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "\n",
    "\n",
    "        ##### YOUTUBE METADATA #####\n",
    "        tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date_sub30']) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date']      )]\n",
    "        tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpnt_dict['bkpt_date']      ) & (tmp_df_yt_meta['upload_date'] <= bkpnt_dict['bkpt_date_add30'])]\n",
    "\n",
    "        # durations\n",
    "        mean_duration_befor = tmp_df_YT_META_sub30['duration'].mean()\n",
    "        mean_duration_after = tmp_df_YT_META_add30['duration'].mean()      \n",
    "\n",
    "        # likes\n",
    "        mean_likes_befor = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "        mean_likes_after = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "\n",
    "        yt_channel_id = tmp_df_yt['channel'].unique()[0]\n",
    "\n",
    "\n",
    "\n",
    "        potential_control_tuples.append(\n",
    "            (          \n",
    "                potential_control, \n",
    "                yt_channel_id,   \n",
    "                treat,\n",
    "                bkpnt_dict[\"d1\"], \n",
    "                bkpnt_dict[\"d2\"], \n",
    "                bkpnt_dict[\"r\"],\n",
    "                bkpnt_dict[\"bkpt_date\"], \n",
    "                bkpnt_dict[\"bkpt_date_sub30\"], \n",
    "                bkpnt_dict[\"bkpt_date_add30\"],\n",
    "                bkpnt_dict[\"avg_patrons_bkpnt\"], \n",
    "                bkpnt_dict[\"avg_patrons_sub30\"], \n",
    "                bkpnt_dict[\"avg_patrons_add30\"], \n",
    "                mean_delta_patrons_befor, \n",
    "                mean_delta_patrons_after, \n",
    "                mean_delta_earnings_befor, \n",
    "                mean_delta_earnings_after, \n",
    "                mean_delta_videos_befor, \n",
    "                mean_delta_videos_after,\n",
    "                mean_delta_views_befor,\n",
    "                mean_delta_views_after,\n",
    "                mean_delta_subs_befor,\n",
    "                mean_delta_subs_after,\n",
    "                mean_duration_befor,\n",
    "                mean_duration_after,\n",
    "                mean_likes_befor,\n",
    "                mean_likes_after\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    df_potential_control = pd.DataFrame(potential_control_tuples, columns = [\n",
    "        'patreon_id',\n",
    "        'yt_channel_id',\n",
    "        'treat',\n",
    "        'd1', \n",
    "        'd2', \n",
    "        'ratio',\n",
    "        'bkpt_date',     \n",
    "        'bkpt_date_sub30', \n",
    "        'bkpt_date_add30', \n",
    "        'avg_patrons_bkpnt', \n",
    "        'avg_patrons_sub30', \n",
    "        'avg_patrons_add30', \n",
    "        'mean_delta_patrons_befor', \n",
    "        'mean_delta_patrons_after', \n",
    "        'mean_delta_earnings_befor', \n",
    "        'mean_delta_earnings_after', \n",
    "        'mean_delta_videos_befor', \n",
    "        'mean_delta_videos_after',\n",
    "        'mean_delta_views_befor',\n",
    "        'mean_delta_views_after',\n",
    "        'mean_delta_subs_befor',\n",
    "        'mean_delta_subs_after',\n",
    "        'mean_duration_befor',\n",
    "        'mean_duration_after',\n",
    "        'mean_likes_befor',\n",
    "        'mean_likes_after'\n",
    "    ])\n",
    "\n",
    "    # print(f'Number of Patreon accounts added to the potential_control group:  {len(df_potential_control)}')\n",
    "    df_potential_control['bkpt_date'] = pd.to_datetime(df_potential_control['bkpt_date'])\n",
    "    df_potential_control['bkpt_date_sub30'] = pd.to_datetime(df_potential_control['bkpt_date_sub30'])\n",
    "    df_potential_control['bkpt_date_add30'] = pd.to_datetime(df_potential_control['bkpt_date_add30'])\n",
    "\n",
    "    df_treat = pd.concat([treated_subject_row, df_potential_control]).reset_index(drop=True)\n",
    "    return df_treat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propensity score model\n",
    "- Use logistic regression to estimate propensity scores for all points in the dataset. \n",
    "-  Use statsmodels to fit the logistic regression model and apply it to each data point to obtain propensity scores.\n",
    "\n",
    "The propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Treatment:**\n",
    "    - Increase of Patrons at breakpoint - converted to a binary variable as follows:\n",
    "        - `treat` = 1 (treatment group), if number of patrons increase ratio at breakpoint > threshold\n",
    "        - `treat` = 0 (control group), if number of patrons increase linearly (increase ratio btw 1 and 2)\n",
    "        \n",
    "- **Outcome**\n",
    "    - `mean_delta_videos_after`: YouTube delta views (post-treatment)\n",
    "    \n",
    "- **Observed covariates:**\n",
    "    - `avg_patrons_sub30`: Average number of patrons 30 days before breakpoint\n",
    "    - `mean_delta_videos_befor`: YouTube delta videos (pre-treatment) \n",
    "    - `mean_delta_views_befor`:  YouTube delta views (pre-treatment) \n",
    "    - `mean_delta_subs_befor`:   YouTube delta subs (pre-treatment) \n",
    "    - `mean_duration_befor`:     YouTube video duration (pre-treatment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = \\\n",
    "['avg_patrons_bkpnt',\n",
    " 'avg_patrons_sub30',\n",
    " 'avg_patrons_add30',\n",
    " 'mean_delta_patrons_befor',\n",
    " 'mean_delta_patrons_after',\n",
    " 'mean_delta_earnings_befor',\n",
    " 'mean_delta_earnings_after',\n",
    " 'mean_delta_videos_befor',\n",
    " 'mean_delta_videos_after',\n",
    " 'mean_delta_views_befor',\n",
    " 'mean_delta_views_after',\n",
    " 'mean_delta_subs_befor',\n",
    " 'mean_delta_subs_after',\n",
    " 'mean_duration_befor',\n",
    " 'mean_duration_after',\n",
    " 'mean_likes_befor',\n",
    " 'mean_likes_after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(df, features_list):\n",
    "    # standardize the continuous features\n",
    "    df_std = df.copy()\n",
    "    for feature in features_list:\n",
    "        df_std[feature] = (df_std[feature] - df_std[feature].mean())/df_std[feature].std()\n",
    "    \n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_weight_matching(df):\n",
    "    # Separate the treatment and control groups\n",
    "    treatment_df = df[df['treat'] == 1]\n",
    "    control_df   = df[df['treat'] == 0]\n",
    "\n",
    "    # Create an empty undirected graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Loop through all the pairs of instances\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "        for control_id, control_row in control_df.iterrows():\n",
    "\n",
    "            # Calculate the similarity \n",
    "            similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                        treatment_row['Propensity_score'])\n",
    "\n",
    "            # Add an edge between the two instances weighted by the similarity between them\n",
    "            G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "    # Generate and return the maximum weight matching on the generated graph\n",
    "    matching = nx.max_weight_matching(G)\n",
    "    return matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datframe with potential control accounts per treated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a DF with treated and potential control accounts (takes about 2h15)\n",
    "# exceptions = 0\n",
    "# all_treat_and_potential_control_df = pd.DataFrame()\n",
    "\n",
    "# print(f'Iterate over {len(dict_potential_matches)} treated accounts...')\n",
    "# for idx, (treated_subject, potential_control_list) in enumerate(tqdm(dict_potential_matches.items())):\n",
    "#     # if idx > 1:\n",
    "#     #     break\n",
    "#     # print(f\"\\nidx: {idx}, treated_subject: {treated_subject}\")\n",
    "#     try: \n",
    "#             df_treat = align_breakpoints(treated_subject, potential_control_list, df_top_pt_accts, df_yt_timeseries_top_pt, df_yt_metadata_pt_filtered)\n",
    "\n",
    "#     except Exception as e: \n",
    "#         exceptions += 1\n",
    "#         print(\"Exception: \", e)\n",
    "#         continue\n",
    "        \n",
    "#     df_treat['treated_patreon_id'] = treated_subject\n",
    "#     all_treat_and_potential_control_df = pd.concat([all_treat_and_potential_control_df, df_treat])\n",
    "    \n",
    "# print(\"total number of exceptions: \", exceptions)\n",
    "# print(\"all_treat_and_potential_control_df: \")\n",
    "# all_treat_and_potential_control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save \"all_treat_and_potential_control_df\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"all_treat_and_potential_control_df.tsv.gz\"\n",
    "# all_treat_and_potential_control_df.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}all_treat_and_potential_control_df.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_treat_and_potential_control_df = pd.read_csv(LOCAL_DATA_FOLDER+\"all_treat_and_potential_control_df.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "first_column = all_treat_and_potential_control_df.pop('treated_patreon_id')  \n",
    "all_treat_and_potential_control_df.insert(0, 'treated_patreon_id', first_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_treat_and_potential_control_df.groupby('treated_patreon_id')['patreon_id'].count()\n",
    "treated_subjects = all_treat_and_potential_control_df.treated_patreon_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# match pairs\n",
    "exceptions = 0\n",
    "all_pairs_df = pd.DataFrame()\n",
    "match_pairs_dict = {} # treatment: control\n",
    "display_cols = ['patreon_id', 'treat', 'Propensity_score', 'd1', 'd2', 'ratio', 'bkpt_date', 'avg_patrons_sub30', 'mean_delta_videos_befor', 'mean_delta_views_befor', 'mean_delta_subs_befor', 'mean_duration_befor']\n",
    "non_na_cols  = ['patreon_id', 'treat', 'd1', 'd2', 'ratio', 'bkpt_date', 'avg_patrons_sub30', 'mean_delta_videos_befor', 'mean_delta_views_befor', 'mean_delta_subs_befor', 'mean_duration_befor']\n",
    "\n",
    "print(f'Iterate over {len(treated_subjects)} treated accounts...')\n",
    "# for idx, (treated_subject, potential_control_list) in enumerate(tqdm(dict_potential_matches.items())):\n",
    "for idx, treated_subject in enumerate(tqdm(treated_subjects)):\n",
    "    # print(f\"\\nidx: {idx}, treated_subject: {treated_subject}\")\n",
    "    # if idx > 4:\n",
    "    #     break\n",
    "    \n",
    "    # restrict dataframe to this treated account\n",
    "    df_treat = all_treat_and_potential_control_df[all_treat_and_potential_control_df['treated_patreon_id'] == treated_subject]\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        # df_treat = align_breakpoints(treated_subject, potential_control_list, df_top_pt_accts, df_yt_timeseries_top_pt, df_yt_metadata_pt_filtered)\n",
    "\n",
    "        # drop rows only from control accounts which have NA values in the columns of interest\n",
    "        df_treat = df_treat.drop(df_treat[(df_treat['treat'] == 0) & (df_treat[non_na_cols].isna().any(axis=1))].index)\n",
    "        df_treat = df_treat.reset_index(drop=True)\n",
    "\n",
    "        df_treat_std = standardize_features(df_treat, continuous_features)\n",
    "\n",
    "        # propensity score matching\n",
    "        mod = smf.logit(formula='treat ~  avg_patrons_sub30 + mean_delta_patrons_befor + mean_delta_videos_befor + mean_delta_views_befor + mean_delta_subs_befor', data=df_treat_std)\n",
    "        # mod = smf.logit(formula='treat ~  d1', data=df_treat_std)\n",
    "        res = mod.fit(disp=0, warn_convergence=True)\n",
    "\n",
    "        df_treat['Propensity_score'] = res.predict()\n",
    "        df_treat_std['Propensity_score'] = res.predict() # Extract the estimated propensity scores\n",
    "        # print(res.summary())\n",
    "\n",
    "        matching_pair = maximum_weight_matching(df_treat)\n",
    "        matched_list = [i[0] for i in list(matching_pair)] + [i[1] for i in list(matching_pair)]\n",
    "        matched_pair_df = df_treat.iloc[matched_list]\n",
    "\n",
    "        # print(\"\\n matched_pair_df: \")\n",
    "        # display(matched_pair_df[display_cols])\n",
    "\n",
    "        match_pairs_dict[matched_pair_df[matched_pair_df.treat == 1].patreon_id.iloc[0]] = matched_pair_df[matched_pair_df.treat == 0].patreon_id.iloc[0]\n",
    "\n",
    "        all_pairs_df = pd.concat([all_pairs_df, matched_pair_df])\n",
    "        # .reset_index(drop=True)\n",
    "        # display(all_pairs_df)\n",
    "    except Exception as e: \n",
    "        exceptions += 1\n",
    "        # print(f\"\\nidx: {idx}, treated_subject: {treated_subject}\")\n",
    "        # print(\"Exception: \", e)\n",
    "        continue\n",
    "\n",
    "    # print(\"\\n\\n ------------------------------------------------------------------------------------------------------------------------------------------------------ \\n\\n\")\n",
    "\n",
    "print(\"\\nnumber of exceptions: \", exceptions)\n",
    "print(\"\\n\\n Dataframe with all pairs: \")\n",
    "all_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_df = all_pairs_df.reset_index(drop=True)\n",
    "all_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_df['bkpt_date'] = pd.to_datetime(all_pairs_df['bkpt_date'])\n",
    "all_pairs_df['bkpt_date_sub30'] = pd.to_datetime(all_pairs_df['bkpt_date_sub30'])\n",
    "all_pairs_df['bkpt_date_add30'] = pd.to_datetime(all_pairs_df['bkpt_date_add30'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate increase between mean before breakpoint and mean after breakpoint for:\n",
    "all_pairs_df['diff_delta_patrons'] = all_pairs_df['mean_delta_patrons_after'] - all_pairs_df['mean_delta_patrons_befor']\n",
    "all_pairs_df['diff_delta_earnings'] = all_pairs_df['mean_delta_earnings_after'] - all_pairs_df['mean_delta_earnings_befor']\n",
    "all_pairs_df['diff_delta_videos'] = all_pairs_df['mean_delta_videos_after'] - all_pairs_df['mean_delta_videos_befor']\n",
    "all_pairs_df['diff_delta_views'] = all_pairs_df['mean_delta_views_after'] - all_pairs_df['mean_delta_views_befor']\n",
    "all_pairs_df['diff_delta_subs'] = all_pairs_df['mean_delta_subs_after'] - all_pairs_df['mean_delta_subs_befor']\n",
    "# all_pairs_df['diff_duration'] = all_pairs_df['mean_duration_befor'] - all_pairs_df['mean_duration_after']  TODO\n",
    "# all_pairs_df['diff_likes'] = all_pairs_df['mean_likes_before'] - all_pairs_df['mean_likes_after'] TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 35)\n",
    "\n",
    "# reorder columns\n",
    "all_pairs_df = all_pairs_df[['treated_patreon_id', 'patreon_id', 'yt_channel_id', 'treat', 'bkpt_date_sub30', 'bkpt_date', 'bkpt_date_add30',\n",
    "       'avg_patrons_bkpnt', 'avg_patrons_sub30', 'avg_patrons_add30', 'd1',\n",
    "       'd2', 'ratio', \n",
    "       'mean_delta_patrons_befor', 'mean_delta_patrons_after', 'diff_delta_patrons',\n",
    "       'mean_delta_earnings_befor', 'mean_delta_earnings_after','diff_delta_earnings', \n",
    "       'mean_delta_videos_befor', 'mean_delta_videos_after', 'diff_delta_videos', \n",
    "       'mean_delta_views_befor', 'mean_delta_views_after', 'diff_delta_views',\n",
    "       'mean_delta_subs_befor', 'mean_delta_subs_after', 'diff_delta_subs',\n",
    "       'mean_duration_befor',\n",
    "       'mean_likes_after', \n",
    "        'Propensity_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pairs_df.head(1)\n",
    "\n",
    "# look at weird thing where treated accts, but neg diff delta patrons\n",
    "# all_pairs_df[(all_pairs_df['treat'] == 1) & (all_pairs_df['diff_delta_patrons'] < 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove unnecessary cols\n",
    "# all_pairs_df = all_pairs_df[['patreon_id', 'yt_channel_id', 'treat', 'd1',\n",
    "#        'd2', 'ratio', 'bkpt_date', 'bkpt_date_sub30', 'bkpt_date_add30',\n",
    "#        'diff_delta_patrons',\n",
    "#        'diff_delta_earnings', \n",
    "#        'diff_delta_videos', \n",
    "#        'diff_delta_views',\n",
    "#        'diff_delta_subs']]\n",
    "\n",
    "all_pairs_df.head()\n",
    "\n",
    "filtered_list = all_pairs_df[(all_pairs_df['treat'] == 1) & (all_pairs_df['ratio'] > 3)].treated_patreon_id\n",
    "print(len(filtered_list))\n",
    "all_pairs_df_filt = all_pairs_df[all_pairs_df['treated_patreon_id'].isin(filtered_list)]\n",
    "print(len(all_pairs_df_filt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter pairs according to increase of the treated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "treatement_cols = ['diff_delta_patrons', 'diff_delta_earnings']\n",
    "\n",
    "# plot with linear scale for x axis and log scale for y axis\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5), sharex=False, sharey=False)\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(treatement_cols, axs.flatten())):\n",
    "    sns.stripplot(x=\"treat\", y=col, data=all_pairs_df_filt, ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(col)\n",
    "    # ax.set(yscale=\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_cols = ['diff_delta_videos', 'diff_delta_views', 'diff_delta_subs']\n",
    "\n",
    "# plot with linear scale for x axis and log scale for y axis\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.stripplot(x=\"treat\", y=col, data=all_pairs_df_filt, ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(col)\n",
    "    # ax.set(yscale=\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['diff_delta_videos', 'diff_delta_views', 'diff_delta_subs']\n",
    "\n",
    "# plot with linear scale for x axis and log scale for y axis\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.boxplot(x=\"treat\", y=col, data=all_pairs_df_filt, ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(col)\n",
    "    # ax.set(yscale=\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = all_pairs_df_filt.loc[all_pairs_df_filt['treat'] == 1] \n",
    "control = all_pairs_df_filt.loc[all_pairs_df_filt['treat'] == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['diff_delta_videos', 'diff_delta_views', 'diff_delta_subs']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "for i,(col,ax) in enumerate(zip(selected_cols, axs.flatten())):\n",
    "    sns.distplot(treated[col], hist=True, label='treated', ax=ax)\n",
    "    sns.distplot(control[col], hist=True, label='control', ax=ax)\n",
    "    ax.set(title=f'Distribution of {col}')\n",
    "    ax.set_ylabel(col)\n",
    "    # ax.set(yscale=\"log\")\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 Plot Treated VS Control accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_plot(ax, x, y, title, x_axis_label=\"default x\", y_axis_label=\"default y\", color=\"#1f77b4\", alpha=1):\n",
    "#     ax.plot(x, y, color, alpha)\n",
    "#     ax.set(title=title)\n",
    "#     ax.set_xlabel(x_axis_label)    \n",
    "#     ax.set_ylabel(y_axis_label)    \n",
    "\n",
    "# custom_plot(axs[0,0], ts_pt_df['date'], ts_pt_df['patrons'], alpha=0.2)\n",
    "# custom_plot(axs[0,0], ts_pt_df['date'], ts_pt_df['patrons_ravg'], \"Number of patrons\", y_axis_label=\"# Patrons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_neg_pos(ax, x, y):\n",
    "    if y.isnull().all():\n",
    "        return\n",
    "    if (y.min() < 0): \n",
    "        # fill negative values in red and draw a horizontal line at 0\n",
    "        ax.fill_between(x, y.min(), 0, color='red', alpha=0.05)\n",
    "        ax.axhline(y=0, linestyle='solid', color= 'black', linewidth=0.5)\n",
    "    # fill positive values in green\n",
    "    # ax.fill_between(x, 0, y.max(), color='green', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number in thousand (k) or in million (M) on y axis\n",
    "def KM(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    if x > 999_999:\n",
    "        return '%2.1fM' % (x * 1e-6)\n",
    "    elif x > 999:\n",
    "        return '%2.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%3.0f ' % (x)\n",
    "KM_formatter = FuncFormatter(KM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter accounts that match selected Patreon ids\n",
    "df_yt_metadata_pt_filtered = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(all_pairs_df['patreon_id'])].copy()\n",
    "print(f'Filter accounts that match selected Patreon ids: {len(df_yt_metadata_pt_filtered):,} ({len(df_yt_metadata_pt_filtered)/len(df_yt_metadata_pt):.1%} of videos containing a PT accounts) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt_filtered['crawl_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['crawl_date'])\n",
    "df_yt_metadata_pt_filtered['upload_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['upload_date'])\n",
    "df_yt_metadata_pt_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in all_pairs_df.iterrows():\n",
    "#     if idx > 0:\n",
    "#         break\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_df = all_pairs_df.sort_values('ratio', ascending=False)\n",
    "all_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare Patreon and YouTube timeseries + YouTube metadata\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# WEEK_OFFSET = pd.DateOffset(weeks=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# variables for Granger Tests\n",
    "MAXLAG = 2\n",
    "granger_dict = {} # dictionary with  keys (cause --> effect) and values with list of corresponding patreon account(s)\n",
    "not_granger = []\n",
    "YT_variables = ['yt_delta_videos', 'yt_delta_views', 'yt_delta_subs']\n",
    "PT_variables = ['pt_delta_patrons']\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in tqdm(all_pairs_df.iterrows()):\n",
    "    patreon = row['patreon_id']\n",
    "    \n",
    "    fig, axs = plt.subplots(7, 4, figsize=(26, 10), sharey=False, sharex=False)\n",
    "        \n",
    "\n",
    "    \n",
    "    ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = df_top_pt_accts[df_top_pt_accts['patreon'] == patreon].copy()  \n",
    "    tmp_df_pt = tmp_df_pt.sort_values(by=['date'])\n",
    "    tmp_df_pt = tmp_df_pt.drop_duplicates()\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon].copy()\n",
    "    tmp_df_yt = tmp_df_yt.sort_values(by=['datetime'])\n",
    "    \n",
    "    # youtube metadata\n",
    "    tmp_df_yt_meta = df_yt_metadata_pt_filtered[df_yt_metadata_pt_filtered['patreon_id'] == patreon].copy()   \n",
    "    tmp_df_yt_meta = tmp_df_yt_meta.sort_values('upload_date')\n",
    "    # tmp_df_yt_meta['upload_date'] = pd.to_datetime(tmp_df_yt_meta['upload_date'])\n",
    "    \n",
    "    # replace dates that were collected after 23:00 to their next day, and remove hour\n",
    "    tmp_df_yt['datetime_original'] = tmp_df_yt['datetime']\n",
    "    tmp_df_yt['datetime'] = tmp_df_yt['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "    \n",
    "    # remove hours and convert to datetime type\n",
    "    tmp_df_yt['datetime'] = pd.to_datetime(tmp_df_yt['datetime'].dt.date)\n",
    "    \n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    ch_ids = tmp_df_yt['channel'].unique()\n",
    "    print(f\"\\n\\n\\n\\033[1m {idx+1}: {patreon[12:]} (treat = {row['treat']})\\033[0m\")\n",
    "    print(f\"https://www.{patreon}\")\n",
    "    print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    for ch_id in ch_ids:\n",
    "        print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "   \n",
    "    print(f'\\nYouTube Metadata: ')\n",
    "    print('• YT videos were uploaded between {} and {}'.format(tmp_df_yt_meta['upload_date'].min().strftime('%B %d, %Y'),\n",
    "                                                             tmp_df_yt_meta['upload_date'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "    print('• YT metadata was crawled between {} and {}'.format(tmp_df_yt_meta['crawl_date'].min().strftime('%B %d, %Y'),\n",
    "                                                             tmp_df_yt_meta['crawl_date'].max().strftime('%B %d, %Y')))\n",
    "    \n",
    "    ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "    # set min and max dates for plots   \n",
    "    date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "    date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "    if date_max < date_min:\n",
    "        print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "        continue\n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "    tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    tmp_df_yt_meta = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min) & (tmp_df_yt_meta['upload_date'] <= date_max)]\n",
    "    \n",
    "    # align both dataframes since youtube starts once a week\n",
    "    tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "              \n",
    "    ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "    # breakpoint_date = find_breakpoint_v2(tmp_df_pt, 'patrons_ma')\n",
    "    print(\"Breakpoint date: \", breakpoint_date.date())\n",
    "\n",
    "    # check that dates prior and after breakpoint exist\n",
    "    if not (((breakpoint_date - 1*MONTH_OFFSET)) in ts_pt_df.index and ((breakpoint_date + 1*MONTH_OFFSET) in ts_pt_df.index)):\n",
    "        print(f\"ERROR: Breakpoint too close to edge of patreon time series or missing data\\n\")\n",
    "        plt.figure().clear(); plt.close(); plt.cla(); plt.clf(); plt.show()\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    \n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "\n",
    "    \n",
    "    r = row['ratio']\n",
    "\n",
    "    print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "    print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "    print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "    print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    \n",
    "    print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "    print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    \n",
    "    print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "    print(f\"• Ratio between 2 increases:                            d2/d1  = {r:.2f}\")\n",
    "    print(f\"• Percentage increase:                              d2/d1*100  = {r:>+.0%}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ################################### CALCULATE DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "    \n",
    "    ##### PATREON #####\n",
    "    tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpt_date_sub30) & (ts_pt_weekly_avg_df_float64.index <= bkpt_date)]\n",
    "    tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpt_date) & (ts_pt_weekly_avg_df_float64.index <= bkpt_date_add30)]\n",
    "\n",
    "    # delta patrons\n",
    "    mean_delta_patrons_befor = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "    mean_delta_patrons_after = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "        \n",
    "    # delta earnings\n",
    "    mean_delta_earnings_befor = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "    mean_delta_earnings_after = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "\n",
    "    \n",
    "    ##### YOUTUBE TIME SERIES #####\n",
    "    tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpt_date_sub30) & (tmp_df_yt['datetime'] <= bkpt_date      )]\n",
    "    tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpt_date      ) & (tmp_df_yt['datetime'] <= bkpt_date_add30)]\n",
    "    \n",
    "    # delta videos\n",
    "    mean_delta_videos_befor = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "    mean_delta_videos_after = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "\n",
    "    # delta views\n",
    "    mean_delta_views_befor = tmp_df_YT_sub30['delta_views'].mean()\n",
    "    mean_delta_views_after = tmp_df_YT_add30['delta_views'].mean()  \n",
    "\n",
    "    # delta subscriptions\n",
    "    mean_delta_subs_befor = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "    mean_delta_subs_after = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "\n",
    "    \n",
    "    ##### YOUTUBE METADATA #####\n",
    "    tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpt_date_sub30) & (tmp_df_yt_meta['upload_date'] <= bkpt_date      )]\n",
    "    tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpt_date      ) & (tmp_df_yt_meta['upload_date'] <= bkpt_date_add30)]\n",
    "        \n",
    "    # durations\n",
    "    mean_duration_befor = tmp_df_YT_META_sub30['duration'].mean()\n",
    "    mean_duration_after = tmp_df_YT_META_add30['duration'].mean()      \n",
    "        \n",
    "    # likes\n",
    "    mean_likes_befor = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "    mean_likes_after = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "        \n",
    "    \n",
    "    # plot dots in the middle of region for the region means   \n",
    "    axs[0,2].plot(tmp_df_PT_sub30.index.mean(), mean_delta_patrons_befor, marker='o', color='green', markersize=15)\n",
    "    axs[0,2].plot(tmp_df_PT_add30.index.mean(), mean_delta_patrons_after, marker='o', color='orange', markersize=15)\n",
    "    axs[1,2].plot(tmp_df_PT_sub30.index.mean(), mean_delta_earnings_befor, marker='o', color='green', markersize=15)\n",
    "    axs[1,2].plot(tmp_df_PT_add30.index.mean(), mean_delta_earnings_after, marker='o', color='orange', markersize=15)\n",
    "    axs[2,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_videos_befor, marker='o', color='green', markersize=15)\n",
    "    axs[2,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_videos_after, marker='o', color='orange', markersize=15)\n",
    "    axs[3,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_views_befor, marker='o', color='green', markersize=15)\n",
    "    axs[3,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_views_after, marker='o', color='orange', markersize=15)  \n",
    "    axs[4,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_subs_befor, marker='o', color='green', markersize=15)\n",
    "    axs[4,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_subs_after, marker='o', color='orange', markersize=15)\n",
    "    \n",
    "    # sometimes there is no value at all for this period of time in YT meta --> error when plotting\n",
    "    if not (tmp_df_YT_META_sub30.empty or tmp_df_YT_META_add30.empty):\n",
    "        axs[5,2].plot(tmp_df_YT_META_sub30['upload_date'].mean(), mean_duration_befor, marker='o', color='green', markersize=15)\n",
    "        axs[5,2].plot(tmp_df_YT_META_add30['upload_date'].mean(), mean_duration_after, marker='o', color='orange', markersize=15)  \n",
    "        axs[6,2].plot(tmp_df_YT_META_sub30['upload_date'].mean(), mean_likes_befor, marker='o', color='green', markersize=15)\n",
    "        axs[6,2].plot(tmp_df_YT_META_add30['upload_date'].mean(), mean_likes_after, marker='o', color='orange', markersize=15)  \n",
    "\n",
    "    \n",
    "    # plot horizontal lines for means\n",
    "    mean_befor_list = [mean_delta_patrons_befor, mean_delta_earnings_befor, mean_delta_videos_befor, mean_delta_views_befor, mean_delta_subs_befor, mean_duration_befor, mean_likes_befor]\n",
    "    mean_afer_list = [mean_delta_patrons_after, mean_delta_earnings_after, mean_delta_videos_after, mean_delta_views_after, mean_delta_subs_after, mean_duration_after, mean_likes_after]\n",
    "       \n",
    "    for idx, mean in enumerate(mean_befor_list):\n",
    "            if not math.isnan(mean):\n",
    "                axs[idx,2].hlines(y=mean, xmin=bkpt_date_sub30, xmax=bkpt_date      , linewidth=2, linestyle='--', color='green')\n",
    "\n",
    "    for idx, mean in enumerate(mean_afer_list):\n",
    "            if not math.isnan(mean):\n",
    "                axs[idx,2].hlines(y=mean, xmin=bkpt_date,       xmax=bkpt_date_add30, linewidth=2, linestyle='--', color='orange')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    ################################### ZOOM OUT PLOTS ###################################\n",
    "    \n",
    "    # number of patrons (delta)\n",
    "    axs[0,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,0].set(title=\"Delta patrons per week\")\n",
    "    axs[0,0].set_ylabel(\"Δ Patrons\")    \n",
    "    color_neg_pos(axs[0,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'])\n",
    "\n",
    "    # number of patrons (cumulative)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons'], alpha=0.2)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons_ma'])\n",
    "    axs[0,1].set(title=\"Number of patrons\")\n",
    "    axs[0,1].set_ylabel(\"# Patrons\")\n",
    "\n",
    "    # patreon earnings (delta)\n",
    "    axs[1,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,0].set(title=\"Patreon delta earnings per week\")\n",
    "    axs[1,0].set_ylabel(\"Δ Earnings\") \n",
    "    color_neg_pos(axs[1,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'])\n",
    "\n",
    "    # patreon earnings (cumulative)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning'], alpha=0.2)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning_ma'], color='royalblue')\n",
    "    axs[1,1].set(title=\"Patreon earnings per month\")\n",
    "    axs[1,1].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # youtube videos (delta)\n",
    "    axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,0].set(title=\"YouTube delta videos per week\")\n",
    "    axs[2,0].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,0], tmp_df_yt['datetime'], tmp_df_yt['delta_videos'])\n",
    "\n",
    "    # youtube videos (cumulative)\n",
    "    axs[2,1].plot(tmp_df_yt['datetime'], tmp_df_yt['videos'], 'r')\n",
    "    axs[2,1].set(title=\"YouTube cumulative videos\")\n",
    "    axs[2,1].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # youtube views (delta)\n",
    "    axs[3,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,0].set(title=\"YouTube delta views per week\")\n",
    "    axs[3,0].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,0], tmp_df_yt['datetime'], tmp_df_yt['delta_views'])\n",
    "\n",
    "    # youtube views (cumulative)\n",
    "    axs[3,1].plot(tmp_df_yt['datetime'], tmp_df_yt['views'], 'g')\n",
    "    axs[3,1].set(title=\"YouTube cumulative views\")\n",
    "    axs[3,1].set_ylabel(\"# Views\")\n",
    "\n",
    "    # youtube subs (delta)\n",
    "    axs[4,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,0].set(title=\"YouTube delta subscriptions per week\")\n",
    "    axs[4,0].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,0], tmp_df_yt['datetime'], tmp_df_yt['delta_subs'])\n",
    "\n",
    "    # youtube subs (cumulative)\n",
    "    axs[4,1].plot(tmp_df_yt['datetime'], tmp_df_yt['subs'], 'm')\n",
    "    axs[4,1].set(title=\"YouTube cumulative subscriptions\")\n",
    "    axs[4,1].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,0].set(title=\"YouTube videos durations\")\n",
    "    axs[5,0].set_ylabel(\"Duration\")\n",
    "    \n",
    "    \n",
    "    # youtube likes at crawl date\n",
    "    axs[6,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,0].set(title=\"YouTube likes (plotted against upload date)\")\n",
    "    axs[6,0].set_ylabel(\"Likes\")\n",
    "    \n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = breakpoint_date - (2 * MONTH_OFFSET)\n",
    "    date_max_zoom = breakpoint_date + (2 * MONTH_OFFSET)\n",
    "            \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_meta_zoomed = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min_zoom) & (tmp_df_yt_meta['upload_date'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "    \n",
    "   ################################### ZOOM IN PLOTS  ###################################\n",
    "\n",
    "    # zoomed in patron numbers (delta)\n",
    "    axs[0,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', alpha=0.3)\n",
    "    axs[0,2].set(title=\"Delta patrons per week\")\n",
    "    axs[0,2].set_ylabel(\"Δ Patrons\")\n",
    "    color_neg_pos(axs[0,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'])\n",
    "    \n",
    "    # zoomed in patron numbers (cumulative)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons'], alpha=0.2)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons_ma'])\n",
    "    axs[0,3].set(title=\"Number of patrons (zoomed in)\")\n",
    "    axs[0,3].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "    # zoomed in patron earnings (delta)\n",
    "    axs[1,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', alpha=0.3)\n",
    "    axs[1,2].set(title=\"Delta Patreon earnings per week (zoomed in)\")\n",
    "    axs[1,2].set_ylabel(\"Earnings\")  \n",
    "    color_neg_pos(axs[1,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_earning'])\n",
    "\n",
    "    # zoomed in patron earnings (cumulative)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning'], alpha=0.2)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning_ma'], color='royalblue')\n",
    "    axs[1,3].set(title=\"Patreon earnings per month (zoomed in)\")\n",
    "    axs[1,3].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # zoomed in youtube videos (delta)\n",
    "    axs[2,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', alpha=0.3)\n",
    "    axs[2,2].set(title=\"YouTube delta videos per week (zoomed in)\")\n",
    "    axs[2,2].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_videos'])\n",
    "\n",
    "    # zoomed in youtube videos (cumulative)\n",
    "    axs[2,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['videos'], 'r')\n",
    "    axs[2,3].set(title=\"YouTube cumulative videos (zoomed in)\")\n",
    "    axs[2,3].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # zoomed in youtube views (delta)\n",
    "    axs[3,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', alpha=0.3)\n",
    "    axs[3,2].set(title=\"YouTube delta views per week (zoomed in)\")\n",
    "    axs[3,2].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_views'])\n",
    "\n",
    "    # zoomed in youtube views (cumulative)\n",
    "    axs[3,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['views'], 'g')\n",
    "    axs[3,3].set(title=\"YouTube cumulative views (zoomed in)\")\n",
    "    axs[3,3].set_ylabel(\"# Views\")\n",
    "    \n",
    "    # zoomed in youtube subs (delta)\n",
    "    axs[4,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', alpha=0.3)\n",
    "    axs[4,2].set(title=\"YouTube delta subscriptions per week (zoomed in)\")\n",
    "    axs[4,2].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_subs'])\n",
    "\n",
    "    # zoomed in youtube subs (cumulative)\n",
    "    axs[4,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['subs'], 'm')\n",
    "    axs[4,3].set(title=\"YouTube cumulative subscriptions (zoomed in)\")\n",
    "    axs[4,3].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', alpha=0.3)\n",
    "    axs[5,2].set(title=\"YouTube videos durations (zoomed in)\")\n",
    "    axs[5,2].set_ylabel(\"Duration\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'])\n",
    "    \n",
    "        \n",
    "   # youtube likes per uploads\n",
    "    axs[6,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', alpha=0.3)\n",
    "    axs[6,2].set(title=\"YouTube likes (plotted against upload date) (zoomed in)\")\n",
    "    axs[6,2].set_ylabel(\"Likes\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['crawl_date'], tmp_df_yt_meta_zoomed['like_count'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################### FORMAT AXES ###################################\n",
    "\n",
    "    # format the axes\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if j < 2:\n",
    "                axs[i,j].set_xlim([date_min, date_max])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            if j >= 2:\n",
    "                axs[i,j].set_xlim([date_min_zoom, date_max_zoom])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "                # axs[i,j].xaxis.set_minor_locator(mdates.WeekdayLocator())\n",
    "            axs[i,j].xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.set_major_formatter(KM_formatter)\n",
    "            \n",
    "            \n",
    "    ################################### PLOT BREAKPOINT LINES AND POINTS ###################################\n",
    "\n",
    "    # plot vertical lines for breakpoint, breakpoint-1month, breakpoint+1month\n",
    "    print_legend = True\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if print_legend:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', label='break', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', label='- ' + str(MONTH_OFFSET.months)+' months', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', label='+' + str(MONTH_OFFSET.months)+' months', linewidth=2)          \n",
    "                # print_legend = False\n",
    "            else:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "    # axs[0,0].legend()\n",
    "    axs[0,1].legend()\n",
    "\n",
    "    # plot point for mean nb of patrons for breakpoint, breakpoint-1month, breakpoint+1month    \n",
    "    axs[0,3].plot(breakpoint_date - MONTH_OFFSET, ts_pt_df.at[(breakpoint_date - MONTH_OFFSET), 'patrons_ma'], marker='o', color='green')\n",
    "    axs[0,3].plot(breakpoint_date,               ts_pt_df.at[breakpoint_date              , 'patrons_ma'], marker='o', color='red')    \n",
    "    axs[0,3].plot(breakpoint_date + MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "\n",
    "\n",
    "    ################################### GRANGER CAUSALITY TESTS ###################################\n",
    "\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    fig.tight_layout(w_pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# print('\\n\\nGranger tests summary statistics:')\n",
    "    \n",
    "# print(f'• Number of patreon accounts analysed (patrons increase ratio > {incr_thresh_ratio}): {len(df_granger)}')\n",
    "# print(f'• Number of patreon with no Granger-causal link: {len(not_granger)} ({len(not_granger)/len(df_granger):.0%})')\n",
    "\n",
    "# print(f'• Number of patreon accounts per Granger-causal link:')\n",
    "# # Converting granger dict into list of tuples (in order to sort it), the 2nd value of the tuple being the count of accounts\n",
    "# granger_list = [(k, len(v)) for k, v in granger_dict.items()]\n",
    "# # sort by count desc\n",
    "# granger_list_desc = sorted(granger_list, key=lambda tup: -tup[1])\n",
    "# for (k,v) in granger_list_desc:\n",
    "#     print(f'    • {k[0]} \\t--> {k[1]}:\\t {v} ({v/len(df_granger):.0%})')\n",
    "\n",
    "\n",
    "# df_granger[columns] = df_granger[columns].astype('Int64')\n",
    "# df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save matches to disk in pickle format\n",
    "# output_file_path = LOCAL_DATA_FOLDER+\"match_pairs_dict.pickle\"\n",
    "\n",
    "# with open(output_file_path, 'wb') as f:\n",
    "#     pickle.dump(match_pairs_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -l {LOCAL_DATA_FOLDER}match_pairs_dict.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(LOCAL_DATA_FOLDER+\"match_pairs_dict.pickle\", 'rb') as f:\n",
    "#     match_pairs_dict = pickle.load(f)\n",
    "# print(f\"Total number of matched pairs loaded: {len(match_pairs_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_balanced = balanced_df.loc[balanced_df['treat'] == 1] #People that attained the program\n",
    "control_balanced = balanced_df.loc[balanced_df['treat'] == 0] #People that didn't attain the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of control accounts  < ratio < {CONTROL_MAX_RATIO}):\\t {len(control_balanced)}')\n",
    "print(f'Number of treated accounts ({TREATED_MIN_RATIO} < ratio):\\t\\t {len(treated_balanced)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_balanced.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_balanced.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of patrons at breakpoint\n",
    "balanced_df.boxplot(by='treat', column='avg_patrons_bkpnt', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "balanced_df.boxplot(by='treat', column='mean_delta_patrons_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "balanced_df.boxplot(by='treat', column='mean_delta_views_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(balanced_df[columns_to_explore_relationships])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution BEFORE matching\n",
    "ax = sns.distplot(treated['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint, BEFORE matching', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution AFTER matching\n",
    "ax = sns.distplot(treated_balanced['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control_balanced['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint, AFTER matching', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granger causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute Granger Tests and store statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granger_columns = [\n",
    "'pt_delta_patrons->yt_delta_videos',\n",
    "'pt_delta_patrons->yt_delta_views',\n",
    "'pt_delta_patrons->yt_delta_subs',\n",
    "'yt_delta_videos->pt_delta_patrons',\n",
    "'yt_delta_views->pt_delta_patrons',\n",
    "'yt_delta_subs->pt_delta_patrons'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare YouTube and Patreon timeseries for top patreon accounts with rolling average - MANUAL VERSION 2\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# WEEK_OFFSET = pd.DateOffset(weeks=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# variables for Granger Tests\n",
    "MAXLAG = 2\n",
    "granger_dict = {} # dictionary with  keys (cause --> effect) and values with list of corresponding patreon account(s)\n",
    "not_granger = []\n",
    "YT_variables = ['yt_delta_videos', 'yt_delta_views', 'yt_delta_subs']\n",
    "# PT_variables = ['pt_delta_patrons', 'pt_delta_earning']\n",
    "PT_variables = ['pt_delta_patrons']\n",
    "\n",
    "df_granger = df_treated.copy()\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in tqdm(df_granger.iterrows()):   \n",
    "\n",
    "    \n",
    "    ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    patreon = row['patreon_id']\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = df_top_pt_accts[df_top_pt_accts['patreon'] == patreon].copy()  \n",
    "    tmp_df_pt = tmp_df_pt.sort_values(by=['date'])\n",
    "    tmp_df_pt = tmp_df_pt.drop_duplicates()\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon].copy()\n",
    "    tmp_df_yt = tmp_df_yt.sort_values(by=['datetime'])\n",
    "    \n",
    "    # replace dates that were collected after 23:00 to their next day, and remove hour\n",
    "    tmp_df_yt['datetime_original'] = tmp_df_yt['datetime']\n",
    "    tmp_df_yt['datetime'] = tmp_df_yt['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "    \n",
    "    # remove hours and convert to datetime type\n",
    "    tmp_df_yt['datetime'] = pd.to_datetime(tmp_df_yt['datetime'].dt.date)\n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    # ch_ids = tmp_df_yt['channel'].unique()\n",
    "    # print(f\"\\n\\n\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "    # print(f\"https://www.{patreon}\")\n",
    "    # print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    # for ch_id in ch_ids:\n",
    "    #     print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "\n",
    "    \n",
    "    ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "    # set min and max dates for plots   \n",
    "    date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "    date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "    if date_max < date_min:\n",
    "        print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "        continue\n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "    tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    \n",
    "    # align both dataframes since youtube starts once a week\n",
    "    tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "               \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    \n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "\n",
    "    \n",
    "    r = row['ratio']\n",
    "\n",
    "#     print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "#     print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "#     print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "#     print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    \n",
    "#     print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "#     print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "#     print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    \n",
    "#     print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "#     print(f\"• Ratio between 2 increases:                            d2/d1  = {r:.2f}\")\n",
    "#     print(f\"• Percentage increase:                            |d2/d1|*100  = {abs(r):>+.0%}\")\n",
    "    \n",
    "\n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = breakpoint_date - (2 * MONTH_OFFSET)\n",
    "    date_max_zoom = breakpoint_date + (2 * MONTH_OFFSET)\n",
    "            \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "\n",
    "\n",
    "    ################################### GRANGER CAUSALITY TESTS ###################################\n",
    "\n",
    "    # create a new dataframe with merged columns (the dates might have a day difference)\n",
    "    selected_pt_columns  = ['delta_earning', 'delta_patrons']\n",
    "    df_pt = ts_pt_weekly_avg_df_zoomed\n",
    "    df_pt = df_pt[selected_pt_columns].reset_index().add_prefix('pt_')\n",
    "\n",
    "    # selected_yt_columns = ['datetime', 'delta_views', 'delta_subs', 'delta_videos']\n",
    "    selected_yt_columns = ['datetime', 'datetime_original', 'delta_views', 'delta_subs', 'delta_videos']\n",
    "    df_yt = tmp_df_yt_zoomed\n",
    "    df_yt = df_yt[selected_yt_columns].reset_index().add_prefix('yt_')\n",
    "\n",
    "    # concatenated 2 dfs and select and reorder columns\n",
    "    df_concat = pd.concat([df_pt, df_yt], axis=1)\n",
    "    concat_columns = ['pt_date', 'yt_datetime', 'pt_delta_earning', 'pt_delta_patrons', 'yt_delta_views', 'yt_delta_subs', 'yt_delta_videos']\n",
    "    df_concat = df_concat[concat_columns]\n",
    "    # df_concat['dates_match'] = df_concat['pt_date'] == df_concat['yt_datetime']\n",
    "    \n",
    "    # display(df_concat.round())\n",
    "    # display(df_concat.style.set_caption(f\"df_concat\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(f\"\\nGranger Causality Tests:\")\n",
    "    \n",
    "    granger_causal_link = False\n",
    "    for pt_var in PT_variables:\n",
    "        for yt_var in YT_variables:\n",
    "            \n",
    "            # if nan values in this df, skip\n",
    "            if df_concat[[yt_var, pt_var]].isna().values.any():\n",
    "                continue\n",
    "                \n",
    "            pvalue_fwd = {}\n",
    "            pvalue_rev = {}\n",
    "            \n",
    "            try:\n",
    "                # print(f'\\n\\n• {pt_var} --> {yt_var}')\n",
    "                granger_test_fwd = grangercausalitytests(df_concat[[yt_var, pt_var]], maxlag=MAXLAG, verbose=False)  \n",
    "                # print(f'\\n\\n• {yt_var} --> {pt_var}')\n",
    "                granger_test_rev = grangercausalitytests(df_concat[[pt_var, yt_var]], maxlag=MAXLAG, verbose=False) \n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "\n",
    "            for lag in range(1, MAXLAG+1):           \n",
    "                pvalue_fwd[lag] = granger_test_fwd[lag][0]['ssr_ftest'][1]\n",
    "                pvalue_rev[lag] = granger_test_rev[lag][0]['ssr_ftest'][1]\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            min_pvalue_fwd = min(pvalue_fwd.values())\n",
    "            if min_pvalue_fwd < 0.05:\n",
    "                granger_causal_link = True\n",
    "                min_lag_fwd = [k for k, v in pvalue_fwd.items() if v == min_pvalue_fwd][0]\n",
    "                # print(f'• {pt_var} --> {yt_var} (pvalue={min_pvalue_fwd:.3f}, lag={min_lag_fwd})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 1\n",
    "\n",
    "                if (pt_var, yt_var) in granger_dict:                   \n",
    "                    granger_dict[(pt_var, yt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(pt_var, yt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 0\n",
    "                \n",
    "            min_pvalue_rev = min(pvalue_rev.values())\n",
    "            if min_pvalue_rev < 0.05:\n",
    "                granger_causal_link = True\n",
    "                min_lag_rev = [k for k, v in pvalue_rev.items() if v == min_pvalue_rev][0]\n",
    "                # print(f'• {yt_var} --> {pt_var} (pvalue={min_pvalue_rev:.3f}, lag={min_lag_rev})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 1\n",
    "                \n",
    "                if (yt_var, pt_var) in granger_dict:\n",
    "                    granger_dict[(yt_var, pt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(yt_var, pt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 0\n",
    "                \n",
    "\n",
    "    if (granger_causal_link == False):\n",
    "        # print(\"• No Granger causality found for this account\")\n",
    "        not_granger.append(patreon)\n",
    "    \n",
    "    # print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "print(f'\\n\\nGranger tests summary statistics: (with maxlag = {MAXLAG})')\n",
    "    \n",
    "print(f'• Number of patreon accounts analysed (patrons increase ratio > {INCR_RATIO_THRESH}): {len(df_granger)}')\n",
    "print(f'• Number of patreon with no Granger-causal link: {len(not_granger)} ({len(not_granger)/len(df_granger):.0%})')\n",
    "\n",
    "print(f'• Number of patreon accounts per Granger-causal link:')\n",
    "\n",
    "# Converting granger dict into list of tuples (in order to sort it), the 2nd value of the tuple being the count of accounts\n",
    "granger_list = [(k, len(v)) for k, v in granger_dict.items()]\n",
    "# sort by count desc\n",
    "granger_list_desc = sorted(granger_list, key=lambda tup: -tup[1])\n",
    "for (k,v) in granger_list_desc:\n",
    "    print(f'    • {k[0]} \\t--> {k[1]}:\\t {v} ({v/len(df_granger):.0%})')\n",
    "\n",
    "df_granger[granger_columns] = df_granger[granger_columns].astype('Int64')\n",
    "df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"df_granger\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "output_file_path = LOCAL_DATA_FOLDER+\"df_granger.tsv.gz\"\n",
    "df_granger.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Granger causality plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {LOCAL_DATA_FOLDER}df_granger.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load granger dataframe\n",
    "df_granger = pd.read_csv(LOCAL_DATA_FOLDER+\"df_granger.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_granger['bkpt_date'] = pd.to_datetime(df_granger['bkpt_date'])\n",
    "df_granger['bkpt_date_sub30'] = pd.to_datetime(df_granger['bkpt_date_sub30'])\n",
    "df_granger['bkpt_date_add30'] = pd.to_datetime(df_granger['bkpt_date_add30'])\n",
    "df_granger[granger_columns] = df_granger[granger_columns].astype('Int64')\n",
    "df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split columns in PT->YT, and reverse YT->PT\n",
    "cols1 = [\n",
    "'pt_delta_patrons->yt_delta_videos',\n",
    "'pt_delta_patrons->yt_delta_views',\n",
    "'pt_delta_patrons->yt_delta_subs'\n",
    "]\n",
    "cols2 = [\n",
    "'yt_delta_videos->pt_delta_patrons',\n",
    "'yt_delta_views->pt_delta_patrons',\n",
    "'yt_delta_subs->pt_delta_patrons'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For different minimum ratios of increase, plot sum of Granger-causal links between Patreon and YouTube time-series (in blue) and vice-versa (in orange)\n",
    "\n",
    "nb_plots = 10\n",
    "sbplt_cols = 5\n",
    "sbplt_rows = int(nb_plots / sbplt_cols)\n",
    "\n",
    "fig, axs = plt.subplots(sbplt_rows, sbplt_cols, figsize=(16,12), sharey=True, sharex=True)\n",
    "for idx in range(0, nb_plots):\n",
    "\n",
    "    row = math.floor(idx/sbplt_cols)\n",
    "    col = idx % sbplt_cols\n",
    "    sbplt = axs[row, col]\n",
    "    \n",
    "    ratio_df = df_granger[df_granger['ratio'] > idx+1]\n",
    "    \n",
    "    # print(f'\\n\\nratio > {idx+1}:')\n",
    "    # print(f'total number of accounts: {len(ratio_df)}:')\n",
    "    # no_causal_links_df = ratio_df[ratio_df[cols1 + cols2].sum(axis=1) == 0]\n",
    "    # print(f'nb accts with no Granger-causal links: {len(no_causal_links_df)} ({len(no_causal_links_df)/len(ratio_df):.0%})')\n",
    "    # print(f'\\nPatreon --> YouTube:')\n",
    "    # print(ratio_df[cols1].sum())\n",
    "    # print(f'\\nYouTube --> Patreon:')\n",
    "    # print(ratio_df[cols2].sum())\n",
    "    \n",
    "    granger_series = ratio_df[cols1 + cols2].sum()/len(ratio_df)\n",
    "    sbplt.bar(granger_series[cols1].index, granger_series[cols1].values, label='PT --> YT')\n",
    "    sbplt.bar(granger_series[cols2].index, granger_series[cols2].values, label='YT --> PT')\n",
    "    sbplt.set_title(f\"ratio > {idx+1}\\n # accnts = {len(ratio_df)}\")\n",
    "    # sbplt.set_xlabel(\"Granger-causal links\")\n",
    "    # sbplt.set_ylabel(\"% of PT accts\")\n",
    "    sbplt.tick_params(labelrotation=90)\n",
    "    \n",
    "\n",
    "axs[0, 0].legend()\n",
    "axs[1, 4].legend()\n",
    "\n",
    "\n",
    "fig.suptitle('Granger-causal links between Patreon and YouTube time-series, for different minimum ratios of increase at breakpoint \\n (one account can have multiple causal-links)', fontweight=\"bold\")\n",
    "fig.text(0.5,0, 'Granger-causal links')\n",
    "fig.text(0,0.5, 'Percentage of Patreon accts ', rotation = 90)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=3, w_pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Although there are granger links in both directions, we notice that there are more granger links going from Patreon --> YouTube, than YouTube --> Patreon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 Plot PT time-series, YT time-series, and YT metadata + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_plot(ax, x, y, title, x_axis_label=\"default x\", y_axis_label=\"default y\", color=\"#1f77b4\", alpha=1):\n",
    "#     ax.plot(x, y, color, alpha)\n",
    "#     ax.set(title=title)\n",
    "#     ax.set_xlabel(x_axis_label)    \n",
    "#     ax.set_ylabel(y_axis_label)    \n",
    "\n",
    "# custom_plot(axs[0,0], ts_pt_df['date'], ts_pt_df['patrons'], alpha=0.2)\n",
    "# custom_plot(axs[0,0], ts_pt_df['date'], ts_pt_df['patrons_ravg'], \"Number of patrons\", y_axis_label=\"# Patrons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_neg_pos(ax, x, y):\n",
    "    if y.isnull().all():\n",
    "        return\n",
    "    if (y.min() < 0): \n",
    "        # fill negative values in red and draw a horizontal line at 0\n",
    "        ax.fill_between(x, y.min(), 0, color='red', alpha=0.05)\n",
    "        ax.axhline(y=0, linestyle='solid', color= 'black', linewidth=0.5)\n",
    "    # fill positive values in green\n",
    "    # ax.fill_between(x, 0, y.max(), color='green', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number in thousand (k) or in million (M) on y axis\n",
    "def KM(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    if x > 999_999:\n",
    "        return '%2.1fM' % (x * 1e-6)\n",
    "    elif x > 999:\n",
    "        return '%2.1fK' % (x * 1e-3)\n",
    "    else:\n",
    "        return '%3.0f ' % (x)\n",
    "KM_formatter = FuncFormatter(KM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter accounts that match selected Patreon ids\n",
    "df_yt_metadata_pt_filtered = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(df_treated['patreon_id'])].copy()\n",
    "print(f'Filter accounts that match selected Patreon ids: {len(df_yt_metadata_pt_filtered):,} ({len(df_yt_metadata_pt_filtered)/len(df_yt_metadata_pt):.1%} of videos containing a PT accounts) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt_filtered['crawl_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['crawl_date'])\n",
    "df_yt_metadata_pt_filtered['upload_date'] = pd.to_datetime(df_yt_metadata_pt_filtered['upload_date'])\n",
    "df_yt_metadata_pt_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare Patreon and YouTube timeseries + YouTube metadata\n",
    "MONTH_OFFSET = pd.DateOffset(months=1)\n",
    "# WEEK_OFFSET = pd.DateOffset(weeks=1)\n",
    "ROLLING_AVG_WINDOW = 30\n",
    "\n",
    "# variables for Granger Tests\n",
    "MAXLAG = 2\n",
    "granger_dict = {} # dictionary with  keys (cause --> effect) and values with list of corresponding patreon account(s)\n",
    "not_granger = []\n",
    "YT_variables = ['yt_delta_videos', 'yt_delta_views', 'yt_delta_subs']\n",
    "# PT_variables = ['pt_delta_patrons', 'pt_delta_earning']\n",
    "PT_variables = ['pt_delta_patrons']\n",
    "\n",
    "# df_granger = df_pt_bkpnt_filt.copy()\n",
    "\n",
    "# LOOP OVER TOP PATREON ACCOUNTS\n",
    "for idx, row in df_treated.iterrows():\n",
    "    fig, axs = plt.subplots(7, 4, figsize=(26, 10), sharey=False, sharex=False)\n",
    "    \n",
    "    \n",
    "    patreon = row['patreon_id']\n",
    "    \n",
    "    ########################## RESTRICT DATAFRAMES TO 1 PATREON ACCOUNT ##########################\n",
    "\n",
    "    # patreon earnings and users\n",
    "    tmp_df_pt = df_top_pt_accts[df_top_pt_accts['patreon'] == patreon].copy()  \n",
    "    tmp_df_pt = tmp_df_pt.sort_values(by=['date'])\n",
    "    tmp_df_pt = tmp_df_pt.drop_duplicates()\n",
    "\n",
    "    # youtube videos\n",
    "    tmp_df_yt = df_yt_timeseries_top_pt[df_yt_timeseries_top_pt['patreon_id'] == patreon].copy()\n",
    "    tmp_df_yt = tmp_df_yt.sort_values(by=['datetime'])\n",
    "    \n",
    "    # youtube metadata\n",
    "    tmp_df_yt_meta = df_yt_metadata_pt_filtered[df_yt_metadata_pt_filtered['patreon_id'] == patreon].copy()   \n",
    "    tmp_df_yt_meta = tmp_df_yt_meta.sort_values('upload_date')\n",
    "    # tmp_df_yt_meta['upload_date'] = pd.to_datetime(tmp_df_yt_meta['upload_date'])\n",
    "    \n",
    "    # replace dates that were collected after 23:00 to their next day, and remove hour\n",
    "    tmp_df_yt['datetime_original'] = tmp_df_yt['datetime']\n",
    "    tmp_df_yt['datetime'] = tmp_df_yt['datetime'].apply(lambda date: (date + pd.DateOffset(days=1)) if date.hour >= 23 else date) \n",
    "    \n",
    "    # remove hours and convert to datetime type\n",
    "    tmp_df_yt['datetime'] = pd.to_datetime(tmp_df_yt['datetime'].dt.date)\n",
    "    \n",
    "    \n",
    "    ########################## PRINT TITLES ##########################\n",
    "    \n",
    "    # print URLs for patreon, graphtreon, YT channel(s) related to this patreon account, and breakpoint date\n",
    "    ch_ids = tmp_df_yt['channel'].unique()\n",
    "    print(f\"\\n\\n\\n\\033[1mRank {idx+1}: {patreon[12:]} \\033[0m\")\n",
    "    print(f\"https://www.{patreon}\")\n",
    "    print(f\"https://graphtreon.com/creator/{patreon[12:]}\")\n",
    "    for ch_id in ch_ids:\n",
    "        print(f\"https://youtube.com/channel/{ch_id}\")\n",
    "   \n",
    "    print(f'\\nYouTube Metadata: ')\n",
    "    print('• YT videos were uploaded between {} and {}'.format(tmp_df_yt_meta['upload_date'].min().strftime('%B %d, %Y'),\n",
    "                                                             tmp_df_yt_meta['upload_date'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "    print('• YT metadata was crawled between {} and {}'.format(tmp_df_yt_meta['crawl_date'].min().strftime('%B %d, %Y'),\n",
    "                                                             tmp_df_yt_meta['crawl_date'].max().strftime('%B %d, %Y')))\n",
    "    \n",
    "    ########################## RESTRICT DATES FOR ZOOM OUT ##########################\n",
    "    \n",
    "    # set min and max dates for plots   \n",
    "    date_min = max([tmp_df_yt['datetime'].min(), tmp_df_pt['date'].min()])\n",
    "    date_max = min([tmp_df_yt['datetime'].max(), tmp_df_pt['date'].max()])\n",
    "    \n",
    "    if date_max < date_min:\n",
    "        print(f\":( no overlapping period between YouTube and Patreon datasets\\n\")\n",
    "        continue\n",
    "    \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt = tmp_df_pt[(tmp_df_pt['date'] >= date_min) & (tmp_df_pt['date'] <= date_max)]\n",
    "    tmp_df_yt = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min) & (tmp_df_yt['datetime'] <= date_max)]\n",
    "    tmp_df_yt_meta = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min) & (tmp_df_yt_meta['upload_date'] <= date_max)]\n",
    "    \n",
    "    # align both dataframes since youtube starts once a week\n",
    "    tmp_df_pt = tmp_df_pt[tmp_df_pt['date'] >= tmp_df_yt['datetime'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## PATREON: CALCULATE MOVING AVERAGE AND WEEKLY DELTAS ##########################\n",
    "    \n",
    "    tmp_df_pt['patrons_ma'] = tmp_df_pt['patrons'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    tmp_df_pt['earning_ma'] = tmp_df_pt['earning'].rolling(ROLLING_AVG_WINDOW, center=True).mean()\n",
    "    ts_pt_df = tmp_df_pt.set_index(tmp_df_pt['date']) # set the date as the index\n",
    "    \n",
    "    # resample time series to get 7 days intervals in order to calculate weekly deltas\n",
    "    ts_pt_weekly_avg_df = ts_pt_df.resample('7D').mean()\n",
    "    ts_pt_weekly_avg_df['delta_patrons'] = ts_pt_weekly_avg_df['patrons'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df['delta_earning'] = ts_pt_weekly_avg_df['earning'].diff(periods=1)\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[1:]  # remove 1st row (which is NA)\n",
    "    tmp_df_yt = tmp_df_yt[1:] # remove YT 1st row to start at the same time as PT\n",
    "    \n",
    "    # reorder columns to have deltas columns next to their respective columns\n",
    "    patreon_column_names = ['earning', 'delta_earning', 'earning_ma', 'patrons', 'delta_patrons', 'patrons_ma']\n",
    "    ts_pt_weekly_avg_df = ts_pt_weekly_avg_df[patreon_column_names]\n",
    "    \n",
    "    # convert Float64 columns to float64 to avoid Matplotlib NAType error\n",
    "    ts_pt_weekly_avg_df_float64 = ts_pt_weekly_avg_df.astype({'patrons': 'float64', 'delta_patrons': 'float64'})\n",
    "    \n",
    "              \n",
    "    ########################## DETECT BREAKPOINT AND REJECT PATREON ACCOUNT IF NOT VALID ##########################\n",
    "\n",
    "    breakpoint_date = row['bkpt_date']\n",
    "    # breakpoint_date = find_breakpoint_v2(tmp_df_pt, 'patrons_ma')\n",
    "    # print(\"Breakpoint date: \", breakpoint_date.date())\n",
    "\n",
    "    # check that dates prior and after breakpoint exist\n",
    "    if not (((breakpoint_date - 1*MONTH_OFFSET)) in ts_pt_df.index and ((breakpoint_date + 1*MONTH_OFFSET) in ts_pt_df.index)):\n",
    "        print(f\"ERROR: Breakpoint too close to edge of patreon time series or missing data\\n\")\n",
    "        plt.figure().clear(); plt.close(); plt.cla(); plt.clf(); plt.show()\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ################################### CALCULATE INCREASE AND REJECT IF NOT VALID OR LESS THAN THRESHOLD ###################################\n",
    "\n",
    "    avg_patrons_bkpnt = row['avg_patrons_bkpnt']\n",
    "    avg_patrons_sub30 = row['avg_patrons_sub30']\n",
    "    avg_patrons_add30 = row['avg_patrons_add30']\n",
    "    \n",
    "    bkpt_date       = row['bkpt_date']\n",
    "    bkpt_date_sub30 = row['bkpt_date_sub30']\n",
    "    bkpt_date_add30 = row['bkpt_date_add30']\n",
    "    \n",
    "    d1 = row['d1']\n",
    "    d2 = row['d2']\n",
    "\n",
    "    \n",
    "    r = row['ratio']\n",
    "\n",
    "    print(f'\\nAverage number of patrons: (values calculated using a 30 days centered moving average)')\n",
    "    print(f'• At breakpoint - 30days ({bkpt_date_sub30.date()}): {avg_patrons_sub30:,.1f}')\n",
    "    print(f'• At breakpoint          ({bkpt_date.date()}): {avg_patrons_bkpnt:,.1f}')\n",
    "    print(f'• At breakpoint + 30days ({bkpt_date_add30.date()}): {avg_patrons_add30:,.1f}')\n",
    "    \n",
    "    print(f'\\nIncrease of patrons in the period before and after the breakpoint:')\n",
    "    print(f\"• Increase of patrons from {bkpt_date_sub30.date()} to {bkpt_date.date()}:        d1  = {d1:>+6.1f} patrons\")\n",
    "    print(f\"• Increase of patrons from {bkpt_date.date()} to {bkpt_date_add30.date()}:        d2  = {d2:>+6.1f} patrons\")\n",
    "    \n",
    "    print(f'\\nRatio of the increases of the 2 periods: ')\n",
    "    print(f\"• Ratio between 2 increases:                            d2/d1  = {r:.2f}\")\n",
    "    print(f\"• Percentage increase:                            |d2/d1|*100  = {abs(r):>+.0%}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ################################### CALCULATE DELTA MEANS BEFORE AND AFTER BKPOINT ###################################  \n",
    "    \n",
    "    ##### PATREON #####\n",
    "    tmp_df_PT_sub30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpt_date_sub30) & (ts_pt_weekly_avg_df_float64.index <= bkpt_date)]\n",
    "    tmp_df_PT_add30 = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= bkpt_date) & (ts_pt_weekly_avg_df_float64.index <= bkpt_date_add30)]\n",
    "\n",
    "    # delta patrons\n",
    "    mean_delta_patrons_befor = tmp_df_PT_sub30['delta_patrons'].mean()\n",
    "    mean_delta_patrons_after = tmp_df_PT_add30['delta_patrons'].mean()\n",
    "        \n",
    "    # delta earnings\n",
    "    mean_delta_earnings_befor = tmp_df_PT_sub30['delta_earning'].mean()\n",
    "    mean_delta_earnings_after = tmp_df_PT_add30['delta_earning'].mean()  \n",
    "\n",
    "    \n",
    "    ##### YOUTUBE TIME SERIES #####\n",
    "    tmp_df_YT_sub30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpt_date_sub30) & (tmp_df_yt['datetime'] <= bkpt_date      )]\n",
    "    tmp_df_YT_add30 = tmp_df_yt[(tmp_df_yt['datetime'] >= bkpt_date      ) & (tmp_df_yt['datetime'] <= bkpt_date_add30)]\n",
    "    \n",
    "    # delta videos\n",
    "    mean_delta_videos_befor = tmp_df_YT_sub30['delta_videos'].mean()\n",
    "    mean_delta_videos_after = tmp_df_YT_add30['delta_videos'].mean()  \n",
    "\n",
    "    # delta views\n",
    "    mean_delta_views_befor = tmp_df_YT_sub30['delta_views'].mean()\n",
    "    mean_delta_views_after = tmp_df_YT_add30['delta_views'].mean()  \n",
    "\n",
    "    # delta subscriptions\n",
    "    mean_delta_subs_befor = tmp_df_YT_sub30['delta_subs'].mean()\n",
    "    mean_delta_subs_after = tmp_df_YT_add30['delta_subs'].mean()  \n",
    "\n",
    "    \n",
    "    ##### YOUTUBE METADATA #####\n",
    "    tmp_df_YT_META_sub30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpt_date_sub30) & (tmp_df_yt_meta['upload_date'] <= bkpt_date      )]\n",
    "    tmp_df_YT_META_add30 = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= bkpt_date      ) & (tmp_df_yt_meta['upload_date'] <= bkpt_date_add30)]\n",
    "        \n",
    "    # durations\n",
    "    mean_duration_befor = tmp_df_YT_META_sub30['duration'].mean()\n",
    "    mean_duration_after = tmp_df_YT_META_add30['duration'].mean()      \n",
    "        \n",
    "    # likes\n",
    "    mean_likes_befor = tmp_df_YT_META_sub30['like_count'].mean()\n",
    "    mean_likes_after = tmp_df_YT_META_add30['like_count'].mean()      \n",
    "        \n",
    "    \n",
    "    # plot dots in the middle of region for the region means   \n",
    "    axs[0,2].plot(tmp_df_PT_sub30.index.mean(), mean_delta_patrons_befor, marker='o', color='green', markersize=15)\n",
    "    axs[0,2].plot(tmp_df_PT_add30.index.mean(), mean_delta_patrons_after, marker='o', color='orange', markersize=15)\n",
    "    axs[1,2].plot(tmp_df_PT_sub30.index.mean(), mean_delta_earnings_befor, marker='o', color='green', markersize=15)\n",
    "    axs[1,2].plot(tmp_df_PT_add30.index.mean(), mean_delta_earnings_after, marker='o', color='orange', markersize=15)\n",
    "    axs[2,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_videos_befor, marker='o', color='green', markersize=15)\n",
    "    axs[2,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_videos_after, marker='o', color='orange', markersize=15)\n",
    "    axs[3,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_views_befor, marker='o', color='green', markersize=15)\n",
    "    axs[3,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_views_after, marker='o', color='orange', markersize=15)  \n",
    "    axs[4,2].plot(tmp_df_YT_sub30['datetime'].mean(), mean_delta_subs_befor, marker='o', color='green', markersize=15)\n",
    "    axs[4,2].plot(tmp_df_YT_add30['datetime'].mean(), mean_delta_subs_after, marker='o', color='orange', markersize=15)\n",
    "    \n",
    "    # sometimes there is no value at all for this period of time in YT meta --> error when plotting\n",
    "    if not (tmp_df_YT_META_sub30.empty or tmp_df_YT_META_add30.empty):\n",
    "        axs[5,2].plot(tmp_df_YT_META_sub30['upload_date'].mean(), mean_duration_befor, marker='o', color='green', markersize=15)\n",
    "        axs[5,2].plot(tmp_df_YT_META_add30['upload_date'].mean(), mean_duration_after, marker='o', color='orange', markersize=15)  \n",
    "        axs[6,2].plot(tmp_df_YT_META_sub30['upload_date'].mean(), mean_likes_befor, marker='o', color='green', markersize=15)\n",
    "        axs[6,2].plot(tmp_df_YT_META_add30['upload_date'].mean(), mean_likes_after, marker='o', color='orange', markersize=15)  \n",
    "\n",
    "    \n",
    "    # plot horizontal lines for means\n",
    "    mean_befor_list = [mean_delta_patrons_befor, mean_delta_earnings_befor, mean_delta_videos_befor, mean_delta_views_befor, mean_delta_subs_befor, mean_duration_befor, mean_likes_befor]\n",
    "    mean_afer_list = [mean_delta_patrons_after, mean_delta_earnings_after, mean_delta_videos_after, mean_delta_views_after, mean_delta_subs_after, mean_duration_after, mean_likes_after]\n",
    "       \n",
    "    for idx, mean in enumerate(mean_befor_list):\n",
    "            if not math.isnan(mean):\n",
    "                axs[idx,2].hlines(y=mean, xmin=bkpt_date_sub30, xmax=bkpt_date      , linewidth=2, linestyle='--', color='green')\n",
    "\n",
    "    for idx, mean in enumerate(mean_afer_list):\n",
    "            if not math.isnan(mean):\n",
    "                axs[idx,2].hlines(y=mean, xmin=bkpt_date,       xmax=bkpt_date_add30, linewidth=2, linestyle='--', color='orange')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    ################################### ZOOM OUT PLOTS ###################################\n",
    "    \n",
    "    # number of patrons (delta)\n",
    "    axs[0,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,0].set(title=\"Delta patrons per week\")\n",
    "    axs[0,0].set_ylabel(\"Δ Patrons\")    \n",
    "    color_neg_pos(axs[0,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_patrons'])\n",
    "\n",
    "    # number of patrons (cumulative)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons'], alpha=0.2)\n",
    "    axs[0,1].plot(tmp_df_pt['date'], tmp_df_pt['patrons_ma'])\n",
    "    axs[0,1].set(title=\"Number of patrons\")\n",
    "    axs[0,1].set_ylabel(\"# Patrons\")\n",
    "\n",
    "    # patreon earnings (delta)\n",
    "    axs[1,0].scatter(ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,0].set(title=\"Patreon delta earnings per week\")\n",
    "    axs[1,0].set_ylabel(\"Δ Earnings\") \n",
    "    color_neg_pos(axs[1,0], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_float64['delta_earning'])\n",
    "\n",
    "    # patreon earnings (cumulative)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning'], alpha=0.2)\n",
    "    axs[1,1].plot(tmp_df_pt['date'], tmp_df_pt['earning_ma'], color='royalblue')\n",
    "    axs[1,1].set(title=\"Patreon earnings per month\")\n",
    "    axs[1,1].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # youtube videos (delta)\n",
    "    axs[2,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,0].set(title=\"YouTube delta videos per week\")\n",
    "    axs[2,0].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,0], tmp_df_yt['datetime'], tmp_df_yt['delta_videos'])\n",
    "\n",
    "    # youtube videos (cumulative)\n",
    "    axs[2,1].plot(tmp_df_yt['datetime'], tmp_df_yt['videos'], 'r')\n",
    "    axs[2,1].set(title=\"YouTube cumulative videos\")\n",
    "    axs[2,1].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # youtube views (delta)\n",
    "    axs[3,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,0].set(title=\"YouTube delta views per week\")\n",
    "    axs[3,0].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,0], tmp_df_yt['datetime'], tmp_df_yt['delta_views'])\n",
    "\n",
    "    # youtube views (cumulative)\n",
    "    axs[3,1].plot(tmp_df_yt['datetime'], tmp_df_yt['views'], 'g')\n",
    "    axs[3,1].set(title=\"YouTube cumulative views\")\n",
    "    axs[3,1].set_ylabel(\"# Views\")\n",
    "\n",
    "    # youtube subs (delta)\n",
    "    axs[4,0].scatter(tmp_df_yt['datetime'], tmp_df_yt['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,0].set(title=\"YouTube delta subscriptions per week\")\n",
    "    axs[4,0].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,0], tmp_df_yt['datetime'], tmp_df_yt['delta_subs'])\n",
    "\n",
    "    # youtube subs (cumulative)\n",
    "    axs[4,1].plot(tmp_df_yt['datetime'], tmp_df_yt['subs'], 'm')\n",
    "    axs[4,1].set(title=\"YouTube cumulative subscriptions\")\n",
    "    axs[4,1].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,0].set(title=\"YouTube videos durations\")\n",
    "    axs[5,0].set_ylabel(\"Duration\")\n",
    "    \n",
    "    \n",
    "    # youtube likes at crawl date\n",
    "    axs[6,0].scatter(tmp_df_yt_meta['upload_date'], tmp_df_yt_meta['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,0].set(title=\"YouTube likes (plotted against upload date)\")\n",
    "    axs[6,0].set_ylabel(\"Likes\")\n",
    "    \n",
    "\n",
    "    ########################## RESTRICT DATES FOR ZOOM IN (+/- 2 months around breakpoint) ##########################\n",
    "\n",
    "    # calculate min and max dates for zoom\n",
    "    date_min_zoom = breakpoint_date - (2 * MONTH_OFFSET)\n",
    "    date_max_zoom = breakpoint_date + (2 * MONTH_OFFSET)\n",
    "            \n",
    "    # restrict datasets between min and max dates\n",
    "    tmp_df_pt_zoomed = tmp_df_pt[(tmp_df_pt['date'] >= date_min_zoom) & (tmp_df_pt['date'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_zoomed = tmp_df_yt[(tmp_df_yt['datetime'] >= date_min_zoom) & (tmp_df_yt['datetime'] <= date_max_zoom)].copy()\n",
    "    tmp_df_yt_meta_zoomed = tmp_df_yt_meta[(tmp_df_yt_meta['upload_date'] >= date_min_zoom) & (tmp_df_yt_meta['upload_date'] <= date_max_zoom)].copy()\n",
    "\n",
    "    # used for coloration\n",
    "    ts_pt_weekly_avg_df_zoomed = ts_pt_weekly_avg_df_float64[(ts_pt_weekly_avg_df_float64.index >= date_min_zoom) & (ts_pt_weekly_avg_df_float64.index <= date_max_zoom)]\n",
    "    \n",
    "    \n",
    "   ################################### ZOOM IN PLOTS  ###################################\n",
    "\n",
    "    # zoomed in patron numbers (delta)\n",
    "    axs[0,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', s=30, marker='+')\n",
    "    axs[0,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'], c='orange', alpha=0.3)\n",
    "    axs[0,2].set(title=\"Delta patrons per week\")\n",
    "    axs[0,2].set_ylabel(\"Δ Patrons\")\n",
    "    color_neg_pos(axs[0,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_patrons'])\n",
    "    \n",
    "    # zoomed in patron numbers (cumulative)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons'], alpha=0.2)\n",
    "    axs[0,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['patrons_ma'])\n",
    "    axs[0,3].set(title=\"Number of patrons (zoomed in)\")\n",
    "    axs[0,3].set_ylabel(\"# Patrons\")\n",
    "    \n",
    "    # zoomed in patron earnings (delta)\n",
    "    axs[1,2].scatter(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', s=30, marker='+')\n",
    "    axs[1,2].plot(ts_pt_weekly_avg_df_zoomed.index, ts_pt_weekly_avg_df_zoomed['delta_earning'], color='royalblue', alpha=0.3)\n",
    "    axs[1,2].set(title=\"Delta Patreon earnings per week (zoomed in)\")\n",
    "    axs[1,2].set_ylabel(\"Earnings\")  \n",
    "    color_neg_pos(axs[1,2], ts_pt_weekly_avg_df_float64.index, ts_pt_weekly_avg_df_zoomed['delta_earning'])\n",
    "\n",
    "    # zoomed in patron earnings (cumulative)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning'], alpha=0.2)\n",
    "    axs[1,3].plot(tmp_df_pt_zoomed['date'], tmp_df_pt_zoomed['earning_ma'], color='royalblue')\n",
    "    axs[1,3].set(title=\"Patreon earnings per month (zoomed in)\")\n",
    "    axs[1,3].set_ylabel(\"Earnings\")\n",
    "    \n",
    "    # zoomed in youtube videos (delta)\n",
    "    axs[2,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', s=30, marker='+')\n",
    "    axs[2,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_videos'], c='r', alpha=0.3)\n",
    "    axs[2,2].set(title=\"YouTube delta videos per week (zoomed in)\")\n",
    "    axs[2,2].set_ylabel(\"Δ Videos\")\n",
    "    color_neg_pos(axs[2,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_videos'])\n",
    "\n",
    "    # zoomed in youtube videos (cumulative)\n",
    "    axs[2,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['videos'], 'r')\n",
    "    axs[2,3].set(title=\"YouTube cumulative videos (zoomed in)\")\n",
    "    axs[2,3].set_ylabel(\"# Videos\")\n",
    "\n",
    "    # zoomed in youtube views (delta)\n",
    "    axs[3,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', s=30, marker='+')\n",
    "    axs[3,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_views'], c='g', alpha=0.3)\n",
    "    axs[3,2].set(title=\"YouTube delta views per week (zoomed in)\")\n",
    "    axs[3,2].set_ylabel(\"Δ Views\")\n",
    "    color_neg_pos(axs[3,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_views'])\n",
    "\n",
    "    # zoomed in youtube views (cumulative)\n",
    "    axs[3,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['views'], 'g')\n",
    "    axs[3,3].set(title=\"YouTube cumulative views (zoomed in)\")\n",
    "    axs[3,3].set_ylabel(\"# Views\")\n",
    "    \n",
    "    # zoomed in youtube subs (delta)\n",
    "    axs[4,2].scatter(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', s=30, marker='+')\n",
    "    axs[4,2].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['delta_subs'], c='m', alpha=0.3)\n",
    "    axs[4,2].set(title=\"YouTube delta subscriptions per week (zoomed in)\")\n",
    "    axs[4,2].set_ylabel(\"Δ Subscriptions\")\n",
    "    color_neg_pos(axs[4,2], tmp_df_yt['datetime'], tmp_df_yt_zoomed['delta_subs'])\n",
    "\n",
    "    # zoomed in youtube subs (cumulative)\n",
    "    axs[4,3].plot(tmp_df_yt_zoomed['datetime'], tmp_df_yt_zoomed['subs'], 'm')\n",
    "    axs[4,3].set(title=\"YouTube cumulative subscriptions (zoomed in)\")\n",
    "    axs[4,3].set_ylabel(\"# Subscriptions\")\n",
    "    \n",
    "    \n",
    "    # youtube durations per uploads\n",
    "    axs[5,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', s=30, marker='+')\n",
    "    axs[5,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'], c='brown', alpha=0.3)\n",
    "    axs[5,2].set(title=\"YouTube videos durations (zoomed in)\")\n",
    "    axs[5,2].set_ylabel(\"Duration\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['duration'])\n",
    "    \n",
    "        \n",
    "   # youtube likes per uploads\n",
    "    axs[6,2].scatter(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', s=30, marker='+')\n",
    "    axs[6,2].plot(tmp_df_yt_meta_zoomed['upload_date'], tmp_df_yt_meta_zoomed['like_count'], c='lightblue', alpha=0.3)\n",
    "    axs[6,2].set(title=\"YouTube likes (plotted against upload date) (zoomed in)\")\n",
    "    axs[6,2].set_ylabel(\"Likes\")\n",
    "    color_neg_pos(axs[5,2], tmp_df_yt_meta_zoomed['crawl_date'], tmp_df_yt_meta_zoomed['like_count'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################### FORMAT AXES ###################################\n",
    "\n",
    "    # format the axes\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if j < 2:\n",
    "                axs[i,j].set_xlim([date_min, date_max])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            if j >= 2:\n",
    "                axs[i,j].set_xlim([date_min_zoom, date_max_zoom])\n",
    "                axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "                # axs[i,j].xaxis.set_minor_locator(mdates.WeekdayLocator())\n",
    "            axs[i,j].xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "            axs[i,j].yaxis.set_major_formatter(KM_formatter)\n",
    "            \n",
    "            \n",
    "    ################################### PLOT BREAKPOINT LINES AND POINTS ###################################\n",
    "\n",
    "    # plot vertical lines for breakpoint, breakpoint-1month, breakpoint+1month\n",
    "    print_legend = True\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            if print_legend:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', label='break', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', label='- ' + str(MONTH_OFFSET.months)+' months', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', label='+' + str(MONTH_OFFSET.months)+' months', linewidth=2)          \n",
    "                # print_legend = False\n",
    "            else:\n",
    "                axs[i,j].axvline(breakpoint_date, color='red', linestyle='--', linewidth=2.5)\n",
    "                axs[i,j].axvline(breakpoint_date - MONTH_OFFSET, color='green', linestyle=':', linewidth=2)\n",
    "                axs[i,j].axvline(breakpoint_date + MONTH_OFFSET, color='orange', linestyle=':', linewidth=2)\n",
    "    # axs[0,0].legend()\n",
    "    axs[0,1].legend()\n",
    "\n",
    "    # plot point for mean nb of patrons for breakpoint, breakpoint-1month, breakpoint+1month    \n",
    "    axs[0,3].plot(breakpoint_date - MONTH_OFFSET, ts_pt_df.at[(breakpoint_date - MONTH_OFFSET), 'patrons_ma'], marker='o', color='green')\n",
    "    axs[0,3].plot(breakpoint_date,               ts_pt_df.at[breakpoint_date              , 'patrons_ma'], marker='o', color='red')    \n",
    "    axs[0,3].plot(breakpoint_date + MONTH_OFFSET, ts_pt_df.at[(breakpoint_date + MONTH_OFFSET), 'patrons_ma'], marker='o', color='orange')    \n",
    "\n",
    "\n",
    "    ################################### GRANGER CAUSALITY TESTS ###################################\n",
    "\n",
    "    # create a new dataframe with merged columns (the dates might have a day difference)\n",
    "    selected_pt_columns  = ['delta_earning', 'delta_patrons']\n",
    "    df_pt = ts_pt_weekly_avg_df_zoomed\n",
    "    df_pt = df_pt[selected_pt_columns].reset_index().add_prefix('pt_')\n",
    "\n",
    "    # selected_yt_columns = ['datetime', 'delta_views', 'delta_subs', 'delta_videos']\n",
    "    selected_yt_columns = ['datetime', 'datetime_original', 'delta_views', 'delta_subs', 'delta_videos']\n",
    "    df_yt = tmp_df_yt_zoomed\n",
    "    df_yt = df_yt[selected_yt_columns].reset_index().add_prefix('yt_')\n",
    "\n",
    "    # concatenated 2 dfs and select and reorder columns\n",
    "    df_concat = pd.concat([df_pt, df_yt], axis=1)\n",
    "    concat_columns = ['pt_date', 'yt_datetime', 'pt_delta_earning', 'pt_delta_patrons', 'yt_delta_views', 'yt_delta_subs', 'yt_delta_videos']\n",
    "    df_concat = df_concat[concat_columns]\n",
    "    # df_concat['dates_match'] = df_concat['pt_date'] == df_concat['yt_datetime']\n",
    "    \n",
    "    # display(df_concat.round())\n",
    "    # display(df_concat.style.set_caption(f\"df_concat\"))\n",
    "    \n",
    "    \n",
    "    print(f\"\\nGranger Causality Tests:\")\n",
    "    \n",
    "    granger_causal_link = False\n",
    "    for pt_var in PT_variables:\n",
    "        for yt_var in YT_variables:\n",
    "            \n",
    "            # if nan values in this df, skip\n",
    "            if df_concat[[yt_var, pt_var]].isna().values.any():\n",
    "                continue\n",
    "                \n",
    "            pvalue_fwd = {}\n",
    "            pvalue_rev = {}\n",
    "            \n",
    "            try:\n",
    "                # print(f'\\n\\n• {pt_var} --> {yt_var}')\n",
    "                granger_test_fwd = grangercausalitytests(df_concat[[yt_var, pt_var]], maxlag=MAXLAG, verbose=False)  \n",
    "                # print(f'\\n\\n• {yt_var} --> {pt_var}')\n",
    "                granger_test_rev = grangercausalitytests(df_concat[[pt_var, yt_var]], maxlag=MAXLAG, verbose=False) \n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "\n",
    "            for lag in range(1, MAXLAG+1):           \n",
    "                pvalue_fwd[lag] = granger_test_fwd[lag][0]['ssr_ftest'][1]\n",
    "                pvalue_rev[lag] = granger_test_rev[lag][0]['ssr_ftest'][1]\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            min_pvalue_fwd = min(pvalue_fwd.values())\n",
    "            if min_pvalue_fwd < 0.05:\n",
    "                granger_causal_link = True\n",
    "                min_lag_fwd = [k for k, v in pvalue_fwd.items() if v == min_pvalue_fwd][0]\n",
    "                print(f'• {pt_var} --> {yt_var} (pvalue={min_pvalue_fwd:.3f}, lag={min_lag_fwd})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 1\n",
    "\n",
    "                if (pt_var, yt_var) in granger_dict:                   \n",
    "                    granger_dict[(pt_var, yt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(pt_var, yt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, pt_var+'->'+yt_var] = 0\n",
    "                \n",
    "                \n",
    "                \n",
    "            min_pvalue_rev = min(pvalue_rev.values())\n",
    "            if min_pvalue_rev < 0.05:\n",
    "                granger_causal_link = True\n",
    "                min_lag_rev = [k for k, v in pvalue_rev.items() if v == min_pvalue_rev][0]\n",
    "                print(f'• {yt_var} --> {pt_var} (pvalue={min_pvalue_rev:.3f}, lag={min_lag_rev})')\n",
    "\n",
    "                # add value to df\n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 1\n",
    "                \n",
    "                if (yt_var, pt_var) in granger_dict:\n",
    "                    granger_dict[(yt_var, pt_var)].append(patreon)\n",
    "                else:\n",
    "                    granger_dict[(yt_var, pt_var)] = [patreon]\n",
    "            else: \n",
    "                df_granger.loc[idx, yt_var+'->'+pt_var] = 0\n",
    "                \n",
    "\n",
    "    if (granger_causal_link == False):\n",
    "        print(\"• No Granger causality found for this account\")\n",
    "        not_granger.append(patreon)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    fig.tight_layout(w_pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\\n\\n---------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# print('\\n\\nGranger tests summary statistics:')\n",
    "    \n",
    "# print(f'• Number of patreon accounts analysed (patrons increase ratio > {incr_thresh_ratio}): {len(df_granger)}')\n",
    "# print(f'• Number of patreon with no Granger-causal link: {len(not_granger)} ({len(not_granger)/len(df_granger):.0%})')\n",
    "\n",
    "# print(f'• Number of patreon accounts per Granger-causal link:')\n",
    "# # Converting granger dict into list of tuples (in order to sort it), the 2nd value of the tuple being the count of accounts\n",
    "# granger_list = [(k, len(v)) for k, v in granger_dict.items()]\n",
    "# # sort by count desc\n",
    "# granger_list_desc = sorted(granger_list, key=lambda tup: -tup[1])\n",
    "# for (k,v) in granger_list_desc:\n",
    "#     print(f'    • {k[0]} \\t--> {k[1]}:\\t {v} ({v/len(df_granger):.0%})')\n",
    "\n",
    "\n",
    "# df_granger[columns] = df_granger[columns].astype('Int64')\n",
    "# df_granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Propensity Score Matching\n",
    "_(inspired by ADA exercise session by Tiziano Piccardi and Kristina Gligoric)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt_bkpnt = pd.read_csv(LOCAL_DATA_FOLDER+\"df_pt_bkpnt.tsv.gz\", sep=\"\\t\", compression='gzip')\n",
    "df_pt_bkpnt['bkpt_date'] = pd.to_datetime(df_pt_bkpnt['bkpt_date'])\n",
    "df_pt_bkpnt['bkpt_date_sub30'] = pd.to_datetime(df_pt_bkpnt['bkpt_date_sub30'])\n",
    "df_pt_bkpnt['bkpt_date_add30'] = pd.to_datetime(df_pt_bkpnt['bkpt_date_add30'])\n",
    "df_pt_bkpnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split \"treated\" VS \"non-treated\" PT accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the following caracteristics for the Treated vs Non-Treated accounts:\n",
    "\n",
    "    \n",
    "- **Treated** accounts (increasing more after the breakpoint) will have\n",
    "    - increase ratio > 2\n",
    "    - d1 > 0\n",
    "    - d2 > 0\n",
    "\n",
    "\n",
    "- **Non-Treated / Control** accounts(increasing linearly before and after breakpoint) will have\n",
    "    - increase ratio = 1\n",
    "    - d1 > 0\n",
    "    - d2 > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns to see d1 d2 and ratio at the beginning\n",
    "columns_new_order = ['patreon_id', 'yt_channel_id', 'treat', 'd1', 'd2', 'ratio', 'bkpt_date', 'bkpt_date_sub30',\n",
    "       'bkpt_date_add30', 'avg_patrons_bkpnt', 'avg_patrons_sub30',\n",
    "       'avg_patrons_add30', 'mean_delta_patrons_befor',\n",
    "       'mean_delta_patrons_after', 'mean_delta_earnings_befor',\n",
    "       'mean_delta_earnings_after', 'mean_delta_videos_befor',\n",
    "       'mean_delta_videos_after', 'mean_delta_views_befor',\n",
    "       'mean_delta_views_after', 'mean_delta_subs_befor',\n",
    "       'mean_delta_subs_after', 'mean_duration_befor', 'mean_likes_after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCR_MIN_RATIO = 1\n",
    "TREATED_MIN_RATIO = 2\n",
    "CONTROL_MAX_RATIO = 2\n",
    "\n",
    "predicate1 =        df_pt_bkpnt['d1'] > 0\n",
    "predicate2 =        df_pt_bkpnt['d2'] > 0\n",
    "predicate3 =        df_pt_bkpnt['ratio'] > INCR_MIN_RATIO\n",
    "\n",
    "df_treat = df_pt_bkpnt[predicate1 & predicate2 & predicate3]\n",
    "df_treat = df_treat.reset_index(drop=True)\n",
    "\n",
    "df_treat['treat'] = df_treat['ratio'].map(lambda x: 1 if x > TREATED_MIN_RATIO else 0)\n",
    "\n",
    "df_treat = df_treat[columns_new_order]\n",
    "\n",
    "\n",
    "treated = df_treat[df_treat['treat'] == 1]\n",
    "control = df_treat[df_treat['treat'] == 0]\n",
    "                   \n",
    "print(f'Total number of accounts   ({INCR_MIN_RATIO} < ratio):\\t\\t {len(df_treat)}')\n",
    "print(f'Number of control accounts ({INCR_MIN_RATIO} < ratio < {CONTROL_MAX_RATIO}):\\t {len(control)}')\n",
    "print(f'Number of treated accounts ({TREATED_MIN_RATIO} < ratio):\\t\\t {len(treated)}')\n",
    "df_treat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(treated['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The treated group has:\n",
    "\n",
    "- lower mean delta videos value\n",
    "- higher first (25%) percentile\n",
    "- Some outliers of really high delta videos - with maximum income\n",
    "\n",
    "The control group has:\n",
    "- higher mean earnings value\n",
    "- higher percentile (50%,75%)\n",
    "- higher number of accounts with avg delta videos in the interval 3 - 9\n",
    "\n",
    "We conclude that, in general, the control group outperforms the treated one in most of the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A closer look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_explore_relationships = ['treat', 'd1', 'd2', 'ratio',\n",
    "        'bkpt_date', 'avg_patrons_bkpnt', \n",
    "        'mean_delta_patrons_befor',\n",
    "        'mean_delta_earnings_befor',\n",
    "        'mean_delta_videos_befor',\n",
    "        'mean_delta_views_befor',\n",
    "        'mean_delta_views_after',\n",
    "        'mean_delta_subs_befor',\n",
    "        'mean_duration_befor',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairwise relationships\n",
    "sns.pairplot(df_treat[columns_to_explore_relationships])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of patrons at breakpoint\n",
    "df_treat.boxplot(by='treat', column='avg_patrons_bkpnt', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "df_treat.boxplot(by='treat', column='mean_delta_patrons_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "df_treat.boxplot(by='treat', column='mean_delta_views_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propensity score model\n",
    "- Use logistic regression to estimate propensity scores for all points in the dataset. \n",
    "-  Use statsmodels to fit the logistic regression model and apply it to each data point to obtain propensity scores.\n",
    "\n",
    "The propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Treatment:**\n",
    "    - Increase of Patrons at breakpoint - converted to a binary variable as follows:\n",
    "        - `treat` = 1 (treatment group), if number of patrons increase ratio at breakpoint > threshold\n",
    "        - `treat` = 0 (control group), if number of patrons increase linearly (increase ratio btw 1 and 2)\n",
    "        \n",
    "- **Outcome**\n",
    "    - `mean_delta_videos_after`: YouTube delta views (post-treatment)\n",
    "    \n",
    "- **Observed covariates:**\n",
    "    - `mean_delta_videos_befor`: YouTube delta videos (pre-treatment) \n",
    "    - `mean_delta_views_befor`:  YouTube delta views (pre-treatment) \n",
    "    - `mean_delta_subs_befor`:   YouTube delta subs (pre-treatment) \n",
    "    - `mean_duration_befor`:     YouTube video duration (pre-treatment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = ['avg_patrons_bkpnt',\n",
    " 'avg_patrons_sub30',\n",
    " 'avg_patrons_add30',\n",
    " 'mean_delta_patrons_befor',\n",
    " 'mean_delta_patrons_after',\n",
    " 'mean_delta_earnings_befor',\n",
    " 'mean_delta_earnings_after',\n",
    " 'mean_delta_videos_befor',\n",
    " 'mean_delta_videos_after',\n",
    " 'mean_delta_views_befor',\n",
    " 'mean_delta_views_after',\n",
    " 'mean_delta_subs_befor',\n",
    " 'mean_delta_subs_after',\n",
    " 'mean_duration_befor',\n",
    " 'mean_likes_after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the continuous features\n",
    "for feature in continuous_features:\n",
    "    df_treat[feature] = (df_treat[feature] - df_treat[feature].mean())/df_treat[feature].std()\n",
    "\n",
    "df_treat_notna = df_treat.dropna(subset=['mean_delta_videos_befor', 'mean_delta_views_befor', 'mean_delta_subs_befor', 'mean_duration_befor']).copy()\n",
    "df_treat_notna = df_treat_notna.reset_index(drop=True)\n",
    "df_treat_notna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.logit(formula='treat ~  avg_patrons_bkpnt + mean_delta_videos_befor + mean_delta_views_befor + mean_delta_subs_befor + mean_duration_befor', data=df_treat_notna)\n",
    "\n",
    "res = mod.fit()\n",
    "\n",
    "# Extract the estimated propensity scores\n",
    "df_treat_notna['Propensity_score'] = res.predict()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the treatment and control groups\n",
    "treatment_df = df_treat_notna[df_treat_notna['treat'] == 1]\n",
    "control_df   = df_treat_notna[df_treat_notna['treat'] == 0]\n",
    "\n",
    "# Create an empty undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Loop through all the pairs of instances\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        # Calculate the similarity \n",
    "        similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                    treatment_row['Propensity_score'])\n",
    "\n",
    "        # Add an edge between the two instances weighted by the similarity between them\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "# Generate and return the maximum weight matching on the generated graph\n",
    "matching = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = df_treat_notna.iloc[matched]\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_balanced = balanced_df.loc[balanced_df['treat'] == 1] #People that attained the program\n",
    "control_balanced = balanced_df.loc[balanced_df['treat'] == 0] #People that didn't attain the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of control accounts ({INCR_MIN_RATIO} < ratio < {CONTROL_MAX_RATIO}):\\t {len(control_balanced)}')\n",
    "print(f'Number of treated accounts ({TREATED_MIN_RATIO} < ratio):\\t\\t {len(treated_balanced)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_balanced.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_balanced.mean_delta_videos_after.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of patrons at breakpoint\n",
    "balanced_df.boxplot(by='treat', column='avg_patrons_bkpnt', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "balanced_df.boxplot(by='treat', column='mean_delta_patrons_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta patrons during month before breakpoint\n",
    "balanced_df.boxplot(by='treat', column='mean_delta_views_befor', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(balanced_df[columns_to_explore_relationships])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution BEFORE matching\n",
    "ax = sns.distplot(treated['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint, BEFORE matching', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution AFTER matching\n",
    "ax = sns.distplot(treated_balanced['mean_delta_videos_after'], hist=True, label='treated');\n",
    "ax = sns.distplot(control_balanced['mean_delta_videos_after'], hist=True, label='control')\n",
    "ax.set(title='Average delta videos distribution comparison during month after breakpoint, AFTER matching', xlabel='Delta videos after breakpoint', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
