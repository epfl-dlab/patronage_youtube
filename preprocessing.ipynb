{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Characterizing Patronage on YouTube - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libaries imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import timeit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths to data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source files used in this notebook:\n",
    "\n",
    "**YouNiverse dataset:**\n",
    "\n",
    "- `df_channels_en.tsv.gz`: channel metadata.\n",
    "- `df_timeseries_en.tsv.gz`: channel-level time-series.\n",
    "- `yt_metadata_en.jsonl.gz`: raw video metadata.\n",
    "\n",
    "**Graphteon dataset:**\n",
    "- `final_processed_file.jsonl.gz` all graphtreon time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder paths\n",
    "DATA_FOLDER = \"/dlabdata1/youtube_large/\"\n",
    "LOCAL_DATA_FOLDER = \"local_data/\"\n",
    "\n",
    "# YouTube Metadata\n",
    "PATH_YT_METADATA_SRC = DATA_FOLDER+\"yt_metadata_en.jsonl.gz\"\n",
    "PATH_YT_METADATA_DST = LOCAL_DATA_FOLDER+\"yt_metadata_en_pt.tsv.gz\"\n",
    "PATH_YT_METADATA_UNIQUE_PT_DST = LOCAL_DATA_FOLDER+\"yt_metadata_en_unique_pt_per_chan.tsv.gz\"\n",
    "PATH_YT_METADATA_UNIQUE_YT_PT_DST = LOCAL_DATA_FOLDER+\"yt_metadata_en_unique_pt_yt.tsv.gz\"\n",
    "PATH_YT_METADATA_RESTRICTED = LOCAL_DATA_FOLDER+\"yt_metadata_en_restricted.tsv.gz\"\n",
    "\n",
    "# Linked channels and patrons\n",
    "PATH_LINKED_CHANNELS_PATRONS_DST = LOCAL_DATA_FOLDER+\"df_linked_channels_patreons.tsv.gz\"\n",
    "\n",
    "# YouTube Timeseries\n",
    "PATH_YT_TIMESERIES_SRC = DATA_FOLDER+\"df_timeseries_en.tsv.gz\"\n",
    "PATH_YT_TIMESERIES_RESTRICTED_DST = LOCAL_DATA_FOLDER+\"df_yt_timeseries_restricted.tsv.gz\"\n",
    "\n",
    "# Patreon timeseries\n",
    "PATH_GT_TIMESERIES_SRC = DATA_FOLDER+\"final_processed_file.jsonl.gz\"\n",
    "PATH_GT_TIMESERIES_DST = LOCAL_DATA_FOLDER+\"df_gt_timeseries_filtered.tsv.gz\"\n",
    "PATH_GT_TIMESERIES_RESTRICTED_DST = LOCAL_DATA_FOLDER+\"df_gt_timeseries_restricted.tsv.gz\"\n",
    "PATH_GT_TIMESERIES_EARNING_DST = LOCAL_DATA_FOLDER+\"dailyGraph_earningsSeries.tsv.gz\"\n",
    "PATH_GT_TIMESERIES_PATRONS_DST = LOCAL_DATA_FOLDER+\"dailyGraph_patronsSeries.tsv.gz\"\n",
    "PATH_GT_TIMESERIES_PATRONS_AND_EARNINGS_DST = LOCAL_DATA_FOLDER+\"dailyGraph_patrons_and_earnings_Series.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convention for variable names\n",
    "# use 'df_' prefix when dataframe in the original form (not group by etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list all files in DATA_FOLDER\n",
    "# !ls -lh {DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in LOCAL_DATA_FOLDER\n",
    "# !ls -lh {LOCAL_DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Preprocess YouTube metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Filter YouTube metadata containing patreon id\n",
    "_Extract Patreon urls from YouTube metadata description (if they exist) and keep only those rows_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_escape(str):\n",
    "    \"\"\"\n",
    "    replace new line special character by a space\n",
    "    \"\"\"\n",
    "    return str.replace(\"\\\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract patreon accounts from youtube channel descriptions and\n",
    "# # filter the metadata to retain only the rows which description contains a patreon url\n",
    "\n",
    "# TEST_RUN = False\n",
    "# # MAX_ITER = 10_000\n",
    "\n",
    "# nb_rows_read = 0\n",
    "# JSONDecodeErrors_cnt = 0 \n",
    "# lines_json = []    \n",
    "\n",
    "# # match patterns starting with patreon.com/ and matching at least 1 character after\n",
    "# # until it reaches anything thats not a word character\n",
    "# pattern = re.compile(r'patreon.com/[^\\W]+')\n",
    "\n",
    "# compressed_file_size = os.stat(PATH_YT_METADATA_SRC).st_size\n",
    "# print(\"Compressed file size is :                 {:>3,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "# uncompressed_file_size = 97_600_000_000\n",
    "# print(\"Estimated Uncompressed file size is :     {:>3,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "# start = timeit.default_timer()\n",
    "\n",
    "# # Load tqdm with size counter instead of file counter\n",
    "# with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "#     with gzip.open(PATH_YT_METADATA_SRC, \"r\") as f:\n",
    "#         for i, line_byte in enumerate(f): \n",
    "\n",
    "#             read_bytes = len(line_byte)\n",
    "#             if read_bytes:\n",
    "#                 pbar.set_postfix(file=PATH_YT_METADATA_SRC[len(DATA_FOLDER)+1:], refresh=False)\n",
    "#                 pbar.update(read_bytes)\n",
    "\n",
    "#             nb_rows_read += 1\n",
    "            \n",
    "#             # set a maximum iteration for tests\n",
    "#             if TEST_RUN == True:\n",
    "#                 if nb_rows_read >= MAX_ITER:\n",
    "#                     break\n",
    "\n",
    "#             # convert bytes into string\n",
    "#             line_str = line_byte.decode(\"utf-8\")\n",
    "\n",
    "#             # convert string into json after escaping new line characters\n",
    "#             line_str_esc = json_escape(line_str)\n",
    "#             try:\n",
    "#                 line_json = json.loads(line_str_esc)\n",
    "#             except Exception as e:\n",
    "#                 JSONDecodeErrors_cnt += 1\n",
    "#                 pass\n",
    "\n",
    "#             # print(line_json)\n",
    "#             # print(line_json['categories'])\n",
    "            \n",
    "#             # add line if description contains a patreon.com id\n",
    "#             if re.search(pattern, line_json['description']):\n",
    "#                 patreon_id = re.findall(pattern, line_json['description'])[0]\n",
    "#                 line_json['patreon_id'] = patreon_id\n",
    "#                 lines_json.append(line_json)\n",
    "\n",
    "# stop = timeit.default_timer()\n",
    "# time_diff = stop - start\n",
    "\n",
    "# print()\n",
    "# print(\"==> total time to read and filter youtube metadata:                {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "# print(\"==> number of rows (= videos) read:                                {:>10,}\".format(nb_rows_read))\n",
    "# print(\"==> number of videos containing a patreon link in the description: {:>10,} ({:.3%})\".format(len(lines_json), len(lines_json)/nb_rows_read ))\n",
    "# print(\"==> number of skipped rows (JSONDecodeErrors):                     {:>10,} ({:.3%})\".format(JSONDecodeErrors_cnt, JSONDecodeErrors_cnt/nb_rows_read))\n",
    "\n",
    "# # create new dataframe with the filtered lines\n",
    "# df_yt_metadata_pt = pd.DataFrame(data=lines_json, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove rows where patreon_ids = patreon.com/posts or patreon.com/user (in the future fix in regex)\n",
    "# df_yt_metadata_pt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'] != 'patreon.com/posts']\n",
    "# df_yt_metadata_pt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'] != 'patreon.com/user']\n",
    "# df_yt_metadata_pt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'] != 'patreon.com/join']\n",
    "\n",
    "# # lowercase all patreon ids to avoid duplicates\n",
    "# df_yt_metadata_pt['patreon_id'] = df_yt_metadata_pt['patreon_id'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save youtube metadata df containing patreon accounts\n",
    "# df_yt_metadata_pt.to_csv(PATH_YT_METADATA_DST, index=False, sep='\\t', compression='gzip')\n",
    "!ls -lh {PATH_YT_METADATA_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restrict to 1 patreon id per youtube channel\n",
    "Some YouTube channels use multiple patreon accounts. We'll only keep the most used patreon account for each YouTube channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read youtube metadata file containing patreon accounts (takes about 2 mins)\n",
    "df_yt_metadata_pt = pd.read_csv(PATH_YT_METADATA_DST, sep=\"\\t\", lineterminator='\\n', compression='gzip')\n",
    "df_yt_metadata_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original YT dataset\n",
    "\n",
    "# use if running script above\n",
    "# DF_YT_METADATA_ROWS = nb_rows_read \n",
    "\n",
    "# use if load df_yt_metadata_pt\n",
    "DF_YT_METADATA_ROWS = 72_924_794 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats \n",
    "print(\"[YouTube metadata] Total number of videos:                                                {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "print(\"[Filtered YouTube metadata] number of videos that contain a patreon link in description:  {:>10,} ({:.1%} of total dataset)\".format(len(df_yt_metadata_pt), len(df_yt_metadata_pt)/DF_YT_METADATA_ROWS))\n",
    "\n",
    "# get list of all unique patreon ids in df_yt_metadata_pt\n",
    "yt_patreon_list = df_yt_metadata_pt['patreon_id'].unique()\n",
    "yt_pt_channel_list = df_yt_metadata_pt['channel_id'].unique()\n",
    "\n",
    "print(\"[Filtered YouTube metadata] number of unique channels that contain a patreon account:     {:>9,}\".format(len(yt_pt_channel_list)))\n",
    "print(\"[Filtered YouTube metadata] number of unique patreon ids:                                 {:>9,}\".format(len(yt_patreon_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** \\\n",
    "We can see that we have _**more patreon ids than channels**_ . Let's investigate further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by channel_id AND patreon_id and count the number of unique videos (display_ids)\n",
    "yt_metadata_pt_grp_chan = df_yt_metadata_pt.groupby(['channel_id','patreon_id']).agg(display_id_cnt=(\"display_id\", pd.Series.nunique))\n",
    "yt_metadata_pt_grp_chan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "yt_metadata_pt_grp_chan = yt_metadata_pt_grp_chan.reset_index()\n",
    "yt_metadata_pt_grp_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of patreon_ids per channel\n",
    "pt_id_cnt_pr_chan = yt_metadata_pt_grp_chan.groupby('channel_id').count()['patreon_id'].sort_values(ascending=False)\n",
    "pt_id_cnt_pr_chan = pt_id_cnt_pr_chan.to_frame(name='patreon_id_cnt')\n",
    "pt_id_cnt_pr_chan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_gt_1_pt = pt_id_cnt_pr_chan[pt_id_cnt_pr_chan['patreon_id_cnt']>1]\n",
    "print(f\"Total number of channels:                            {len(pt_id_cnt_pr_chan):>6,}\")\n",
    "print(f\"Number of channels with more than 1 patreon account: {len(chan_gt_1_pt):>6,} ({len(chan_gt_1_pt) / len(pt_id_cnt_pr_chan):.1%})\")\n",
    "\n",
    "# plot Distribution of patreon ids per channel\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(6,4))\n",
    "\n",
    "# plot with log scale for x axis and log scale for y axis\n",
    "sns.histplot(data=pt_id_cnt_pr_chan, ax=axs, bins=50, kde=False, legend=False, color=f'C{0}')\n",
    "axs.set(title=f'Distribution of patreon ids per channel (log scale)')\n",
    "axs.set_xlabel(\"Number of patreon ids\")\n",
    "axs.set_ylabel(\"Count of channels (log scale)\")\n",
    "axs.set(yscale=\"log\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# descriptive statistics table\n",
    "pt_id_cnt_pr_chan.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "As we observed earlier, some channels use more than 1 patreon id, and use different patreon urls for different videos. For example:\n",
    "- [Patreon_Gaming](https://www.youtube.com/channel/UCAsLyFlWkbdhvri02tO6veA) uses 73 different patreon ids.\n",
    "- [Artistic Maniacs](https://www.youtube.com/channel/UC3pcSD6_RRisNLaHGznemJA) uses 69 different patreon ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for Artistic Maniacs\n",
    "yt_metadata_pt_grp_chan[yt_metadata_pt_grp_chan['channel_id'] == 'UC3pcSD6_RRisNLaHGznemJA'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keep only most used patreon_id per channel (patreon_id with most videos for each channel)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort metadata df by diplay_id_cnt within each channel_id group\n",
    "yt_metadata_pt_grp_chan = yt_metadata_pt_grp_chan.sort_values(['channel_id','display_id_cnt'], ascending=[True, False])\n",
    "yt_metadata_pt_grp_chan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of duplicate of rows with same channel id but different patreon ids\n",
    "dup_chan_id = yt_metadata_pt_grp_chan[yt_metadata_pt_grp_chan.duplicated(subset=['channel_id'], keep='first')]\n",
    "print(\"Number of duplicate rows (same channel id with multiple patreon_ids): {:,}\".format(len(dup_chan_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at duplicate rows\n",
    "# yt_metadata_pt_grp_chan[yt_metadata_pt_grp_chan.duplicated('channel_id')].sort_values('channel_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_metadata_pt_grp_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows, keep the patreon ids with the most videos (keep first)\n",
    "yt_metadata_unique_pt = yt_metadata_pt_grp_chan.drop_duplicates(subset='channel_id', keep='first')\n",
    "print('Removed {:,} rows'.format(len(yt_metadata_pt_grp_chan) - len(yt_metadata_unique_pt)))\n",
    "yt_metadata_unique_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_unique_pt_per_chan = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(yt_metadata_unique_pt['patreon_id'])]\n",
    "print(f\"removed {len(df_yt_metadata_pt) - len(df_yt_metadata_unique_pt_per_chan):,} videos from dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After keeping only patreon appearing on the most videos for each channel:\")\n",
    "print(f\"Number of videos:    {len(df_yt_metadata_unique_pt_per_chan):>10,}\")\n",
    "print(f\"Unique channel ids:  {df_yt_metadata_unique_pt_per_chan.channel_id.nunique():>10,}\")\n",
    "print(f\"Unique patreon ids:  {df_yt_metadata_unique_pt_per_chan.patreon_id.nunique():>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"YouTube Metadata unique patreon account per channel\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# df_yt_metadata_unique_pt_per_chan.to_csv(PATH_YT_METADATA_UNIQUE_PT_DST, index=False, sep='\\t', compression='gzip')\n",
    "!ls -lh {PATH_YT_METADATA_UNIQUE_PT_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restrict to 1 youtube channel per patreon id \n",
    "We grouping YouTube metadata by `channel_id` and `patreon_id`, we also notice that we have more rows than the total number of unique patreon ids. \\\n",
    "This is because some `patreon_id` are used on multiple YT channels. We'll remove those accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total rows (grouped by channel_id and patreon_id):   {len(yt_metadata_unique_pt):,}\")\n",
    "print(f\"total number of unique patreon ids:                  {yt_metadata_unique_pt.patreon_id.nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show patreon_id that are used on multiple channels.\n",
    "yt_metadata_unique_pt[yt_metadata_unique_pt.duplicated(subset=['patreon_id'], keep=False)].sort_values(by='patreon_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[Filtered YouTube metadata] number of channels per patreon id:\")\n",
    "\n",
    "# chan_cnt_per_patreon_id = df_yt_metadata_pt.groupby('patreon_id')\\\n",
    "#                                             .agg(channel_id_count=('channel_id', 'count'))\\\n",
    "#                                             .sort_values(by=['channel_id_count'], ascending=False)\n",
    "# chan_cnt_per_patreon_id\n",
    "# # chan_cnt_per_patreon_id.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_metadata_unique_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_unique_pt_per_chan.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove patreon accounts that have more than 1 youtube channel\n",
    "\n",
    "# Count YT channel_ids per patreon_id\n",
    "yt_metadata_pt_chan_id_cnt = df_yt_metadata_unique_pt_per_chan.groupby(['patreon_id','channel_id']).agg(channel_id_cnt=(\"channel_id\", pd.Series.nunique)).groupby('patreon_id').count().sort_values('channel_id_cnt', ascending=False)\n",
    "yt_metadata_pt_chan_id_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep patreon ids that have exactly 1 channel id only\n",
    "yt_metadata_pt_chan_id_cnt_unique_chan = yt_metadata_pt_chan_id_cnt[yt_metadata_pt_chan_id_cnt['channel_id_cnt']==1]\n",
    "\n",
    "print(f\"removed {len(yt_metadata_pt_chan_id_cnt) - len(yt_metadata_pt_chan_id_cnt_unique_chan)} accounts\")\n",
    "\n",
    "patreons_with_unique_chan = yt_metadata_pt_chan_id_cnt_unique_chan.index\n",
    "\n",
    "print(f\"Number of patreon accounts with only 1 YT channel: {patreons_with_unique_chan.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_unique_pt_yt = df_yt_metadata_pt[df_yt_metadata_pt['patreon_id'].isin(patreons_with_unique_chan)]\n",
    "print(f\"removed {len(df_yt_metadata_pt) - len(df_yt_metadata_unique_pt_yt)} videos from dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df_yt_metadata_unique_pt_yt):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of videos:    {len(df_yt_metadata_unique_pt_yt):>10,}\")\n",
    "print(f\"Unique channel ids:  {df_yt_metadata_unique_pt_yt.channel_id.nunique():>10,}\")\n",
    "print(f\"Unique patreon ids:  {df_yt_metadata_unique_pt_yt.patreon_id.nunique():>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"YouTube Metadata unique patreon account per channel\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# df_yt_metadata_unique_pt_yt.to_csv(PATH_YT_METADATA_UNIQUE_YT_PT_DST, index=False, sep='\\t', compression='gzip')\n",
    "!ls -lh {PATH_YT_METADATA_UNIQUE_YT_PT_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Restrict YouTube metadata further according to YouTube Timeseries restricted dataset_ (see 1.3)\n",
    "_This is done at the end of section 1.3 below, after the YouTube Timeseries 4 filters have been applied_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [_Ignore for now_] Number of videos per patreon id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by patreon_id and count the number of unique display_ids\n",
    "vids_cnt_per_patreon_id = df_yt_metadata_pt.groupby('patreon_id').agg({\"display_id\": pd.Series.nunique}).sort_values(by='display_id', ascending=False)\n",
    "vids_cnt_per_patreon_id.rename(columns={'display_id':'display_id_cnt'}, inplace=True)\n",
    "\n",
    "print(\"[Filtered YouTube metadata] number of videos per patreon id:\")\n",
    "vids_cnt_per_patreon_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with linear scale for both axes\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(6,4))\n",
    "\n",
    "\n",
    "# plot with log scale for x axis and log scale for y axis\n",
    "sns.histplot(data=vids_cnt_per_patreon_id, ax=axs, bins=50, kde=False, color=f'C{0}')\n",
    "axs.set(title=f'Distribution of videos per patreon id (log scale)')\n",
    "axs.set_xlabel(\"Number of videos\")\n",
    "axs.set_ylabel(\"# patreon ids (log scale)\")\n",
    "axs.set(yscale=\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# descriptive statistics table\n",
    "vids_cnt_per_patreon_id.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \\\n",
    "From the above graph and table, we can see that the _videos_ distributions among patreon ids follows a **power law**, meaning that most patreon accounts have only a few videos, but a few of them have a lot of videos.\n",
    "\n",
    "More specifically:\n",
    "- 25% of the Patreon accounts have 1 video\n",
    "- 50% of the Patreon accounts have less than 4 videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 Link YT channels and Patrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### \"Link\" dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider them linked only if \n",
    "- Link YouTube channel to Patreon id which appears in most of its videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_metadata_unique_all = yt_metadata_unique_pt[yt_metadata_unique_pt['patreon_id'].isin(patreons_with_unique_chan)]\n",
    "print(f\"Removed {len(yt_metadata_unique_pt) - len(yt_metadata_unique_all)} accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store into new \"matched\" dataframe\n",
    "df_linked_channels_patreons = yt_metadata_unique_all[['channel_id', 'patreon_id']]\n",
    "df_linked_channels_patreons = df_linked_channels_patreons.reset_index(drop=True)\n",
    "df_linked_channels_patreons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"linked\" dataframe to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# df_linked_channels_patreons.to_csv(PATH_LINKED_CHANNELS_PATRONS_DST, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_LINKED_CHANNELS_PATRONS_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.3 Filter YouTube timeseries - Restrict YouTube channels (4 filters)\n",
    "Restrict YouTube channels according to the following criteria (filters are applied sequentially):\n",
    "- Filter 1: Keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account \n",
    "- Filter 2: At least 2 year between first and last video\n",
    "- Filter 3: At least 20 videos with patreon ids\n",
    "- Filter 4: At least 250k subscribers at data crawling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_YT_TIMESERIES_SRC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load channel-level time-series. (takes about 50 secs)\n",
    "df_yt_timeseries = pd.read_csv(PATH_YT_TIMESERIES_SRC, sep=\"\\t\", compression='gzip', parse_dates=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global values for filters\n",
    "MIN_DAYS_DELTA = \"730 day\"    # filter 2\n",
    "NB_PATREON_VIDS = 20          # filter 3\n",
    "NB_SUBS = 250_000             # filter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of channels of original YT timeseries dataset (need to first load df_yt_timeseries in 1.1.2)\n",
    "yt_ts_uniq_chan_cnt = df_yt_timeseries['channel'].nunique()\n",
    "print(\"[YouTube Timeseries] Nb of videos of original dataset:                  {:>10,}\".format(len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of channels of original dataset:                {:>10,}\".format(yt_ts_uniq_chan_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Filter 1:** Keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account (using \"Link\" df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter 1: retain only the YT channels that exist in the filtered YT metadata dataset (need to first load df_yt_metadata_pt and yt_pt_channel_list in 2.2.1)\n",
    "df_yt_timeseries_filt1 = df_yt_timeseries[df_yt_timeseries['channel'].isin(df_linked_channels_patreons['channel_id'])]\n",
    "chan_list_filt1 = df_yt_timeseries_filt1['channel'].unique()\n",
    "chan_list_filt1_cnt = len(chan_list_filt1)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1:           {:>10,} ({:5.1%} of original dataset)\".format(len(df_yt_timeseries_filt1), len(df_yt_timeseries_filt1)/len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1:          {:>10,} ({:5.1%} of original dataset)\".format(chan_list_filt1_cnt, chan_list_filt1_cnt/yt_ts_uniq_chan_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Filter 2:** At least 2 years between first and last video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# among filter1 channels, calculate time difference between the first and the last video for each channel\n",
    "datetime_data = df_yt_timeseries_filt1.groupby('channel').agg(datetime_min=('datetime', 'min'),\n",
    "                                                              datetime_max=('datetime', 'max'))\n",
    "datetime_data['delta_datetime'] = datetime_data['datetime_max'] - datetime_data['datetime_min']\n",
    "\n",
    "# filter channels that we have data for at least MIN_TIME_DELTA days\n",
    "datetime_data_filt2 = datetime_data[datetime_data['delta_datetime'] > pd.Timedelta(MIN_DAYS_DELTA)]\n",
    "\n",
    "# Apply filter on YT Timeseries dataset: retain only those channels that have data for at least MIN_TIME_DELTA days\n",
    "df_yt_timeseries_filt2 = df_yt_timeseries_filt1[df_yt_timeseries_filt1['channel'].isin(datetime_data_filt2.index)]\n",
    "\n",
    "chan_list_filt2 = df_yt_timeseries_filt2['channel'].unique()\n",
    "chan_list_filt2_cnt = len(chan_list_filt2)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2:         {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 dataset)\".format(len(df_yt_timeseries_filt2), len(df_yt_timeseries_filt2)/len(df_yt_timeseries), len(df_yt_timeseries_filt2)/len(df_yt_timeseries_filt1)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2:        {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 channels)\".format(chan_list_filt2_cnt, chan_list_filt2_cnt/yt_ts_uniq_chan_cnt, chan_list_filt2_cnt/chan_list_filt1_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Filter 3:** At least 20 videos with patreon ids per channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by channel_id AND patreon_id and count the number of unique videos (=display_ids). (need to load yt_metadata_pt_grp_chan from point 2.2.1)\n",
    "# Then filter rows that have at least 20 videos (display_ids) \n",
    "yt_metadata_pt_grp_chan_filt3 = yt_metadata_pt_grp_chan[yt_metadata_pt_grp_chan['display_id_cnt'] > NB_PATREON_VIDS]\n",
    "yt_metadata_pt_grp_chan_filt3\n",
    "\n",
    "# get list of unique channels satisfying filter 3\n",
    "chan_list_filt_3 = yt_metadata_pt_grp_chan_filt3['channel_id'].unique()\n",
    "\n",
    "# Apply filter on YT Timeseries dataset: retain only those channels from filt 2 that are in the chan_list_filt_3\n",
    "df_yt_timeseries_filt3 = df_yt_timeseries_filt2[df_yt_timeseries_filt2['channel'].isin(chan_list_filt_3)]\n",
    "\n",
    "chan_list_filt3 = df_yt_timeseries_filt3['channel'].unique()\n",
    "chan_list_filt3_cnt = len(chan_list_filt3)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3:       {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 dataset)\".format(len(df_yt_timeseries_filt3), len(df_yt_timeseries_filt3)/len(df_yt_timeseries), len(df_yt_timeseries_filt3)/len(df_yt_timeseries_filt2)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3:      {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 channels)\".format(chan_list_filt3_cnt, chan_list_filt3_cnt/yt_ts_uniq_chan_cnt, chan_list_filt3_cnt/chan_list_filt2_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Filter 4:** At least 250k subscribers at data crawling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregates per channel\n",
    "subs_aggr_per_channel = df_yt_timeseries_filt3.groupby('channel')\\\n",
    "                                               .agg(min_subs=('subs', 'min'),\n",
    "                                                    max_subs=('subs', 'max'))\\\n",
    "                                                .sort_values(by=['max_subs'], ascending=False)\\\n",
    "                                                .reset_index()\n",
    "# subs_aggr_per_channel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to first load data_per_channel (aggregates per channel in 1.1.2 'Datetime points accross channels' section)\n",
    "subs_per_channel_filt4 = subs_aggr_per_channel[subs_aggr_per_channel['max_subs'] > NB_SUBS]\n",
    "\n",
    "# get list of unique channels satisfying filter 4\n",
    "chan_list_filt_4 = subs_per_channel_filt4['channel'].unique()\n",
    "\n",
    "# # Apply filter on YT Timeseries dataset: retain only those channels from filt_3 that are in the chan_list_filt_4\n",
    "df_yt_timeseries_filt4 = df_yt_timeseries_filt3[df_yt_timeseries_filt3['channel'].isin(chan_list_filt_4)]\n",
    "\n",
    "chan_list_filt4 = df_yt_timeseries_filt4['channel'].unique()\n",
    "chan_list_filt4_cnt = len(chan_list_filt4)\n",
    "\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3+4:     {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 dataset)\".format(len(df_yt_timeseries_filt4), len(df_yt_timeseries_filt4)/len(df_yt_timeseries), len(df_yt_timeseries_filt4)/len(df_yt_timeseries_filt3)))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3+4:    {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 channels)\".format(chan_list_filt4_cnt, chan_list_filt4_cnt/yt_ts_uniq_chan_cnt, chan_list_filt4_cnt/chan_list_filt3_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "**• Filters summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[YouTube Timeseries] Stats before and after filters:\")\n",
    "print()\n",
    "\n",
    "print(\"Filter 1 = \\\"keep only YouTube channels that are in YouTube Timeseries dataset AND linked to a patreon account\\\"\")\n",
    "print(\"Filter 2 = \\\"at least {:.1f} years ({} days) between first and last video\\\"\".format(pd.Timedelta(MIN_DAYS_DELTA).days/365, pd.Timedelta(MIN_DAYS_DELTA).days))\n",
    "print(\"Filter 3 = \\\"at least {:,} videos with patreon ids per channel\\\"\".format(NB_PATREON_VIDS))\n",
    "print(\"Filter 4 = \\\"at least {:,} subscribers at data crawling time\\\"\".format(NB_SUBS))\n",
    "print()\n",
    "print(\"[YouTube Timeseries] Nb of rows of original dataset:                  {:>10,}\".format(len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1:           {:>10,} ({:5.1%} of original dataset)\".format(len(df_yt_timeseries_filt1), len(df_yt_timeseries_filt1)/len(df_yt_timeseries)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2:         {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 dataset)\".format(len(df_yt_timeseries_filt2), len(df_yt_timeseries_filt2)/len(df_yt_timeseries), len(df_yt_timeseries_filt2)/len(df_yt_timeseries_filt1)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3:       {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 dataset)\".format(len(df_yt_timeseries_filt3), len(df_yt_timeseries_filt3)/len(df_yt_timeseries), len(df_yt_timeseries_filt3)/len(df_yt_timeseries_filt2)))\n",
    "print(\"[YouTube Timeseries] Nb of rows of after applying filter 1+2+3+4:     {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 dataset)\".format(len(df_yt_timeseries_filt4), len(df_yt_timeseries_filt4)/len(df_yt_timeseries), len(df_yt_timeseries_filt4)/len(df_yt_timeseries_filt3)))\n",
    "print()\n",
    "print(\"[YouTube Timeseries] Nb of channels of original dataset:              {:>10,}\".format(yt_ts_uniq_chan_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1:          {:>10,} ({:5.1%} of original dataset)\".format(chan_list_filt1_cnt, chan_list_filt1_cnt/yt_ts_uniq_chan_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2:        {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 1 channels)\".format(chan_list_filt2_cnt, chan_list_filt2_cnt/yt_ts_uniq_chan_cnt, chan_list_filt2_cnt/chan_list_filt1_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3:      {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 2 channels)\".format(chan_list_filt3_cnt, chan_list_filt3_cnt/yt_ts_uniq_chan_cnt, chan_list_filt3_cnt/chan_list_filt2_cnt))\n",
    "print(\"[YouTube Timeseries] Nb of channels after applying filter 1+2+3+4:    {:>10,} ({:5.1%} of original dataset, {:5.1%} of filter 3 channels)\".format(chan_list_filt4_cnt, chan_list_filt4_cnt/yt_ts_uniq_chan_cnt, chan_list_filt4_cnt/chan_list_filt3_cnt))\n",
    "print()\n",
    "print('[YouTube Timeseries] Time range of original dataset                   {} and {}'.format(df_yt_timeseries['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                              df_yt_timeseries['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "print('[YouTube Timeseries] Time range after applying filter 1+2+3+4        {} and {}'.format(df_yt_timeseries_filt4['datetime'].min().strftime('%B %d, %Y'),\n",
    "                                                              df_yt_timeseries_filt4['datetime'].max().strftime('%B %d, %Y')))\n",
    "\n",
    "display(df_yt_timeseries_filt4.head())\n",
    "print(\"Restricted list of channels after 4 filters (count = {:,}):\".format(chan_list_filt4_cnt))\n",
    "print(chan_list_filt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_timeseries_restricted = df_yt_timeseries_filt4.copy()\n",
    "\n",
    "# save youtube restricted timeseries df\n",
    "# df_yt_timeseries_restricted.to_csv(PATH_YT_TIMESERIES_RESTRICTED_DST, index=False, sep='\\t', compression='gzip')\n",
    "# !ls -lh {PATH_YT_TIMESERIES_RESTRICTED_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Restrict YouTube Metadata accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter YT metadata dataset by list of filtered channels from YT timeseries above\n",
    "df_yt_metadata_pt_restr = df_yt_metadata_unique_pt_yt[df_yt_metadata_unique_pt_yt['channel_id'].isin(chan_list_filt4)]\n",
    "\n",
    "# get unique channels for youtube metadata (original and restricted)\n",
    "yt_metadata_uniq_chan = df_yt_metadata_pt['channel_id'].unique()\n",
    "yt_metadata_uniq_chan_restr = df_yt_metadata_pt_restr['channel_id'].unique()\n",
    "\n",
    "# get unique patreon ids for youtube metadata (original and restricted)\n",
    "yt_metadata_uniq_pat = df_yt_metadata_pt['patreon_id'].unique()\n",
    "yt_metadata_uniq_pat_restr = df_yt_metadata_pt_restr['patreon_id'].unique()\n",
    "\n",
    "print(\"[YouTube Metadata]:\")\n",
    "print()\n",
    "print(\"Restriction = \\\"keep only YouTube channels that are in YouTube Timeseries filtered (filters 1-4) dataset\\\"\")\n",
    "print()\n",
    "# print(\"[YouTube Metadata] Nb of videos in original dataset:                                   {:>10,}\".format(DF_YT_METADATA_ROWS))\n",
    "# print(\"[YouTube Metadata] Nb of videos in pre-filtered (containing patreon id) dataset:       {:>10,}\".format(len(df_yt_metadata_pt)))\n",
    "# print(\"[YouTube Metadata] Nb of videos after filtering by restricted channels:                {:>10,} ({:5.1%} of pre-filtered dataset dataset)\".format(len(df_yt_metadata_pt_restr), len(df_yt_metadata_pt_restr)/len(df_yt_metadata_pt)))\n",
    "# print()\n",
    "print(\"[Filtered YouTube Metadata]   Nb of channels in pre-filtered (containing patreon id) dataset:     {:>10,}\".format(len(yt_metadata_uniq_chan)))\n",
    "print(\"[Restricted YouTube Metadata] Nb of channels after filtering by restricted channels:              {:>10,} ({:5.1%} of pre-filtered dataset dataset)\".format(len(yt_metadata_uniq_chan_restr), len(yt_metadata_uniq_chan_restr)/len(yt_metadata_uniq_chan)))\n",
    "print()\n",
    "print(\"[Filtered YouTube Metadata]   Nb of patreon ids in pre-filtered (containing patreon id) dataset:  {:>10,}\".format(len(yt_metadata_uniq_pat)))\n",
    "print(\"[Restricted YouTube Metadata] Nb of patreon ids after filtering by restricted channels:           {:>10,} ({:5.1%} of pre-filtered dataset dataset)\".format(len(yt_metadata_uniq_pat_restr), len(yt_metadata_uniq_pat_restr)/len(yt_metadata_uniq_pat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save youtube Metadata restricted according to YouTube Timeseries 4 filters\n",
    "# df_yt_metadata_pt_restr.to_csv(PATH_YT_METADATA_RESTRICTED, index=False, sep='\\t', compression='gzip')\n",
    "!ls -lh {PATH_YT_METADATA_RESTRICTED}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preprocess Graphtreon timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Filter Graphtreon to keep only records which patreon id exists the YouTube metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read restricted youtube metadata file from disk... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_YT_METADATA_DST}\n",
    "!ls -lh {PATH_YT_METADATA_RESTRICTED}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filtered youtube metadata file (takes about 2 mins - already loaded in 1.1)\n",
    "# df_yt_metadata_pt = pd.read_csv(PATH_YT_METADATA_DST, sep=\"\\t\", lineterminator='\\n', compression='gzip') \n",
    "# df_yt_metadata_pt.head(3)\n",
    "\n",
    "# read restricted youtube metadata file (takes about 10 seconds)\n",
    "yt_metadata_en_restricted = pd.read_csv(PATH_YT_METADATA_RESTRICTED, sep=\"\\t\", lineterminator='\\n', compression='gzip') \n",
    "yt_metadata_en_restricted.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Restricted YouTube metadata] number of videos in restricted YouTube metadata:                 {:>10,}\".format(len(yt_metadata_en_restricted)))\n",
    "\n",
    "\n",
    "# get list of all unique patreon ids in df_yt_metadata_pt\n",
    "yt_patreon_list = df_yt_metadata_pt.patreon_id.unique()\n",
    "yt_patreon_list_restricted = yt_metadata_en_restricted.patreon_id.unique()\n",
    "\n",
    "print(\"[Restricted YouTube metadata] total number of unique patreon ids:                              {:>10,}\".format(len(yt_patreon_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_GT_TIMESERIES_SRC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_escape(str):\n",
    "    \"\"\"\n",
    "    replace new line special character by a space\n",
    "    \"\"\"\n",
    "    return str.replace(\"\\\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Graphtreon dataset to keep only records which patreon id exists the YouTube metadata (yt_patreon_list) (takes about 2 mins)\n",
    "input_file_path = PATH_GT_TIMESERIES_SRC\n",
    "\n",
    "TEST_RUN = False\n",
    "MAX_ITER = 1_000\n",
    "\n",
    "nb_rows_read = 0\n",
    "JSONDecodeErrors_cnt = 0 \n",
    "lines_json = []    \n",
    "\n",
    "# pattern = re.compile(r'patreon.com/\\w*')\n",
    "\n",
    "compressed_file_size = os.stat(input_file_path).st_size\n",
    "print(\"Compressed file size is :                 {:>4,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "uncompressed_file_size = 13_310_000_000 # (=12.4 GB)\n",
    "print(\"Estimated Uncompressed file size is :     {:>4,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Load tqdm with size counter instead of file counter\n",
    "with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "    with gzip.open(input_file_path, \"r\") as f:\n",
    "        for i, line_byte in enumerate(f): \n",
    "\n",
    "            read_bytes = len(line_byte)\n",
    "            if read_bytes:\n",
    "                pbar.set_postfix(file=input_file_path[len(DATA_FOLDER):], refresh=False)\n",
    "                pbar.update(read_bytes)\n",
    "\n",
    "            nb_rows_read += 1\n",
    "            \n",
    "            # set a maximum iteration for tests\n",
    "            if TEST_RUN == True:\n",
    "                if nb_rows_read >= MAX_ITER:\n",
    "                    break\n",
    "\n",
    "            # convert bytes into string\n",
    "            line_str = line_byte.decode(\"utf-8\")\n",
    "            \n",
    "            # convert string into json after escaping new line characters\n",
    "            line_str_esc = json_escape(line_str)\n",
    "            try:\n",
    "                line_json = json.loads(line_str_esc)\n",
    "            except Exception as e:\n",
    "                JSONDecodeErrors_cnt += 1\n",
    "                pass\n",
    "           \n",
    "            \n",
    "            # add line if patreon id is exists in df_yt_metadata_pt\n",
    "            if line_json['patreon'] in yt_patreon_list:\n",
    "                lines_json.append(line_json)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "time_diff = stop - start\n",
    "\n",
    "print()\n",
    "print(\"==> total time to read and filter graphtreon time series:                     {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "print(\"==> number of rows read:                                                      {:>10,}\".format(nb_rows_read))\n",
    "print(\"==> number of patreon ids that exist in both GTts and YT metadata  :          {:>10,} ({:.2%})\".format(len(lines_json), len(lines_json)/nb_rows_read ))\n",
    "print(\"==> number of skipped rows (JSONDecodeErrors):                                {:>10,}\".format(JSONDecodeErrors_cnt))\n",
    "\n",
    "# create new dataframe with the filtered lines\n",
    "df_gt_timeseries_filtered = pd.DataFrame(data=lines_json)\n",
    "\n",
    "# calculate memory usage of the new dataframe\n",
    "mem_cons = df_gt_timeseries_filtered.memory_usage(index=True).sum()\n",
    "print(\"==> memory usage of new (filtered) dataframe:                                  {:12,.2f} MB ({:,} bytes)\".format(mem_cons / 2**20, mem_cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# df_gt_timeseries_filtered.to_csv(PATH_GT_TIMESERIES_DST, index=False, sep='\\t', compression='gzip')\n",
    "!ls -lh {PATH_GT_TIMESERIES_DST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save restricted data to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "# df_gt_timeseries_restricted.to_csv(PATH_GT_TIMESERIES_RESTRICTED_DST, index=False, sep='\\t', compression='gzip')\n",
    "# !ls -lh {PATH_GT_TIMESERIES_RESTRICTED_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global variable for size of original GT dataset\n",
    "GT_final_processed_file_ROWS = 232_269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_GT_TIMESERIES_DST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_filtered = pd.read_csv(PATH_GT_TIMESERIES_DST, sep=\"\\t\", compression='gzip')\n",
    "df_gt_timeseries_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistics of loaded pre-filtered Graphtreon Timeseries file:\")\n",
    "print(\"[Graphtreon Timeseries] Total number of patreon ids:                                                   {:>9,}\".format(GT_final_processed_file_ROWS))\n",
    "print(\"[Graphtreon Timeseries] Nb of patreon ids that exist in both GT Timeseries and YT metadata restricted: {:>9,} ({:.1%} of GT timeseries dataset)\".format(len(df_gt_timeseries_filtered), len(df_gt_timeseries_filtered)/GT_final_processed_file_ROWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Join GT timeseries with matched channel_id\n",
    "Add corresponding YT channel id to dataframe \\\n",
    "(join to the channels in the restricted list of channels of the matched dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linked_channels_patreons.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_filtered.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join GT timeseries and matched channels\n",
    "df_gt_timeseries_merged = df_gt_timeseries_filtered.merge(df_linked_channels_patreons, left_on='patreon', right_on='patreon_id')\n",
    "df_gt_timeseries_merged.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract daily earnings per patreon account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique patreon ids in df_gt_timeseries_filtered\n",
    "yt_gt_patreon_list_filtered = df_gt_timeseries_filtered.patreon.unique()\n",
    "print(\"number of filtered patreon ids\", len(yt_gt_patreon_list_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Graphtreon source dataset, for each channel, extract the date and earnings from “dailyGraph_earningsSeriesData” (takes about 3 mins)\n",
    "input_file_path = PATH_GT_TIMESERIES_SRC\n",
    "\n",
    "TEST_RUN = False\n",
    "# MAX_ITER = 100\n",
    "\n",
    "nb_rows_read = 0\n",
    "valid_predicate_count = 0\n",
    "JSONDecodeErrors_cnt = 0 \n",
    "dailyEarningsError_cnt = 0 \n",
    "lines_json = []    \n",
    "\n",
    "compressed_file_size = os.stat(input_file_path).st_size\n",
    "print(\"Compressed file size is :                 {:>8,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "uncompressed_file_size = 13_310_000_000\n",
    "print(\"Estimated Uncompressed file size is :     {:>8,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Load tqdm with size counter instead of file counter\n",
    "with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "    with gzip.open(input_file_path, \"r\") as f:\n",
    "        for i, line in enumerate(f): \n",
    "\n",
    "            read_bytes = len(line)\n",
    "            if read_bytes:\n",
    "                pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "                pbar.update(read_bytes)\n",
    "\n",
    "            nb_rows_read += 1\n",
    "            \n",
    "            # set a maximum iteration for tests\n",
    "            if TEST_RUN == True:\n",
    "                if nb_rows_read >= MAX_ITER:\n",
    "                    break\n",
    "    \n",
    "            try:\n",
    "                line_json = json.loads(line)\n",
    "            except Exception as e:\n",
    "                JSONDecodeErrors_cnt += 1\n",
    "                continue\n",
    "                \n",
    "            # add line if patreon id is exists in df_yt_metadata_pt\n",
    "            if line_json['patreon'] in yt_gt_patreon_list_filtered:\n",
    "                valid_predicate_count += 1\n",
    "                \n",
    "                # Use ast.literal_eval to convert string of lists, to list of list\n",
    "                dailyGraph_earningsSeriesData = line_json.get('dailyGraph_earningsSeriesData')\n",
    "                \n",
    "                if dailyGraph_earningsSeriesData:\n",
    "                    daily_earnings = ast.literal_eval(dailyGraph_earningsSeriesData)\n",
    "                else:\n",
    "                    daily_earnings = [[np.nan, np.nan]]\n",
    "                                            \n",
    "                for daily_earning in daily_earnings:\n",
    "                    # case where there are multiple tuples per row\n",
    "                    if isinstance(daily_earning, list):\n",
    "                        date = daily_earning[0]\n",
    "                        earning = daily_earning[1]\n",
    "                        lines_json.append({\n",
    "                            'creatorName':   line_json.get('creatorName'), \n",
    "                            'creatorRange':  line_json.get('creatorRange'), \n",
    "                            'startDate':     line_json.get('startDate'),\n",
    "                            'categoryTitle': line_json.get('categoryTitle'),\n",
    "                            'patreon':       line_json.get('patreon'),\n",
    "                            'date':          date,\n",
    "                            'earning':       earning\n",
    "                        })\n",
    "                    else:\n",
    "                        dailyEarningsError_cnt += 1\n",
    "                        print(\">>>> dailyEarningsError - skipped line value: \")\n",
    "                        print(line_json.get('creatorName'), line_json.get('creatorRange'), line_json.get('startDate'), line_json.get('categoryTitle'), line_json.get('patreon'), daily_earnings)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "time_diff = stop - start\n",
    "\n",
    "print()\n",
    "print(\"==> total time to read and filter graphtreon time series:                      {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "print(\"==> number of rows read:                                                       {:>10,}\".format(nb_rows_read))\n",
    "print(\"==> number of patreon ids that exist in both GTts and restricted YT metadata:  {:>10,} ({:.2%})\".format(valid_predicate_count, valid_predicate_count/nb_rows_read ))\n",
    "print(\"==> number of skipped rows (JSONDecodeErrors):                                 {:>10,}\".format(JSONDecodeErrors_cnt))\n",
    "print(\"==> number of skipped rows (dailyEarningsError):                               {:>10,}\".format(dailyEarningsError_cnt))\n",
    "\n",
    "# create new dataframe with the filtered lines\n",
    "df_dailyGraph_earningsSeries = pd.DataFrame(data=lines_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Graphtreon dataset, for each channel, extract the date and earnings from “dailyGraph_earningsSeriesData” (takes about 3 mins)\n",
    "input_file_path = PATH_GT_TIMESERIES_SRC\n",
    "\n",
    "TEST_RUN = False\n",
    "# MAX_ITER = 100\n",
    "\n",
    "nb_rows_read = 0\n",
    "valid_predicate_count = 0\n",
    "JSONDecodeErrors_cnt = 0 \n",
    "dailyEarningsError_cnt = 0 \n",
    "lines_json = []    \n",
    "\n",
    "compressed_file_size = os.stat(input_file_path).st_size\n",
    "print(\"Compressed file size is :                 {:>8,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "uncompressed_file_size = 13_310_000_000\n",
    "print(\"Estimated Uncompressed file size is :     {:>8,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Load tqdm with size counter instead of file counter\n",
    "with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "    with gzip.open(input_file_path, \"r\") as f:\n",
    "        for i, line in enumerate(f): \n",
    "\n",
    "            read_bytes = len(line)\n",
    "            if read_bytes:\n",
    "                pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "                pbar.update(read_bytes)\n",
    "\n",
    "            nb_rows_read += 1\n",
    "            \n",
    "            # set a maximum iteration for tests\n",
    "            if TEST_RUN == True:\n",
    "                if nb_rows_read >= MAX_ITER:\n",
    "                    break\n",
    "    \n",
    "            try:\n",
    "                line_json = json.loads(line)\n",
    "            except Exception as e:\n",
    "                JSONDecodeErrors_cnt += 1\n",
    "                continue\n",
    "                \n",
    "            # add line if patreon id is exists in df_yt_metadata_pt\n",
    "            if line_json['patreon'] in yt_gt_patreon_list_filtered:\n",
    "                valid_predicate_count += 1\n",
    "                \n",
    "                # Use ast.literal_eval to convert string of lists, to list of list\n",
    "                dailyGraph_earningsSeriesData = line_json.get('dailyGraph_earningsSeriesData')\n",
    "                \n",
    "                if dailyGraph_earningsSeriesData:\n",
    "                    daily_earnings = ast.literal_eval(dailyGraph_earningsSeriesData)\n",
    "                else:\n",
    "                    daily_earnings = [[np.nan, np.nan]]\n",
    "                                            \n",
    "                for daily_earning in daily_earnings:\n",
    "                    # case where there are multiple tuples per row\n",
    "                    if isinstance(daily_earning, list):\n",
    "                        date = daily_earning[0]\n",
    "                        earning = daily_earning[1]\n",
    "                        lines_json.append({\n",
    "                            'creatorName':   line_json.get('creatorName'), \n",
    "                            'creatorRange':  line_json.get('creatorRange'), \n",
    "                            'startDate':     line_json.get('startDate'),\n",
    "                            'categoryTitle': line_json.get('categoryTitle'),\n",
    "                            'patreon':       line_json.get('patreon'),\n",
    "                            'date':          date,\n",
    "                            'earning':       earning\n",
    "                        })\n",
    "                    else:\n",
    "                        dailyEarningsError_cnt += 1\n",
    "                        print(\">>>> dailyEarningsError - skipped line value: \")\n",
    "                        print(line_json.get('creatorName'), line_json.get('creatorRange'), line_json.get('startDate'), line_json.get('categoryTitle'), line_json.get('patreon'), daily_earnings)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "time_diff = stop - start\n",
    "\n",
    "print()\n",
    "print(\"==> total time to read and filter graphtreon time series:                      {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "print(\"==> number of rows read:                                                       {:>10,}\".format(nb_rows_read))\n",
    "print(\"==> number of patreon ids that exist in both GTts and restricted YT metadata:  {:>10,} ({:.2%})\".format(valid_predicate_count, valid_predicate_count/nb_rows_read ))\n",
    "print(\"==> number of skipped rows (JSONDecodeErrors):                                 {:>10,}\".format(JSONDecodeErrors_cnt))\n",
    "print(\"==> number of skipped rows (dailyEarningsError):                               {:>10,}\".format(dailyEarningsError_cnt))\n",
    "\n",
    "# create new dataframe with the filtered lines\n",
    "df_dailyGraph_earningsSeries = pd.DataFrame(data=lines_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values\n",
    "df_dailyGraph_earningsSeries[df_dailyGraph_earningsSeries.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv (5.3Mb)\n",
    "# df_dailyGraph_earningsSeries.to_csv(PATH_GT_TIMESERIES_EARNING_DST, index=False, sep='\\t', compression='gzip')\n",
    "!ls -lh {PATH_GT_TIMESERIES_EARNING_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract daily patrons per patreon account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Graphtreon dataset, for each channel, extract the date and patrons from “dailyGraph_patronSeriesData” (takes about 3 mins)\n",
    "input_file_path = PATH_GT_TIMESERIES_SRC\n",
    "\n",
    "TEST_RUN = False\n",
    "MAX_ITER = 1000\n",
    "\n",
    "nb_rows_read = 0\n",
    "valid_predicate_count = 0\n",
    "JSONDecodeErrors_cnt = 0 \n",
    "dailyPatronsError_cnt = 0 \n",
    "lines_json = []    \n",
    "\n",
    "compressed_file_size = os.stat(input_file_path).st_size\n",
    "print(\"Compressed file size is :                 {:>8,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "uncompressed_file_size = 13_310_000_000\n",
    "print(\"Estimated Uncompressed file size is :     {:>8,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Load tqdm with size counter instead of file counter\n",
    "with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "    with gzip.open(input_file_path, \"r\") as f:\n",
    "        for i, line in enumerate(f): \n",
    "\n",
    "            read_bytes = len(line)\n",
    "            if read_bytes:\n",
    "                pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "                pbar.update(read_bytes)\n",
    "\n",
    "            nb_rows_read += 1\n",
    "            \n",
    "            # set a maximum iteration for tests\n",
    "            if TEST_RUN == True:\n",
    "                if nb_rows_read >= MAX_ITER:\n",
    "                    break\n",
    "    \n",
    "            try:\n",
    "                line_json = json.loads(line)\n",
    "            except Exception as e:\n",
    "                JSONDecodeErrors_cnt += 1\n",
    "                continue\n",
    "                \n",
    "            # add line if patreon id is exists in df_yt_metadata_pt\n",
    "            if line_json['patreon'] in yt_gt_patreon_list_filtered:\n",
    "                valid_predicate_count += 1\n",
    "                \n",
    "                # Use ast.literal_eval to convert string of lists, to list of list\n",
    "                dailyGraph_patronSeriesData = line_json.get('dailyGraph_patronSeriesData')\n",
    "                \n",
    "                if dailyGraph_patronSeriesData:\n",
    "                    daily_patrons = ast.literal_eval(dailyGraph_patronSeriesData)\n",
    "                else:\n",
    "                    daily_patrons = [[np.nan, np.nan]]\n",
    "                                            \n",
    "                for daily_patron in daily_patrons:\n",
    "                    # case where there are multiple tuples per row\n",
    "                    if isinstance(daily_patron, list):\n",
    "                        date = daily_patron[0]\n",
    "                        patrons = daily_patron[1]\n",
    "                        lines_json.append({\n",
    "                            'creatorName':   line_json.get('creatorName'), \n",
    "                            'creatorRange':  line_json.get('creatorRange'), \n",
    "                            'startDate':     line_json.get('startDate'),\n",
    "                            'categoryTitle': line_json.get('categoryTitle'),\n",
    "                            'patreon':       line_json.get('patreon'),\n",
    "                            'date':          date,\n",
    "                            'patrons':       patrons\n",
    "                        })\n",
    "                    else:\n",
    "                        dailyPatronsError_cnt += 1\n",
    "                        print(\">>>> dailyPatronsError - skipped line value: \")\n",
    "                        print(line_json.get('creatorName'), line_json.get('creatorRange'), line_json.get('startDate'), line_json.get('categoryTitle'), line_json.get('patreon'), daily_patrons)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "time_diff = stop - start\n",
    "\n",
    "print()\n",
    "print(\"==> total time to read and filter graphtreon time series:                      {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "print(\"==> number of rows read:                                                       {:>10,}\".format(nb_rows_read))\n",
    "print(\"==> number of patreon ids that exist in both GTts and restricted YT metadata:  {:>10,} ({:.2%})\".format(valid_predicate_count, valid_predicate_count/nb_rows_read ))\n",
    "print(\"==> number of skipped rows (JSONDecodeErrors):                                 {:>10,}\".format(JSONDecodeErrors_cnt))\n",
    "print(\"==> number of skipped rows (dailyPatronsError):                               {:>10,}\".format(dailyPatronsError_cnt))\n",
    "\n",
    "# create new dataframe with the filtered lines\n",
    "df_dailyGraph_patronsSeries = pd.DataFrame(data=lines_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values\n",
    "# df_dailyGraph_patronsSeries[df_dailyGraph_patronsSeries.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv (7.1Mb)\n",
    "# df_dailyGraph_patronsSeries.to_csv(PATH_GT_TIMESERIES_PATRONS_DST, index=False, sep='\\t', compression='gzip')\n",
    "!ls -lh {PATH_GT_TIMESERIES_PATRONS_DST}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge extracted times series of daily earnings and daily patrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_GT_TIMESERIES_EARNING_DST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dailyGraph_earningsSeries file from disk and convert dates\n",
    "df_dailyGraph_earningsSeries = pd.read_csv(PATH_GT_TIMESERIES_EARNING_DST, sep=\"\\t\", compression='gzip')\n",
    "# df_dailyGraph_earningsSeries.date = pd.to_datetime(df_dailyGraph_earningsSeries.date, unit='ms')\n",
    "df_dailyGraph_earningsSeries.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PATH_GT_TIMESERIES_PATRONS_DST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dailyGraph_patronsSeries from disk and convert dates\n",
    "df_dailyGraph_patronsSeries = pd.read_csv(PATH_GT_TIMESERIES_PATRONS_DST, sep=\"\\t\", compression='gzip')\n",
    "# df_dailyGraph_patronsSeries.date = pd.to_datetime(df_dailyGraph_patronsSeries.date, unit='ms')\n",
    "df_dailyGraph_patronsSeries.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dailyGraph_earningsSeries with df_dailyGraph_patronsSeries\n",
    "df_dailyGraph_patrons_and_earnings_Series = df_dailyGraph_earningsSeries.merge(df_dailyGraph_patronsSeries, how='outer')\n",
    "\n",
    "# convert patrons column to Int64 so it can hold NaN values after outer join\n",
    "df_dailyGraph_patrons_and_earnings_Series['patrons'] = df_dailyGraph_patrons_and_earnings_Series['patrons'].astype('Int64')\n",
    "df_dailyGraph_patrons_and_earnings_Series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv (6.2Mb)\n",
    "# df_dailyGraph_patrons_and_earnings_Series.to_csv(PATH_GT_TIMESERIES_PATRONS_AND_EARNINGS_DST, index=False, sep='\\t', compression='gzip')\n",
    "!ls -lh {PATH_GT_TIMESERIES_PATRONS_AND_EARNINGS_DST}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
