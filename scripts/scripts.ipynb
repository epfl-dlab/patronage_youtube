{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0adb7c-2f81-4eac-9456-f214b2badabf",
   "metadata": {},
   "source": [
    "# Characterizing Patronage on YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af507cb8-67eb-4b89-bd7e-24d0fbd43c30",
   "metadata": {},
   "source": [
    "## Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e9bb4-0897-40b2-9886-558d93f58715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import zstandard\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2474d7d-7b8e-4642-8c2c-e62407e0f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/dlabdata1/youtube_large/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202315b9-01b8-48f7-99b6-1df8737f2154",
   "metadata": {},
   "source": [
    "### 1. Filter YouTube metadata containing patreon id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbf702-a7af-473d-9b7e-3e06cb590614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uncompressed file size - problem: returns a negative ratio\n",
    "# !gzip -l /dlabdata1/youtube_large/yt_metadata_en.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23908d-4509-4fbf-8ae8-44efb53bcc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(2**10, \"Bytes = 1 KB\")\n",
    "# print(2**20, \"Bytes = 1 MB\")\n",
    "# print(2**30, \"Bytes = 1 GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f12f7-63ac-4d74-b6d1-6c7a216202c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_escape(str):\n",
    "    \"\"\"\n",
    "    replace new line special character by a space\n",
    "    \"\"\"\n",
    "    return str.replace(\"\\\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12380c7a-4708-4122-af59-7c8fdba1a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract patreon accounts from youtube channel descriptions and\n",
    "# filter the metadata to retain only the rows which description contains a patreon url\n",
    "input_file_path = DATA_FOLDER+\"/yt_metadata_en.jsonl.gz\"\n",
    "\n",
    "# MAX_ITER = 10_000\n",
    "\n",
    "nb_rows_read = 0\n",
    "JSONDecodeErrors_cnt = 0 \n",
    "lines_json = []    \n",
    "\n",
    "# match patterns starting with patreon.com/ and matching any character after until space\n",
    "# pattern = re.compile(r'patreon.com/[^\\s]*')\n",
    "\n",
    "# match patterns starting with patreon.com/ and matching at least 1 character after until space\n",
    "pattern = re.compile(r'patreon.com/[^\\s]+')\n",
    "\n",
    "\n",
    "compressed_file_size = os.stat(input_file_path).st_size\n",
    "print(\"Compressed file size is :                 {:>3,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "\n",
    "uncompressed_file_size = 97_600_000_000\n",
    "print(\"Estimated Uncompressed file size is :     {:>3,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Load tqdm with size counter instead of file counter\n",
    "with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "    with gzip.open(input_file_path, \"r\") as f:\n",
    "        for i, line_byte in enumerate(f): \n",
    "\n",
    "            read_bytes = len(line_byte)\n",
    "            if read_bytes:\n",
    "                pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "                pbar.update(read_bytes)\n",
    "\n",
    "            nb_rows_read += 1\n",
    "            \n",
    "            # set a maximum iteration for tests\n",
    "            # if nb_rows_read >= MAX_ITER:\n",
    "            #     break\n",
    "\n",
    "            # convert bytes into string\n",
    "            line_str = line_byte.decode(\"utf-8\")\n",
    "\n",
    "            # convert string into json after escaping new line characters\n",
    "            line_str_esc = json_escape(line_str)\n",
    "            try:\n",
    "                line_json = json.loads(line_str_esc)\n",
    "            except Exception as e:\n",
    "                JSONDecodeErrors_cnt += 1\n",
    "                pass\n",
    "\n",
    "            # print(line_json)\n",
    "            # print(line_json['categories'])\n",
    "            \n",
    "            # add line if description contains a patreon.com id\n",
    "            if re.search(pattern, line_json['description']):\n",
    "                patreon_id = re.findall(pattern, line_json['description'])[0]\n",
    "                line_json['patreon_id'] = patreon_id\n",
    "                lines_json.append(line_json)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "time_diff = stop - start\n",
    "\n",
    "print()\n",
    "print(\"==> total time to read and filter youtube metadata:                {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "print(\"==> number of rows read:                                           {:>10,}\".format(nb_rows_read))\n",
    "print(\"==> number of videos containing a patreon link in the description: {:>10,} ({:.3%})\".format(len(lines_json), len(lines_json)/nb_rows_read ))\n",
    "print(\"==> number of skipped rows (JSONDecodeErrors):                     {:>10,} ({:.3%})\".format(JSONDecodeErrors_cnt, JSONDecodeErrors_cnt/nb_rows_read))\n",
    "\n",
    "# create new dataframe with the filtered lines\n",
    "df_yt_metadata_pt = pd.DataFrame(data=lines_json, index=None)\n",
    "\n",
    "# calculate memory usage of the new dataframe\n",
    "mem_cons = df_yt_metadata_pt.memory_usage(index=True).sum()\n",
    "print(\"==> memory usage of new (filtered) dataframe:                      {:12,.2f} GB ({:,} bytes)\".format(mem_cons / 2**30, mem_cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ce483-1fa0-416e-afb4-a7a516d4d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt_metadata_pt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe5757-e2ed-4bc6-bfa7-851cc44ffc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in DATA_FOLDER\n",
    "# !ls -lh /dlabdata1/youtube_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff5eca-70d2-4aa0-acd3-05b5acb92420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in LOCAL_FOLDER\n",
    "!ls -lh ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f117eeb-319c-435b-a45d-c4e1f555aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "output_file_path = \"../yt_metadata_en_pt.tsv.gz\"\n",
    "df_yt_metadata_pt.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea2bc7-9b32-4a1b-aa9c-2161d57ad842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as feather - PROBLEM: KERNEL DIES\n",
    "# output_file_path = \"../yt_metadata_en_pt.feather\"\n",
    "# df_yt_metadata_pt.to_feather(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6fce6e-c601-40bd-9c34-6afb1f3689ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh \"../yt_metadata_en_pt.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d908a89-07f1-4471-8340-91d316eaaf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read feather file\n",
    "# df_yt_metadata_pt = pd.read_feather(output_file_path)\n",
    "# df_yt_metadata_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5372b-b96b-41f0-af7a-20950ea47edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_yt_metadata_pt = pd.read_csv('../yt_metadata_en_pt.tsv.gz', sep=\"\\t\", lineterminator='\\n', compression='gzip', nrows=100_000)\n",
    "df_yt_metadata_pt = pd.read_csv('../yt_metadata_en_pt.tsv.gz\", sep=\"\\t\", lineterminator='\\n', compression='gzip') # takes about 2 mins\n",
    "df_yt_metadata_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48baa9-f2b3-4141-8c48-ec2044c90cf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Filter Graphtreon to keep only records which patreon id exists the YouTube metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df1910-1770-44ed-9edd-72996dfe8014",
   "metadata": {},
   "source": [
    "Read filtered youtube metadata file from disk... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287215ba-5db6-45d5-bfa1-d80b4e704ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh \"../yt_metadata_en_pt.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc4377-3e3f-41a7-a3ac-e4f1b070e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filtered youtube metadata file (takes about 2 mins)\n",
    "df_yt_metadata_pt = pd.read_csv(\"../yt_metadata_en_pt.tsv.gz\", sep=\"\\t\", lineterminator='\\n', compression='gzip') \n",
    "df_yt_metadata_pt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10abe6d-f574-4bf1-8276-6c44c23d16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[YouTube metadata] number of videos that contain a patreon link in description:      {:>10,}\".format(len(df_yt_metadata_pt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa7a58-b5a7-483b-b745-3d2950318838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique patreon ids in df_yt_metadata_pt\n",
    "yt_patreon_list = df_yt_metadata_pt.patreon_id.unique()\n",
    "print(\"[Filtered YouTube metadata] total number of unique patreon ids:                       {:>9,}\".format(len(yt_patreon_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bba112-5dfd-4627-aaf9-92b12b9d53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dlabdata1/youtube_large/final_processed_file.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f2fd0-5776-45a7-bf9e-80c87c5c6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_escape(str):\n",
    "    \"\"\"\n",
    "    replace new line special character by a space\n",
    "    \"\"\"\n",
    "    return str.replace(\"\\\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e92943-7879-4531-9219-70c314cf3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract patreon accounts from youtube channel descriptions and\n",
    "# filter the metadata to retain only the rows which patreon url exists in the filtered YT metadata \n",
    "input_file_path = DATA_FOLDER+\"/final_processed_file.jsonl.gz\"\n",
    "\n",
    "# MAX_ITER = 1_000\n",
    "\n",
    "nb_rows_read = 0\n",
    "JSONDecodeErrors_cnt = 0 \n",
    "lines_json = []    \n",
    "\n",
    "# pattern = re.compile(r'patreon.com/\\w*')\n",
    "\n",
    "compressed_file_size = os.stat(input_file_path).st_size\n",
    "print(\"Compressed file size is :                 {:>3,.2f} GB\".format(compressed_file_size / 2**30))\n",
    "# 12.4\n",
    "uncompressed_file_size = 13_310_000_000\n",
    "print(\"Estimated Uncompressed file size is :     {:>3,.2f} GB\".format(uncompressed_file_size / 2**30))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Load tqdm with size counter instead of file counter\n",
    "with tqdm(total=uncompressed_file_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "    with gzip.open(input_file_path, \"r\") as f:\n",
    "        for i, line_byte in enumerate(f): \n",
    "\n",
    "            read_bytes = len(line_byte)\n",
    "            if read_bytes:\n",
    "                pbar.set_postfix(file=input_file_path[len(DATA_FOLDER)+1:], refresh=False)\n",
    "                pbar.update(read_bytes)\n",
    "\n",
    "            nb_rows_read += 1\n",
    "            \n",
    "            # set a maximum iteration for tests\n",
    "            # if nb_rows_read >= MAX_ITER:\n",
    "            #     break\n",
    "\n",
    "            # convert bytes into string\n",
    "            line_str = line_byte.decode(\"utf-8\")\n",
    "\n",
    "            \n",
    "            # convert string into json after escaping new line characters\n",
    "            line_str_esc = json_escape(line_str)\n",
    "            try:\n",
    "                line_json = json.loads(line_str_esc)\n",
    "            except Exception as e:\n",
    "                JSONDecodeErrors_cnt += 1\n",
    "                pass\n",
    "           \n",
    "            \n",
    "            # add line if patreon id is exists in df_yt_metadata_pt\n",
    "            if line_json['patreon'] in yt_patreon_list:\n",
    "                lines_json.append(line_json)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "time_diff = stop - start\n",
    "\n",
    "print()\n",
    "print(\"==> total time to read and filter graphtreon time series:          {:>10.0f} min. ({:.0f}s.)\".format(time_diff/60, time_diff)) \n",
    "print(\"==> number of rows read:                                           {:>10,}\".format(nb_rows_read))\n",
    "print(\"==> number of patreon ids that exist in both GTts and YT metadata: {:>10,} ({:.2%})\".format(len(lines_json), len(lines_json)/nb_rows_read ))\n",
    "print(\"==> number of skipped rows (JSONDecodeErrors):                     {:>10,}\".format(JSONDecodeErrors_cnt))\n",
    "\n",
    "# create new dataframe with the filtered lines\n",
    "df_gt_timeseries_filtered = pd.DataFrame(data=lines_json)\n",
    "\n",
    "# calculate memory usage of the new dataframe\n",
    "mem_cons = df_gt_timeseries_filtered.memory_usage(index=True).sum()\n",
    "print(\"==> memory usage of new (filtered) dataframe:                      {:12,.2f} MB ({:,} bytes)\".format(mem_cons / 2**20, mem_cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682724eb-e0d3-4b40-b99f-0f1db73681bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_timeseries_filtered.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad98c4-cc22-4a34-a0c8-b0ef5177a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to disk as feather - ERROR\n",
    "# output_file_path = \"df_gt_timeseries_filtered.feather\"\n",
    "# df_gt_timeseries_filtered.to_feather(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9368504-7acf-4385-b806-a51c71075fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data to LOCAL SCRATCH FOLDER as a compressed tsv\n",
    "output_file_path = \"../df_gt_timeseries_filtered.tsv.gz\"\n",
    "df_gt_timeseries_filtered.to_csv(output_file_path, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1eef32-8826-4ae0-8486-52579ef7eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh yt_metadata_en_pt.tsv.gz\n",
    "!ls -lh ../df_gt_timeseries_filtered.tsv.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
